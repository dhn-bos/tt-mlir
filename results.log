// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<1x14xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xi64>}> : () -> tensor<19xi64>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xi64>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xi64>, tensor<1x14xi64>) -> tensor<1x14xi64>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xi64>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xi64>, tensor<14xi64>) -> tensor<14xi64>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xi64>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xi64>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xi64>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xi64>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xi32>
    %201 = ttir.empty() : tensor<14x1xi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xi32>, tensor<14x1xi32>) -> tensor<14x1xi32>
    %203 = ttir.empty() : tensor<14x19xi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xi32>, tensor<14x19xi32>) -> tensor<14x19xi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xi32>
    %206 = ttir.empty() : tensor<1x19xi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xi32>, tensor<1x19xi32>) -> tensor<1x19xi32>
    %208 = ttir.empty() : tensor<14x19xi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xi32>, tensor<14x19xi32>) -> tensor<14x19xi32>
    %210 = ttir.empty() : tensor<14x19xi1>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xi32>, tensor<14x19xi32>, tensor<14x19xi1>) -> tensor<14x19xi1>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xi1>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xi64>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xi64>, tensor<1x19xi64>) -> tensor<1x19xi64>
    %220 = ttir.empty() : tensor<14x19xi64>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xi64>, tensor<14x19xi64>) -> tensor<14x19xi64>
    %222 = ttir.empty() : tensor<14x1xi64>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xi64>, tensor<14x1xi64>) -> tensor<14x1xi64>
    %224 = ttir.empty() : tensor<14x19xi64>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xi64>, tensor<14x19xi64>) -> tensor<14x19xi64>
    %226 = ttir.empty() : tensor<14x19xi1>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xi64>, tensor<14x19xi64>, tensor<14x19xi1>) -> tensor<14x19xi1>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xi1>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %201 = ttir.empty() : tensor<14x1xsi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %203 = ttir.empty() : tensor<14x19xsi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %206 = ttir.empty() : tensor<1x19xsi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %208 = ttir.empty() : tensor<14x19xsi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %210 = ttir.empty() : tensor<14x19xbf16>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xsi32>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %220 = ttir.empty() : tensor<14x19xsi32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %222 = ttir.empty() : tensor<14x1xsi32>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %224 = ttir.empty() : tensor<14x19xsi32>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %226 = ttir.empty() : tensor<14x19xbf16>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
        %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %5 = ttir.empty() : tensor<1x14xsi32>
        %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
        %7 = ttir.empty() : tensor<128256x4096xf32>
        %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
        %9 = ttir.empty() : tensor<14xsi32>
        %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
        %11 = ttir.empty() : tensor<64xf32>
        %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
        %13 = ttir.empty() : tensor<128x4096xf32>
        %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %15 = ttir.empty() : tensor<f32>
        %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %17 = ttir.empty() : tensor<4096xf32>
        %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %19 = ttir.empty() : tensor<128x4096xf32>
        %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %21 = ttir.empty() : tensor<4096x1792xf32>
        %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %23 = ttir.empty() : tensor<1792x4096xf32>
        %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %25 = ttir.empty() : tensor<4096x512xf32>
        %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %27 = ttir.empty() : tensor<f32>
        %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %29 = ttir.empty() : tensor<f32>
        %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %31 = ttir.empty() : tensor<512x4096xf32>
        %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %33 = ttir.empty() : tensor<4096xf32>
        %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %35 = ttir.empty() : tensor<1792x4096xf32>
        %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %37 = ttir.empty() : tensor<4096xf32>
        %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %39 = ttir.empty() : tensor<16032x4096xf32>
        %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
        %41 = ttir.empty() : tensor<1x1xf32>
        %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %43 = ttir.empty() : tensor<14x19xf32>
        %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %45 = ttir.empty() : tensor<1x1x1xf32>
        %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %47 = ttir.empty() : tensor<1x14x4096xf32>
        %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %49 = ttir.empty() : tensor<1x1x1x1xbf16>
        %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %51 = ttir.empty() : tensor<1x1x19x128xbf16>
        %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %53 = ttir.empty() : tensor<1x14xui32>
        %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
        %55 = ttir.empty() : tensor<14xui32>
        %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
        %57 = ttir.empty() : tensor<14x4096xf32>
        %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %59 = ttir.empty() : tensor<1x14x4096xf32>
        %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %61 = ttir.empty() : tensor<1x1x4096xf32>
        %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %63 = ttir.empty() : tensor<1x14x4096xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %65 = ttir.empty() : tensor<1x14x4096xf32>
        %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %67 = ttir.empty() : tensor<1x14xf32>
        %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %69 = ttir.empty() : tensor<1x14xf32>
        %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %71 = ttir.empty() : tensor<1x14x1xf32>
        %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %73 = ttir.empty() : tensor<1x1x1xf32>
        %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %75 = ttir.empty() : tensor<1x14x1xf32>
        %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %77 = ttir.empty() : tensor<1x14x1xf32>
        %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %79 = ttir.empty() : tensor<1x14x1xf32>
        %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %81 = ttir.empty() : tensor<1x14x4096xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %83 = ttir.empty() : tensor<1x14x4096xf32>
        %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %85 = ttir.empty() : tensor<1x14x4096xf32>
        %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %87 = ttir.empty() : tensor<14x4096xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %89 = ttir.empty() : tensor<4096x128xf32>
        %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %92 = ttir.empty() : tensor<1x14x1x128xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
        %94 = ttir.empty() : tensor<1x1x14x128xf32>
        %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %96 = ttir.empty() : tensor<1x64x1xf32>
        %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
        %98 = ttir.empty() : tensor<14xf32>
        %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
        %100 = ttir.empty() : tensor<1x1x14xf32>
        %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
        %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
        %103 = ttir.empty() : tensor<1x14x64xf32>
        %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
        %105 = ttir.empty() : tensor<1x14x128xf32>
        %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %107 = ttir.empty() : tensor<1x14x128xf32>
        %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %109 = ttir.empty() : tensor<1x1x14x128xf32>
        %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %111 = ttir.empty() : tensor<1x1x14x128xf32>
        %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %113 = ttir.empty() : tensor<1x1x14x64xf32>
        %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %115 = ttir.empty() : tensor<1x1x14x64xf32>
        %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %117 = ttir.empty() : tensor<1x1x14x64xf32>
        %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %119 = ttir.empty() : tensor<1x1x14x128xf32>
        %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %121 = ttir.empty() : tensor<1x14x128xf32>
        %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %123 = ttir.empty() : tensor<1x1x14x128xf32>
        %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %125 = ttir.empty() : tensor<1x1x14x128xf32>
        %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %127 = ttir.empty() : tensor<1x1x14x128xf32>
        %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %129 = ttir.empty() : tensor<1x1x14x128xbf16>
        %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %131 = ttir.empty() : tensor<1x1x19x128xbf16>
        %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %133 = ttir.empty() : tensor<4096x128xf32>
        %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %136 = ttir.empty() : tensor<14x128xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
        %138 = ttir.empty() : tensor<1x14x1x128xbf16>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
        %140 = ttir.empty() : tensor<1x1x14x128xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %142 = ttir.empty() : tensor<1x1x19x128xbf16>
        %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %144 = ttir.empty() : tensor<1x1x4096xf32>
        %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %146 = ttir.empty() : tensor<1x14x4096xf32>
        %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %148 = ttir.empty() : tensor<4096x512xf32>
        %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
        %151 = ttir.empty() : tensor<1x14x4x128xf32>
        %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %153 = ttir.empty() : tensor<1x4x14x128xf32>
        %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %155 = ttir.empty() : tensor<1x1x14x128xf32>
        %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %157 = ttir.empty() : tensor<1x4x14x128xf32>
        %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %159 = ttir.empty() : tensor<1x4x14x128xf32>
        %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %161 = ttir.empty() : tensor<1x4x14x64xf32>
        %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %163 = ttir.empty() : tensor<1x4x14x64xf32>
        %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %165 = ttir.empty() : tensor<1x4x14x64xf32>
        %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %167 = ttir.empty() : tensor<1x4x14x128xf32>
        %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %169 = ttir.empty() : tensor<1x1x14x128xf32>
        %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %171 = ttir.empty() : tensor<1x4x14x128xf32>
        %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %173 = ttir.empty() : tensor<1x4x14x128xf32>
        %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %175 = ttir.empty() : tensor<1x4x14x128xf32>
        %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %177 = ttir.empty() : tensor<4x14x128xf32>
        %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
        %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %185 = ttir.empty() : tensor<1x4x19x128xf32>
        %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
        %187 = ttir.empty() : tensor<1x4x128x19xf32>
        %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
        %189 = ttir.empty() : tensor<4x128x19xf32>
        %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
        %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
        %192 = ttir.empty() : tensor<1x4x14x19xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x4x14x19xf32>
        %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %198 = ttir.empty() : tensor<1x4x14x19xf32>
        %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
        %201 = ttir.empty() : tensor<14x1xsi32>
        %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %203 = ttir.empty() : tensor<14x19xsi32>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
        %206 = ttir.empty() : tensor<1x19xsi32>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %208 = ttir.empty() : tensor<14x19xsi32>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %210 = ttir.empty() : tensor<14x19xbf16>
        %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %212 = ttir.empty() : tensor<1x1xf32>
        %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %214 = ttir.empty() : tensor<14x19xf32>
        %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %216 = ttir.empty() : tensor<14x19xf32>
        %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %218 = ttir.empty() : tensor<1x19xsi32>
        %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %220 = ttir.empty() : tensor<14x19xsi32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %222 = ttir.empty() : tensor<14x1xsi32>
        %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %224 = ttir.empty() : tensor<14x19xsi32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %226 = ttir.empty() : tensor<14x19xbf16>
        %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %228 = ttir.empty() : tensor<14x19xf32>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %230 = ttir.empty() : tensor<14x19xf32>
        %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %232 = ttir.empty() : tensor<1x1x14x19xf32>
        %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
        %234 = ttir.empty() : tensor<1x4x14x19xf32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %236 = ttir.empty() : tensor<1x4x14x19xf32>
        %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %238 = ttir.empty() : tensor<1x4x14xf32>
        %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %240 = ttir.empty() : tensor<1x4x14x1xf32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %242 = ttir.empty() : tensor<1x4x14x19xf32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %244 = ttir.empty() : tensor<1x4x14x19xf32>
        %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %246 = ttir.empty() : tensor<1x4x14x19xf32>
        %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %248 = ttir.empty() : tensor<1x4x14xf32>
        %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %250 = ttir.empty() : tensor<1x4x14x1xf32>
        %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %252 = ttir.empty() : tensor<1x4x14x19xf32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %254 = ttir.empty() : tensor<1x4x14x19xf32>
        %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %256 = ttir.empty() : tensor<4x14x19xf32>
        %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
        %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %264 = ttir.empty() : tensor<4x19x128xf32>
        %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
        %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
        %267 = ttir.empty() : tensor<1x4x14x128xf32>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %269 = ttir.empty() : tensor<1x14x4x128xf32>
        %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %271 = ttir.empty() : tensor<14x512xf32>
        %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
        %273 = ttir.empty() : tensor<512x4096xf32>
        %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
        %276 = ttir.empty() : tensor<14x4096xf32>
        %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %278 = ttir.empty() : tensor<1x14x4096xf32>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %280 = ttir.empty() : tensor<1x14x4096xf32>
        %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %282 = ttir.empty() : tensor<1x1x4096xf32>
        %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %284 = ttir.empty() : tensor<1x14x4096xf32>
        %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %286 = ttir.empty() : tensor<1x14x4096xf32>
        %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %288 = ttir.empty() : tensor<1x14xf32>
        %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %290 = ttir.empty() : tensor<1x14xf32>
        %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %292 = ttir.empty() : tensor<1x14x1xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %294 = ttir.empty() : tensor<1x14x1xf32>
        %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %296 = ttir.empty() : tensor<1x14x1xf32>
        %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %298 = ttir.empty() : tensor<1x14x4096xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %300 = ttir.empty() : tensor<1x14x4096xf32>
        %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %302 = ttir.empty() : tensor<1x14x4096xf32>
        %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %304 = ttir.empty() : tensor<14x4096xf32>
        %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %306 = ttir.empty() : tensor<4096x1792xf32>
        %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %309 = ttir.empty() : tensor<1x14x1792xf32>
        %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %311 = ttir.empty() : tensor<1x14x1792xf32>
        %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %313 = ttir.empty() : tensor<1x14x1792xf32>
        %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %315 = ttir.empty() : tensor<4096x1792xf32>
        %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %318 = ttir.empty() : tensor<1x14x1792xf32>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %320 = ttir.empty() : tensor<1x14x1792xf32>
        %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %322 = ttir.empty() : tensor<14x1792xf32>
        %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
        %324 = ttir.empty() : tensor<1792x4096xf32>
        %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
        %327 = ttir.empty() : tensor<14x4096xf32>
        %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %329 = ttir.empty() : tensor<1x14x4096xf32>
        %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %331 = ttir.empty() : tensor<1x14x4096xf32>
        %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %333 = ttir.empty() : tensor<1x14x4096xf32>
        %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %335 = ttir.empty() : tensor<1x14xf32>
        %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %337 = ttir.empty() : tensor<1x14xf32>
        %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %339 = ttir.empty() : tensor<1x14x1xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %341 = ttir.empty() : tensor<1x14x1xf32>
        %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %343 = ttir.empty() : tensor<1x14x1xf32>
        %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %345 = ttir.empty() : tensor<1x14x4096xf32>
        %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %347 = ttir.empty() : tensor<1x14x4096xf32>
        %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %349 = ttir.empty() : tensor<1x14x4096xf32>
        %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %351 = ttir.empty() : tensor<14x4096xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %353 = ttir.empty() : tensor<4096x16032xf32>
        %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
        %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
        %356 = ttir.empty() : tensor<1x14x16032xf32>
        %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
        %358 = ttir.empty() : tensor<1x14x4096xf32>
        %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %360 = ttir.empty() : tensor<1x8x19x128xbf16>
        %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %362 = ttir.empty() : tensor<1x8x19x128xbf16>
        %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %364 = ttir.empty() : tensor<1x14x4096xf32>
        %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %366 = ttir.empty() : tensor<14x128256xf32>
        %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
        %368 = ttir.empty() : tensor<1x14x128256xf32>
        %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
        return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
      }
    }
  }
}


// -----// IR Dump After TTIRHoistTransform (ttir-cpu-hoist-transform) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
        %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %5 = ttir.empty() : tensor<1x14xsi32>
        %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
        %7 = ttir.empty() : tensor<128256x4096xf32>
        %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
        %9 = ttir.empty() : tensor<14xsi32>
        %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
        %11 = ttir.empty() : tensor<64xf32>
        %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
        %13 = ttir.empty() : tensor<128x4096xf32>
        %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %15 = ttir.empty() : tensor<f32>
        %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %17 = ttir.empty() : tensor<4096xf32>
        %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %19 = ttir.empty() : tensor<128x4096xf32>
        %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %21 = ttir.empty() : tensor<4096x1792xf32>
        %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %23 = ttir.empty() : tensor<1792x4096xf32>
        %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %25 = ttir.empty() : tensor<4096x512xf32>
        %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %27 = ttir.empty() : tensor<f32>
        %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %29 = ttir.empty() : tensor<f32>
        %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %31 = ttir.empty() : tensor<512x4096xf32>
        %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %33 = ttir.empty() : tensor<4096xf32>
        %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %35 = ttir.empty() : tensor<1792x4096xf32>
        %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %37 = ttir.empty() : tensor<4096xf32>
        %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %39 = ttir.empty() : tensor<16032x4096xf32>
        %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
        %41 = ttir.empty() : tensor<1x1xf32>
        %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %43 = ttir.empty() : tensor<14x19xf32>
        %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %45 = ttir.empty() : tensor<1x1x1xf32>
        %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %47 = ttir.empty() : tensor<1x14x4096xf32>
        %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %49 = ttir.empty() : tensor<1x1x1x1xbf16>
        %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %51 = ttir.empty() : tensor<1x1x19x128xbf16>
        %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %53 = ttir.empty() : tensor<1x14xui32>
        %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
        %55 = ttir.empty() : tensor<14xui32>
        %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
        %57 = ttir.empty() : tensor<14x4096xf32>
        %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %59 = ttir.empty() : tensor<1x14x4096xf32>
        %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %61 = ttir.empty() : tensor<1x1x4096xf32>
        %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %63 = ttir.empty() : tensor<1x14x4096xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %65 = ttir.empty() : tensor<1x14x4096xf32>
        %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %67 = ttir.empty() : tensor<1x14xf32>
        %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %69 = ttir.empty() : tensor<1x14xf32>
        %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %71 = ttir.empty() : tensor<1x14x1xf32>
        %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %73 = ttir.empty() : tensor<1x1x1xf32>
        %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %75 = ttir.empty() : tensor<1x14x1xf32>
        %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %77 = ttir.empty() : tensor<1x14x1xf32>
        %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %79 = ttir.empty() : tensor<1x14x1xf32>
        %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %81 = ttir.empty() : tensor<1x14x4096xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %83 = ttir.empty() : tensor<1x14x4096xf32>
        %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %85 = ttir.empty() : tensor<1x14x4096xf32>
        %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %87 = ttir.empty() : tensor<14x4096xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %89 = ttir.empty() : tensor<4096x128xf32>
        %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %92 = ttir.empty() : tensor<1x14x1x128xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
        %94 = ttir.empty() : tensor<1x1x14x128xf32>
        %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %96 = ttir.empty() : tensor<1x64x1xf32>
        %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
        %98 = ttir.empty() : tensor<14xf32>
        %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
        %100 = ttir.empty() : tensor<1x1x14xf32>
        %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
        %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
        %103 = ttir.empty() : tensor<1x14x64xf32>
        %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
        %105 = ttir.empty() : tensor<1x14x128xf32>
        %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %107 = ttir.empty() : tensor<1x14x128xf32>
        %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %109 = ttir.empty() : tensor<1x1x14x128xf32>
        %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %111 = ttir.empty() : tensor<1x1x14x128xf32>
        %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %113 = ttir.empty() : tensor<1x1x14x64xf32>
        %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %115 = ttir.empty() : tensor<1x1x14x64xf32>
        %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %117 = ttir.empty() : tensor<1x1x14x64xf32>
        %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %119 = ttir.empty() : tensor<1x1x14x128xf32>
        %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %121 = ttir.empty() : tensor<1x14x128xf32>
        %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %123 = ttir.empty() : tensor<1x1x14x128xf32>
        %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %125 = ttir.empty() : tensor<1x1x14x128xf32>
        %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %127 = ttir.empty() : tensor<1x1x14x128xf32>
        %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %129 = ttir.empty() : tensor<1x1x14x128xbf16>
        %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %131 = ttir.empty() : tensor<1x1x19x128xbf16>
        %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %133 = ttir.empty() : tensor<4096x128xf32>
        %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %136 = ttir.empty() : tensor<14x128xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
        %138 = ttir.empty() : tensor<1x14x1x128xbf16>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
        %140 = ttir.empty() : tensor<1x1x14x128xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %142 = ttir.empty() : tensor<1x1x19x128xbf16>
        %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %144 = ttir.empty() : tensor<1x1x4096xf32>
        %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %146 = ttir.empty() : tensor<1x14x4096xf32>
        %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %148 = ttir.empty() : tensor<4096x512xf32>
        %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
        %151 = ttir.empty() : tensor<1x14x4x128xf32>
        %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %153 = ttir.empty() : tensor<1x4x14x128xf32>
        %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %155 = ttir.empty() : tensor<1x1x14x128xf32>
        %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %157 = ttir.empty() : tensor<1x4x14x128xf32>
        %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %159 = ttir.empty() : tensor<1x4x14x128xf32>
        %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %161 = ttir.empty() : tensor<1x4x14x64xf32>
        %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %163 = ttir.empty() : tensor<1x4x14x64xf32>
        %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %165 = ttir.empty() : tensor<1x4x14x64xf32>
        %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %167 = ttir.empty() : tensor<1x4x14x128xf32>
        %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %169 = ttir.empty() : tensor<1x1x14x128xf32>
        %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %171 = ttir.empty() : tensor<1x4x14x128xf32>
        %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %173 = ttir.empty() : tensor<1x4x14x128xf32>
        %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %175 = ttir.empty() : tensor<1x4x14x128xf32>
        %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %177 = ttir.empty() : tensor<4x14x128xf32>
        %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
        %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %185 = ttir.empty() : tensor<1x4x19x128xf32>
        %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
        %187 = ttir.empty() : tensor<1x4x128x19xf32>
        %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
        %189 = ttir.empty() : tensor<4x128x19xf32>
        %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
        %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
        %192 = ttir.empty() : tensor<1x4x14x19xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x4x14x19xf32>
        %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %198 = ttir.empty() : tensor<1x4x14x19xf32>
        %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
        %201 = ttir.empty() : tensor<14x1xsi32>
        %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %203 = ttir.empty() : tensor<14x19xsi32>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
        %206 = ttir.empty() : tensor<1x19xsi32>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %208 = ttir.empty() : tensor<14x19xsi32>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %210 = ttir.empty() : tensor<14x19xbf16>
        %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %212 = ttir.empty() : tensor<1x1xf32>
        %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %214 = ttir.empty() : tensor<14x19xf32>
        %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %216 = ttir.empty() : tensor<14x19xf32>
        %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %218 = ttir.empty() : tensor<1x19xsi32>
        %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %220 = ttir.empty() : tensor<14x19xsi32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %222 = ttir.empty() : tensor<14x1xsi32>
        %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %224 = ttir.empty() : tensor<14x19xsi32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %226 = ttir.empty() : tensor<14x19xbf16>
        %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %228 = ttir.empty() : tensor<14x19xf32>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %230 = ttir.empty() : tensor<14x19xf32>
        %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %232 = ttir.empty() : tensor<1x1x14x19xf32>
        %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
        %234 = ttir.empty() : tensor<1x4x14x19xf32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %236 = ttir.empty() : tensor<1x4x14x19xf32>
        %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %238 = ttir.empty() : tensor<1x4x14xf32>
        %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %240 = ttir.empty() : tensor<1x4x14x1xf32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %242 = ttir.empty() : tensor<1x4x14x19xf32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %244 = ttir.empty() : tensor<1x4x14x19xf32>
        %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %246 = ttir.empty() : tensor<1x4x14x19xf32>
        %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %248 = ttir.empty() : tensor<1x4x14xf32>
        %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %250 = ttir.empty() : tensor<1x4x14x1xf32>
        %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %252 = ttir.empty() : tensor<1x4x14x19xf32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %254 = ttir.empty() : tensor<1x4x14x19xf32>
        %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %256 = ttir.empty() : tensor<4x14x19xf32>
        %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
        %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %264 = ttir.empty() : tensor<4x19x128xf32>
        %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
        %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
        %267 = ttir.empty() : tensor<1x4x14x128xf32>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %269 = ttir.empty() : tensor<1x14x4x128xf32>
        %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %271 = ttir.empty() : tensor<14x512xf32>
        %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
        %273 = ttir.empty() : tensor<512x4096xf32>
        %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
        %276 = ttir.empty() : tensor<14x4096xf32>
        %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %278 = ttir.empty() : tensor<1x14x4096xf32>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %280 = ttir.empty() : tensor<1x14x4096xf32>
        %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %282 = ttir.empty() : tensor<1x1x4096xf32>
        %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %284 = ttir.empty() : tensor<1x14x4096xf32>
        %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %286 = ttir.empty() : tensor<1x14x4096xf32>
        %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %288 = ttir.empty() : tensor<1x14xf32>
        %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %290 = ttir.empty() : tensor<1x14xf32>
        %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %292 = ttir.empty() : tensor<1x14x1xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %294 = ttir.empty() : tensor<1x14x1xf32>
        %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %296 = ttir.empty() : tensor<1x14x1xf32>
        %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %298 = ttir.empty() : tensor<1x14x4096xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %300 = ttir.empty() : tensor<1x14x4096xf32>
        %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %302 = ttir.empty() : tensor<1x14x4096xf32>
        %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %304 = ttir.empty() : tensor<14x4096xf32>
        %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %306 = ttir.empty() : tensor<4096x1792xf32>
        %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %309 = ttir.empty() : tensor<1x14x1792xf32>
        %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %311 = ttir.empty() : tensor<1x14x1792xf32>
        %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %313 = ttir.empty() : tensor<1x14x1792xf32>
        %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %315 = ttir.empty() : tensor<4096x1792xf32>
        %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %318 = ttir.empty() : tensor<1x14x1792xf32>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %320 = ttir.empty() : tensor<1x14x1792xf32>
        %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %322 = ttir.empty() : tensor<14x1792xf32>
        %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
        %324 = ttir.empty() : tensor<1792x4096xf32>
        %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
        %327 = ttir.empty() : tensor<14x4096xf32>
        %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %329 = ttir.empty() : tensor<1x14x4096xf32>
        %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %331 = ttir.empty() : tensor<1x14x4096xf32>
        %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %333 = ttir.empty() : tensor<1x14x4096xf32>
        %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %335 = ttir.empty() : tensor<1x14xf32>
        %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %337 = ttir.empty() : tensor<1x14xf32>
        %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %339 = ttir.empty() : tensor<1x14x1xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %341 = ttir.empty() : tensor<1x14x1xf32>
        %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %343 = ttir.empty() : tensor<1x14x1xf32>
        %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %345 = ttir.empty() : tensor<1x14x4096xf32>
        %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %347 = ttir.empty() : tensor<1x14x4096xf32>
        %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %349 = ttir.empty() : tensor<1x14x4096xf32>
        %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %351 = ttir.empty() : tensor<14x4096xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %353 = ttir.empty() : tensor<4096x16032xf32>
        %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
        %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
        %356 = ttir.empty() : tensor<1x14x16032xf32>
        %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
        %358 = ttir.empty() : tensor<1x14x4096xf32>
        %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %360 = ttir.empty() : tensor<1x8x19x128xbf16>
        %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %362 = ttir.empty() : tensor<1x8x19x128xbf16>
        %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %364 = ttir.empty() : tensor<1x14x4096xf32>
        %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %366 = ttir.empty() : tensor<14x128256xf32>
        %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
        %368 = ttir.empty() : tensor<1x14x128256xf32>
        %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
        return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
      }
    }
  }
}


ttmlir-opt: /localdev/hshah/tt-mlir/lib/Dialect/TTCore/IR/TTCoreOpsTypes.cpp:1143: static DeviceAttr mlir::tt::ttcore::DeviceAttr::get(::mlir::MLIRContext *, SystemDescAttr, ArrayRef<int64_t>): Assertion `systemDesc.getChipDescIndices().size() >= static_cast<size_t>(numChips) && "expected at least one chip"' failed.
PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace.
Stack dump:
0.	Program arguments: ttmlir-opt --ttir-to-ttnn-backend-pipeline llama_ttir.mlir -o llama_ttnn.mlir --mlir-print-ir-after-all
 #0 0x0000558add5c8bcc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x3272bcc)
 #1 0x0000558add5c903e SignalHandler(int, siginfo_t*, void*) Signals.cpp:0:0
 #2 0x00007f4165a1a520 (/lib/x86_64-linux-gnu/libc.so.6+0x42520)
 #3 0x00007f4165a6e9fc pthread_kill (/lib/x86_64-linux-gnu/libc.so.6+0x969fc)
 #4 0x00007f4165a1a476 gsignal (/lib/x86_64-linux-gnu/libc.so.6+0x42476)
 #5 0x00007f4165a007f3 abort (/lib/x86_64-linux-gnu/libc.so.6+0x287f3)
 #6 0x00007f4165a0071b (/lib/x86_64-linux-gnu/libc.so.6+0x2871b)
 #7 0x00007f4165a11e96 (/lib/x86_64-linux-gnu/libc.so.6+0x39e96)
 #8 0x0000558add3c11b8 mlir::tt::ttcore::DeviceAttr::get(mlir::MLIRContext*, mlir::tt::ttcore::SystemDescAttr, llvm::ArrayRef<long>) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x306b1b8)
 #9 0x0000558adcbb4ebe mlir::tt::ttcore::registerDeviceInSymbolTable(mlir::ModuleOp, llvm::ArrayRef<long>) TTCoreRegisterDevice.cpp:0:0
#10 0x0000558adcbb4c8a mlir::tt::ttcore::registerDevice(mlir::ModuleOp, mlir::tt::ttcore::Arch, llvm::ArrayRef<long>) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x285ec8a)
#11 0x0000558adcbb5cb7 mlir::tt::ttcore::(anonymous namespace)::TTCoreRegisterDevicePass::runOnOperation() TTCoreRegisterDevice.cpp:0:0
#12 0x0000558add46eb98 void llvm::function_ref<void ()>::callback_fn<mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int)::$_1>(long) Pass.cpp:0:0
#13 0x0000558add46b3f7 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31153f7)
#14 0x0000558add46b7df mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31157df)
#15 0x0000558add46fdb7 auto void mlir::parallelForEach<__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::$_0>(mlir::MLIRContext*, __gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, __gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::$_0&&)::'lambda'(__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>&&)::operator()<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo&>(__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>&&) const Pass.cpp:0:0
#16 0x0000558add46c6c0 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31166c0)
#17 0x0000558add46eb8a void llvm::function_ref<void ()>::callback_fn<mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int)::$_1>(long) Pass.cpp:0:0
#18 0x0000558add46b3f7 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31153f7)
#19 0x0000558add46b7df mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31157df)
#20 0x0000558add46fdb7 auto void mlir::parallelForEach<__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::$_0>(mlir::MLIRContext*, __gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, __gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::$_0&&)::'lambda'(__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>&&)::operator()<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo&>(__gnu_cxx::__normal_iterator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo*, std::vector<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo, std::allocator<mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo>>>&&) const Pass.cpp:0:0
#21 0x0000558add46c6c0 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31166c0)
#22 0x0000558add46eb8a void llvm::function_ref<void ()>::callback_fn<mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int)::$_1>(long) Pass.cpp:0:0
#23 0x0000558add46b3f7 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31153f7)
#24 0x0000558add46b7df mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31157df)
#25 0x0000558add46d0d8 mlir::PassManager::run(mlir::Operation*) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x31170d8)
#26 0x0000558adb654ea1 performActions(llvm::raw_ostream&, std::shared_ptr<llvm::SourceMgr> const&, mlir::MLIRContext*, mlir::MlirOptMainConfig const&) MlirOptMain.cpp:0:0
#27 0x0000558adb654b61 llvm::LogicalResult llvm::function_ref<llvm::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&)::$_0>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0
#28 0x0000558add57eab9 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<llvm::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, llvm::StringRef, llvm::StringRef) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x3228ab9)
#29 0x0000558adb64d9d7 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x12f79d7)
#30 0x0000558adb64dc0e mlir::MlirOptMain(int, char**, llvm::StringRef, llvm::StringRef, mlir::DialectRegistry&) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x12f7c0e)
#31 0x0000558adb64ddb8 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x12f7db8)
#32 0x0000558adb64349f main (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x12ed49f)
#33 0x00007f4165a01d90 (/lib/x86_64-linux-gnu/libc.so.6+0x29d90)
#34 0x00007f4165a01e40 __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e40)
#35 0x0000558adb6432d5 _start (/localdev/hshah/tt-mlir/build/bin/ttmlir-opt+0x12ed2d5)
