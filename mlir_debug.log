// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<1x14xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xi64>}> : () -> tensor<19xi64>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xi64>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xi64>, tensor<1x14xi64>) -> tensor<1x14xi64>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xi64>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xi64>, tensor<14xi64>) -> tensor<14xi64>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xi64>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xi64>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xi64>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xi64>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xi32>
    %201 = ttir.empty() : tensor<14x1xi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xi32>, tensor<14x1xi32>) -> tensor<14x1xi32>
    %203 = ttir.empty() : tensor<14x19xi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xi32>, tensor<14x19xi32>) -> tensor<14x19xi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xi32>
    %206 = ttir.empty() : tensor<1x19xi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xi32>, tensor<1x19xi32>) -> tensor<1x19xi32>
    %208 = ttir.empty() : tensor<14x19xi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xi32>, tensor<14x19xi32>) -> tensor<14x19xi32>
    %210 = ttir.empty() : tensor<14x19xi1>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xi32>, tensor<14x19xi32>, tensor<14x19xi1>) -> tensor<14x19xi1>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xi1>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xi64>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xi64>, tensor<1x19xi64>) -> tensor<1x19xi64>
    %220 = ttir.empty() : tensor<14x19xi64>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xi64>, tensor<14x19xi64>) -> tensor<14x19xi64>
    %222 = ttir.empty() : tensor<14x1xi64>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xi64>, tensor<14x1xi64>) -> tensor<14x1xi64>
    %224 = ttir.empty() : tensor<14x19xi64>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xi64>, tensor<14x19xi64>) -> tensor<14x19xi64>
    %226 = ttir.empty() : tensor<14x19xi1>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xi64>, tensor<14x19xi64>, tensor<14x19xi1>) -> tensor<14x19xi1>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xi1>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}


// -----// IR Dump After ElementTypeNormalization (ttir-element-type-normalization) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %201 = ttir.empty() : tensor<14x1xsi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %203 = ttir.empty() : tensor<14x19xsi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %206 = ttir.empty() : tensor<1x19xsi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %208 = ttir.empty() : tensor<14x19xsi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %210 = ttir.empty() : tensor<14x19xbf16>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xsi32>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %220 = ttir.empty() : tensor<14x19xsi32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %222 = ttir.empty() : tensor<14x1xsi32>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %224 = ttir.empty() : tensor<14x19xsi32>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %226 = ttir.empty() : tensor<14x19xbf16>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}


// -----// IR Dump After TTCoreWrapDeviceModulePass (ttcore-wrap-device-module) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
        %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %5 = ttir.empty() : tensor<1x14xsi32>
        %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
        %7 = ttir.empty() : tensor<128256x4096xf32>
        %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
        %9 = ttir.empty() : tensor<14xsi32>
        %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
        %11 = ttir.empty() : tensor<64xf32>
        %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
        %13 = ttir.empty() : tensor<128x4096xf32>
        %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %15 = ttir.empty() : tensor<f32>
        %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %17 = ttir.empty() : tensor<4096xf32>
        %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %19 = ttir.empty() : tensor<128x4096xf32>
        %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %21 = ttir.empty() : tensor<4096x1792xf32>
        %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %23 = ttir.empty() : tensor<1792x4096xf32>
        %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %25 = ttir.empty() : tensor<4096x512xf32>
        %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %27 = ttir.empty() : tensor<f32>
        %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %29 = ttir.empty() : tensor<f32>
        %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %31 = ttir.empty() : tensor<512x4096xf32>
        %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %33 = ttir.empty() : tensor<4096xf32>
        %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %35 = ttir.empty() : tensor<1792x4096xf32>
        %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %37 = ttir.empty() : tensor<4096xf32>
        %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %39 = ttir.empty() : tensor<16032x4096xf32>
        %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
        %41 = ttir.empty() : tensor<1x1xf32>
        %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %43 = ttir.empty() : tensor<14x19xf32>
        %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %45 = ttir.empty() : tensor<1x1x1xf32>
        %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %47 = ttir.empty() : tensor<1x14x4096xf32>
        %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %49 = ttir.empty() : tensor<1x1x1x1xbf16>
        %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %51 = ttir.empty() : tensor<1x1x19x128xbf16>
        %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %53 = ttir.empty() : tensor<1x14xui32>
        %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
        %55 = ttir.empty() : tensor<14xui32>
        %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
        %57 = ttir.empty() : tensor<14x4096xf32>
        %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %59 = ttir.empty() : tensor<1x14x4096xf32>
        %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %61 = ttir.empty() : tensor<1x1x4096xf32>
        %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %63 = ttir.empty() : tensor<1x14x4096xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %65 = ttir.empty() : tensor<1x14x4096xf32>
        %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %67 = ttir.empty() : tensor<1x14xf32>
        %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %69 = ttir.empty() : tensor<1x14xf32>
        %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %71 = ttir.empty() : tensor<1x14x1xf32>
        %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %73 = ttir.empty() : tensor<1x1x1xf32>
        %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %75 = ttir.empty() : tensor<1x14x1xf32>
        %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %77 = ttir.empty() : tensor<1x14x1xf32>
        %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %79 = ttir.empty() : tensor<1x14x1xf32>
        %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %81 = ttir.empty() : tensor<1x14x4096xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %83 = ttir.empty() : tensor<1x14x4096xf32>
        %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %85 = ttir.empty() : tensor<1x14x4096xf32>
        %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %87 = ttir.empty() : tensor<14x4096xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %89 = ttir.empty() : tensor<4096x128xf32>
        %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %92 = ttir.empty() : tensor<1x14x1x128xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
        %94 = ttir.empty() : tensor<1x1x14x128xf32>
        %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %96 = ttir.empty() : tensor<1x64x1xf32>
        %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
        %98 = ttir.empty() : tensor<14xf32>
        %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
        %100 = ttir.empty() : tensor<1x1x14xf32>
        %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
        %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
        %103 = ttir.empty() : tensor<1x14x64xf32>
        %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
        %105 = ttir.empty() : tensor<1x14x128xf32>
        %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %107 = ttir.empty() : tensor<1x14x128xf32>
        %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %109 = ttir.empty() : tensor<1x1x14x128xf32>
        %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %111 = ttir.empty() : tensor<1x1x14x128xf32>
        %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %113 = ttir.empty() : tensor<1x1x14x64xf32>
        %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %115 = ttir.empty() : tensor<1x1x14x64xf32>
        %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %117 = ttir.empty() : tensor<1x1x14x64xf32>
        %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %119 = ttir.empty() : tensor<1x1x14x128xf32>
        %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %121 = ttir.empty() : tensor<1x14x128xf32>
        %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %123 = ttir.empty() : tensor<1x1x14x128xf32>
        %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %125 = ttir.empty() : tensor<1x1x14x128xf32>
        %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %127 = ttir.empty() : tensor<1x1x14x128xf32>
        %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %129 = ttir.empty() : tensor<1x1x14x128xbf16>
        %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %131 = ttir.empty() : tensor<1x1x19x128xbf16>
        %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %133 = ttir.empty() : tensor<4096x128xf32>
        %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %136 = ttir.empty() : tensor<14x128xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
        %138 = ttir.empty() : tensor<1x14x1x128xbf16>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
        %140 = ttir.empty() : tensor<1x1x14x128xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %142 = ttir.empty() : tensor<1x1x19x128xbf16>
        %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %144 = ttir.empty() : tensor<1x1x4096xf32>
        %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %146 = ttir.empty() : tensor<1x14x4096xf32>
        %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %148 = ttir.empty() : tensor<4096x512xf32>
        %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
        %151 = ttir.empty() : tensor<1x14x4x128xf32>
        %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %153 = ttir.empty() : tensor<1x4x14x128xf32>
        %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %155 = ttir.empty() : tensor<1x1x14x128xf32>
        %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %157 = ttir.empty() : tensor<1x4x14x128xf32>
        %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %159 = ttir.empty() : tensor<1x4x14x128xf32>
        %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %161 = ttir.empty() : tensor<1x4x14x64xf32>
        %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %163 = ttir.empty() : tensor<1x4x14x64xf32>
        %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %165 = ttir.empty() : tensor<1x4x14x64xf32>
        %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %167 = ttir.empty() : tensor<1x4x14x128xf32>
        %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %169 = ttir.empty() : tensor<1x1x14x128xf32>
        %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %171 = ttir.empty() : tensor<1x4x14x128xf32>
        %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %173 = ttir.empty() : tensor<1x4x14x128xf32>
        %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %175 = ttir.empty() : tensor<1x4x14x128xf32>
        %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %177 = ttir.empty() : tensor<4x14x128xf32>
        %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
        %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %185 = ttir.empty() : tensor<1x4x19x128xf32>
        %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
        %187 = ttir.empty() : tensor<1x4x128x19xf32>
        %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
        %189 = ttir.empty() : tensor<4x128x19xf32>
        %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
        %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
        %192 = ttir.empty() : tensor<1x4x14x19xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x4x14x19xf32>
        %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %198 = ttir.empty() : tensor<1x4x14x19xf32>
        %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
        %201 = ttir.empty() : tensor<14x1xsi32>
        %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %203 = ttir.empty() : tensor<14x19xsi32>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
        %206 = ttir.empty() : tensor<1x19xsi32>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %208 = ttir.empty() : tensor<14x19xsi32>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %210 = ttir.empty() : tensor<14x19xbf16>
        %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %212 = ttir.empty() : tensor<1x1xf32>
        %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %214 = ttir.empty() : tensor<14x19xf32>
        %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %216 = ttir.empty() : tensor<14x19xf32>
        %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %218 = ttir.empty() : tensor<1x19xsi32>
        %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %220 = ttir.empty() : tensor<14x19xsi32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %222 = ttir.empty() : tensor<14x1xsi32>
        %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %224 = ttir.empty() : tensor<14x19xsi32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %226 = ttir.empty() : tensor<14x19xbf16>
        %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %228 = ttir.empty() : tensor<14x19xf32>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %230 = ttir.empty() : tensor<14x19xf32>
        %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %232 = ttir.empty() : tensor<1x1x14x19xf32>
        %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
        %234 = ttir.empty() : tensor<1x4x14x19xf32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %236 = ttir.empty() : tensor<1x4x14x19xf32>
        %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %238 = ttir.empty() : tensor<1x4x14xf32>
        %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %240 = ttir.empty() : tensor<1x4x14x1xf32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %242 = ttir.empty() : tensor<1x4x14x19xf32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %244 = ttir.empty() : tensor<1x4x14x19xf32>
        %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %246 = ttir.empty() : tensor<1x4x14x19xf32>
        %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %248 = ttir.empty() : tensor<1x4x14xf32>
        %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %250 = ttir.empty() : tensor<1x4x14x1xf32>
        %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %252 = ttir.empty() : tensor<1x4x14x19xf32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %254 = ttir.empty() : tensor<1x4x14x19xf32>
        %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %256 = ttir.empty() : tensor<4x14x19xf32>
        %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
        %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %264 = ttir.empty() : tensor<4x19x128xf32>
        %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
        %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
        %267 = ttir.empty() : tensor<1x4x14x128xf32>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %269 = ttir.empty() : tensor<1x14x4x128xf32>
        %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %271 = ttir.empty() : tensor<14x512xf32>
        %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
        %273 = ttir.empty() : tensor<512x4096xf32>
        %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
        %276 = ttir.empty() : tensor<14x4096xf32>
        %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %278 = ttir.empty() : tensor<1x14x4096xf32>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %280 = ttir.empty() : tensor<1x14x4096xf32>
        %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %282 = ttir.empty() : tensor<1x1x4096xf32>
        %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %284 = ttir.empty() : tensor<1x14x4096xf32>
        %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %286 = ttir.empty() : tensor<1x14x4096xf32>
        %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %288 = ttir.empty() : tensor<1x14xf32>
        %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %290 = ttir.empty() : tensor<1x14xf32>
        %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %292 = ttir.empty() : tensor<1x14x1xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %294 = ttir.empty() : tensor<1x14x1xf32>
        %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %296 = ttir.empty() : tensor<1x14x1xf32>
        %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %298 = ttir.empty() : tensor<1x14x4096xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %300 = ttir.empty() : tensor<1x14x4096xf32>
        %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %302 = ttir.empty() : tensor<1x14x4096xf32>
        %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %304 = ttir.empty() : tensor<14x4096xf32>
        %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %306 = ttir.empty() : tensor<4096x1792xf32>
        %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %309 = ttir.empty() : tensor<1x14x1792xf32>
        %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %311 = ttir.empty() : tensor<1x14x1792xf32>
        %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %313 = ttir.empty() : tensor<1x14x1792xf32>
        %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %315 = ttir.empty() : tensor<4096x1792xf32>
        %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %318 = ttir.empty() : tensor<1x14x1792xf32>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %320 = ttir.empty() : tensor<1x14x1792xf32>
        %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %322 = ttir.empty() : tensor<14x1792xf32>
        %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
        %324 = ttir.empty() : tensor<1792x4096xf32>
        %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
        %327 = ttir.empty() : tensor<14x4096xf32>
        %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %329 = ttir.empty() : tensor<1x14x4096xf32>
        %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %331 = ttir.empty() : tensor<1x14x4096xf32>
        %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %333 = ttir.empty() : tensor<1x14x4096xf32>
        %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %335 = ttir.empty() : tensor<1x14xf32>
        %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %337 = ttir.empty() : tensor<1x14xf32>
        %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %339 = ttir.empty() : tensor<1x14x1xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %341 = ttir.empty() : tensor<1x14x1xf32>
        %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %343 = ttir.empty() : tensor<1x14x1xf32>
        %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %345 = ttir.empty() : tensor<1x14x4096xf32>
        %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %347 = ttir.empty() : tensor<1x14x4096xf32>
        %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %349 = ttir.empty() : tensor<1x14x4096xf32>
        %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %351 = ttir.empty() : tensor<14x4096xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %353 = ttir.empty() : tensor<4096x16032xf32>
        %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
        %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
        %356 = ttir.empty() : tensor<1x14x16032xf32>
        %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
        %358 = ttir.empty() : tensor<1x14x4096xf32>
        %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %360 = ttir.empty() : tensor<1x8x19x128xbf16>
        %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %362 = ttir.empty() : tensor<1x8x19x128xbf16>
        %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %364 = ttir.empty() : tensor<1x14x4096xf32>
        %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %366 = ttir.empty() : tensor<14x128256xf32>
        %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
        %368 = ttir.empty() : tensor<1x14x128256xf32>
        %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
        return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
      }
    }
  }
}


// -----// IR Dump After TTIRHoistTransform (ttir-cpu-hoist-transform) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
      func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
        %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
        %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
        %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
        %5 = ttir.empty() : tensor<1x14xsi32>
        %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
        %7 = ttir.empty() : tensor<128256x4096xf32>
        %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
        %9 = ttir.empty() : tensor<14xsi32>
        %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
        %11 = ttir.empty() : tensor<64xf32>
        %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
        %13 = ttir.empty() : tensor<128x4096xf32>
        %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %15 = ttir.empty() : tensor<f32>
        %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %17 = ttir.empty() : tensor<4096xf32>
        %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %19 = ttir.empty() : tensor<128x4096xf32>
        %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
        %21 = ttir.empty() : tensor<4096x1792xf32>
        %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %23 = ttir.empty() : tensor<1792x4096xf32>
        %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %25 = ttir.empty() : tensor<4096x512xf32>
        %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %27 = ttir.empty() : tensor<f32>
        %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %29 = ttir.empty() : tensor<f32>
        %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
        %31 = ttir.empty() : tensor<512x4096xf32>
        %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %33 = ttir.empty() : tensor<4096xf32>
        %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %35 = ttir.empty() : tensor<1792x4096xf32>
        %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %37 = ttir.empty() : tensor<4096xf32>
        %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
        %39 = ttir.empty() : tensor<16032x4096xf32>
        %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
        %41 = ttir.empty() : tensor<1x1xf32>
        %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %43 = ttir.empty() : tensor<14x19xf32>
        %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %45 = ttir.empty() : tensor<1x1x1xf32>
        %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %47 = ttir.empty() : tensor<1x14x4096xf32>
        %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %49 = ttir.empty() : tensor<1x1x1x1xbf16>
        %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
        %51 = ttir.empty() : tensor<1x1x19x128xbf16>
        %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %53 = ttir.empty() : tensor<1x14xui32>
        %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
        %55 = ttir.empty() : tensor<14xui32>
        %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
        %57 = ttir.empty() : tensor<14x4096xf32>
        %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %59 = ttir.empty() : tensor<1x14x4096xf32>
        %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %61 = ttir.empty() : tensor<1x1x4096xf32>
        %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %63 = ttir.empty() : tensor<1x14x4096xf32>
        %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %65 = ttir.empty() : tensor<1x14x4096xf32>
        %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %67 = ttir.empty() : tensor<1x14xf32>
        %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %69 = ttir.empty() : tensor<1x14xf32>
        %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %71 = ttir.empty() : tensor<1x14x1xf32>
        %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %73 = ttir.empty() : tensor<1x1x1xf32>
        %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
        %75 = ttir.empty() : tensor<1x14x1xf32>
        %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %77 = ttir.empty() : tensor<1x14x1xf32>
        %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %79 = ttir.empty() : tensor<1x14x1xf32>
        %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %81 = ttir.empty() : tensor<1x14x4096xf32>
        %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %83 = ttir.empty() : tensor<1x14x4096xf32>
        %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %85 = ttir.empty() : tensor<1x14x4096xf32>
        %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %87 = ttir.empty() : tensor<14x4096xf32>
        %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %89 = ttir.empty() : tensor<4096x128xf32>
        %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %92 = ttir.empty() : tensor<1x14x1x128xf32>
        %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
        %94 = ttir.empty() : tensor<1x1x14x128xf32>
        %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %96 = ttir.empty() : tensor<1x64x1xf32>
        %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
        %98 = ttir.empty() : tensor<14xf32>
        %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
        %100 = ttir.empty() : tensor<1x1x14xf32>
        %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
        %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
        %103 = ttir.empty() : tensor<1x14x64xf32>
        %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
        %105 = ttir.empty() : tensor<1x14x128xf32>
        %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %107 = ttir.empty() : tensor<1x14x128xf32>
        %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %109 = ttir.empty() : tensor<1x1x14x128xf32>
        %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %111 = ttir.empty() : tensor<1x1x14x128xf32>
        %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %113 = ttir.empty() : tensor<1x1x14x64xf32>
        %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %115 = ttir.empty() : tensor<1x1x14x64xf32>
        %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %117 = ttir.empty() : tensor<1x1x14x64xf32>
        %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
        %119 = ttir.empty() : tensor<1x1x14x128xf32>
        %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %121 = ttir.empty() : tensor<1x14x128xf32>
        %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
        %123 = ttir.empty() : tensor<1x1x14x128xf32>
        %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %125 = ttir.empty() : tensor<1x1x14x128xf32>
        %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %127 = ttir.empty() : tensor<1x1x14x128xf32>
        %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %129 = ttir.empty() : tensor<1x1x14x128xbf16>
        %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %131 = ttir.empty() : tensor<1x1x19x128xbf16>
        %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %133 = ttir.empty() : tensor<4096x128xf32>
        %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
        %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
        %136 = ttir.empty() : tensor<14x128xbf16>
        %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
        %138 = ttir.empty() : tensor<1x14x1x128xbf16>
        %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
        %140 = ttir.empty() : tensor<1x1x14x128xbf16>
        %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
        %142 = ttir.empty() : tensor<1x1x19x128xbf16>
        %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
        %144 = ttir.empty() : tensor<1x1x4096xf32>
        %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %146 = ttir.empty() : tensor<1x14x4096xf32>
        %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %148 = ttir.empty() : tensor<4096x512xf32>
        %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
        %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
        %151 = ttir.empty() : tensor<1x14x4x128xf32>
        %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %153 = ttir.empty() : tensor<1x4x14x128xf32>
        %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %155 = ttir.empty() : tensor<1x1x14x128xf32>
        %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %157 = ttir.empty() : tensor<1x4x14x128xf32>
        %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %159 = ttir.empty() : tensor<1x4x14x128xf32>
        %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %161 = ttir.empty() : tensor<1x4x14x64xf32>
        %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %163 = ttir.empty() : tensor<1x4x14x64xf32>
        %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %165 = ttir.empty() : tensor<1x4x14x64xf32>
        %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
        %167 = ttir.empty() : tensor<1x4x14x128xf32>
        %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %169 = ttir.empty() : tensor<1x1x14x128xf32>
        %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
        %171 = ttir.empty() : tensor<1x4x14x128xf32>
        %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %173 = ttir.empty() : tensor<1x4x14x128xf32>
        %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %175 = ttir.empty() : tensor<1x4x14x128xf32>
        %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %177 = ttir.empty() : tensor<4x14x128xf32>
        %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
        %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %185 = ttir.empty() : tensor<1x4x19x128xf32>
        %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
        %187 = ttir.empty() : tensor<1x4x128x19xf32>
        %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
        %189 = ttir.empty() : tensor<4x128x19xf32>
        %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
        %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
        %192 = ttir.empty() : tensor<1x4x14x19xf32>
        %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %194 = ttir.empty() : tensor<1x1x1x1xf32>
        %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
        %196 = ttir.empty() : tensor<1x4x14x19xf32>
        %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %198 = ttir.empty() : tensor<1x4x14x19xf32>
        %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
        %201 = ttir.empty() : tensor<14x1xsi32>
        %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %203 = ttir.empty() : tensor<14x19xsi32>
        %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
        %206 = ttir.empty() : tensor<1x19xsi32>
        %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %208 = ttir.empty() : tensor<14x19xsi32>
        %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %210 = ttir.empty() : tensor<14x19xbf16>
        %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %212 = ttir.empty() : tensor<1x1xf32>
        %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
        %214 = ttir.empty() : tensor<14x19xf32>
        %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %216 = ttir.empty() : tensor<14x19xf32>
        %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %218 = ttir.empty() : tensor<1x19xsi32>
        %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
        %220 = ttir.empty() : tensor<14x19xsi32>
        %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %222 = ttir.empty() : tensor<14x1xsi32>
        %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
        %224 = ttir.empty() : tensor<14x19xsi32>
        %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
        %226 = ttir.empty() : tensor<14x19xbf16>
        %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
        %228 = ttir.empty() : tensor<14x19xf32>
        %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %230 = ttir.empty() : tensor<14x19xf32>
        %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
        %232 = ttir.empty() : tensor<1x1x14x19xf32>
        %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
        %234 = ttir.empty() : tensor<1x4x14x19xf32>
        %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %236 = ttir.empty() : tensor<1x4x14x19xf32>
        %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %238 = ttir.empty() : tensor<1x4x14xf32>
        %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %240 = ttir.empty() : tensor<1x4x14x1xf32>
        %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %242 = ttir.empty() : tensor<1x4x14x19xf32>
        %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %244 = ttir.empty() : tensor<1x4x14x19xf32>
        %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %246 = ttir.empty() : tensor<1x4x14x19xf32>
        %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %248 = ttir.empty() : tensor<1x4x14xf32>
        %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
        %250 = ttir.empty() : tensor<1x4x14x1xf32>
        %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
        %252 = ttir.empty() : tensor<1x4x14x19xf32>
        %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %254 = ttir.empty() : tensor<1x4x14x19xf32>
        %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
        %256 = ttir.empty() : tensor<4x14x19xf32>
        %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
        %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
        %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
        %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
        %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
        %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
        %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
        %264 = ttir.empty() : tensor<4x19x128xf32>
        %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
        %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
        %267 = ttir.empty() : tensor<1x4x14x128xf32>
        %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
        %269 = ttir.empty() : tensor<1x14x4x128xf32>
        %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
        %271 = ttir.empty() : tensor<14x512xf32>
        %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
        %273 = ttir.empty() : tensor<512x4096xf32>
        %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
        %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
        %276 = ttir.empty() : tensor<14x4096xf32>
        %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %278 = ttir.empty() : tensor<1x14x4096xf32>
        %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %280 = ttir.empty() : tensor<1x14x4096xf32>
        %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %282 = ttir.empty() : tensor<1x1x4096xf32>
        %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
        %284 = ttir.empty() : tensor<1x14x4096xf32>
        %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %286 = ttir.empty() : tensor<1x14x4096xf32>
        %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %288 = ttir.empty() : tensor<1x14xf32>
        %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %290 = ttir.empty() : tensor<1x14xf32>
        %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %292 = ttir.empty() : tensor<1x14x1xf32>
        %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %294 = ttir.empty() : tensor<1x14x1xf32>
        %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %296 = ttir.empty() : tensor<1x14x1xf32>
        %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %298 = ttir.empty() : tensor<1x14x4096xf32>
        %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %300 = ttir.empty() : tensor<1x14x4096xf32>
        %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %302 = ttir.empty() : tensor<1x14x4096xf32>
        %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %304 = ttir.empty() : tensor<14x4096xf32>
        %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %306 = ttir.empty() : tensor<4096x1792xf32>
        %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %309 = ttir.empty() : tensor<1x14x1792xf32>
        %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %311 = ttir.empty() : tensor<1x14x1792xf32>
        %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %313 = ttir.empty() : tensor<1x14x1792xf32>
        %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %315 = ttir.empty() : tensor<4096x1792xf32>
        %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
        %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
        %318 = ttir.empty() : tensor<1x14x1792xf32>
        %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %320 = ttir.empty() : tensor<1x14x1792xf32>
        %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
        %322 = ttir.empty() : tensor<14x1792xf32>
        %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
        %324 = ttir.empty() : tensor<1792x4096xf32>
        %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
        %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
        %327 = ttir.empty() : tensor<14x4096xf32>
        %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %329 = ttir.empty() : tensor<1x14x4096xf32>
        %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %331 = ttir.empty() : tensor<1x14x4096xf32>
        %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %333 = ttir.empty() : tensor<1x14x4096xf32>
        %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %335 = ttir.empty() : tensor<1x14xf32>
        %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %337 = ttir.empty() : tensor<1x14xf32>
        %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
        %339 = ttir.empty() : tensor<1x14x1xf32>
        %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %341 = ttir.empty() : tensor<1x14x1xf32>
        %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %343 = ttir.empty() : tensor<1x14x1xf32>
        %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
        %345 = ttir.empty() : tensor<1x14x4096xf32>
        %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %347 = ttir.empty() : tensor<1x14x4096xf32>
        %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %349 = ttir.empty() : tensor<1x14x4096xf32>
        %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %351 = ttir.empty() : tensor<14x4096xf32>
        %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
        %353 = ttir.empty() : tensor<4096x16032xf32>
        %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
        %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
        %356 = ttir.empty() : tensor<1x14x16032xf32>
        %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
        %358 = ttir.empty() : tensor<1x14x4096xf32>
        %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %360 = ttir.empty() : tensor<1x8x19x128xbf16>
        %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %362 = ttir.empty() : tensor<1x8x19x128xbf16>
        %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
        %364 = ttir.empty() : tensor<1x14x4096xf32>
        %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
        %366 = ttir.empty() : tensor<14x128256xf32>
        %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
        %368 = ttir.empty() : tensor<1x14x128256xf32>
        %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
        return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
      }
    }
  }
}


[HET DEBUG GET] numChips: 8
[HET DEBUG GET] systemDesc.getChipDescIndices().size(): 8
// -----// IR Dump After TTCoreRegisterDevicePass (ttcore-register-device) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %201 = ttir.empty() : tensor<14x1xsi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %203 = ttir.empty() : tensor<14x19xsi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %206 = ttir.empty() : tensor<1x19xsi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %208 = ttir.empty() : tensor<14x19xsi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %210 = ttir.empty() : tensor<14x19xbf16>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xsi32>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %220 = ttir.empty() : tensor<14x19xsi32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %222 = ttir.empty() : tensor<14x1xsi32>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %224 = ttir.empty() : tensor<14x19xsi32>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %226 = ttir.empty() : tensor<14x19xbf16>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

llama_ttir.mlir:1:1: warning: Empty argument type map provided. Skipping argument type population. This may affect subsequent compile steps
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} {
^
// -----// IR Dump After TTPopulateArgumentTypes (tt-populate-argument-types) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %201 = ttir.empty() : tensor<14x1xsi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %203 = ttir.empty() : tensor<14x19xsi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %206 = ttir.empty() : tensor<1x19xsi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %208 = ttir.empty() : tensor<14x19xsi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %210 = ttir.empty() : tensor<14x19xbf16>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xsi32>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %220 = ttir.empty() : tensor<14x19xsi32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %222 = ttir.empty() : tensor<14x1xsi32>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %224 = ttir.empty() : tensor<14x19xsi32>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %226 = ttir.empty() : tensor<14x19xbf16>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = ttir.empty() : tensor<1x1x19x128xbf16>
    %132 = "ttir.scatter"(%52, %10, %130, %131) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<4096x128xf32>
    %134 = "ttir.permute"(%20, %133) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %135 = "ttir.dot_general"(%88, %134) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %136 = ttir.empty() : tensor<14x128xbf16>
    %137 = "ttir.typecast"(%135, %136) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %138 = ttir.empty() : tensor<1x14x1x128xbf16>
    %139 = "ttir.reshape"(%137, %138) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %140 = ttir.empty() : tensor<1x1x14x128xbf16>
    %141 = "ttir.permute"(%139, %140) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %142 = ttir.empty() : tensor<1x1x19x128xbf16>
    %143 = "ttir.scatter"(%52, %10, %141, %142) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xsi32>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %144 = ttir.empty() : tensor<1x1x4096xf32>
    %145 = "ttir.reshape"(%38, %144) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %146 = ttir.empty() : tensor<1x14x4096xf32>
    %147 = "ttir.broadcast"(%145, %146) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %148 = ttir.empty() : tensor<4096x512xf32>
    %149 = "ttir.permute"(%32, %148) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %150 = "ttir.dot_general"(%88, %149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %151 = ttir.empty() : tensor<1x14x4x128xf32>
    %152 = "ttir.reshape"(%150, %151) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.permute"(%152, %153) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x1x14x128xf32>
    %156 = "ttir.reshape"(%108, %155) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.broadcast"(%156, %157) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.multiply"(%154, %158, %159) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%154, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.neg"(%162, %163) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.slice"(%154, %165) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.concat"(%164, %166, %167) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x1x14x128xf32>
    %170 = "ttir.reshape"(%122, %169) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.broadcast"(%170, %171) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.multiply"(%168, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.add"(%160, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<4x14x128xf32>
    %178 = "ttir.reshape"(%176, %177) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %180 = "ttir.reshape"(%132, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %184 = "ttir.typecast"(%182, %183) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x19x128xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x128x19xf32>
    %188 = "ttir.permute"(%186, %187) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %189 = ttir.empty() : tensor<4x128x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %191 = "ttir.dot_general"(%178, %190) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %192 = ttir.empty() : tensor<1x4x14x19xf32>
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %194 = ttir.empty() : tensor<1x1x1x1xf32>
    %195 = "ttir.reshape"(%30, %194) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = ttir.empty() : tensor<1x4x14x19xf32>
    %199 = "ttir.multiply"(%193, %197, %198) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %200 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %201 = ttir.empty() : tensor<14x1xsi32>
    %202 = "ttir.reshape"(%200, %201) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %203 = ttir.empty() : tensor<14x19xsi32>
    %204 = "ttir.broadcast"(%202, %203) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %205 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %206 = ttir.empty() : tensor<1x19xsi32>
    %207 = "ttir.reshape"(%205, %206) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %208 = ttir.empty() : tensor<14x19xsi32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %210 = ttir.empty() : tensor<14x19xbf16>
    %211 = "ttir.ge"(%204, %209, %210) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %212 = ttir.empty() : tensor<1x1xf32>
    %213 = "ttir.reshape"(%28, %212) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.broadcast"(%213, %214) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<14x19xf32>
    %217 = "ttir.where"(%211, %44, %215, %216) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %218 = ttir.empty() : tensor<1x19xsi32>
    %219 = "ttir.reshape"(%0, %218) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %220 = ttir.empty() : tensor<14x19xsi32>
    %221 = "ttir.broadcast"(%219, %220) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %222 = ttir.empty() : tensor<14x1xsi32>
    %223 = "ttir.reshape"(%10, %222) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %224 = ttir.empty() : tensor<14x19xsi32>
    %225 = "ttir.broadcast"(%223, %224) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %226 = ttir.empty() : tensor<14x19xbf16>
    %227 = "ttir.gt"(%221, %225, %226) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.typecast"(%227, %228) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<14x19xf32>
    %231 = "ttir.multiply"(%217, %229, %230) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %232 = ttir.empty() : tensor<1x1x14x19xf32>
    %233 = "ttir.reshape"(%231, %232) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.broadcast"(%233, %234) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x19xf32>
    %237 = "ttir.add"(%199, %235, %236) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %238 = ttir.empty() : tensor<1x4x14xf32>
    %239 = "ttir.max"(%237, %238) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %240 = ttir.empty() : tensor<1x4x14x1xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.broadcast"(%241, %242) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<1x4x14x19xf32>
    %245 = "ttir.subtract"(%237, %243, %244) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %246 = ttir.empty() : tensor<1x4x14x19xf32>
    %247 = "ttir.exp"(%245, %246) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %248 = ttir.empty() : tensor<1x4x14xf32>
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x4x14x19xf32>, tensor<1x4x14xf32>) -> tensor<1x4x14xf32>
    %250 = ttir.empty() : tensor<1x4x14x1xf32>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 4 : i32, 14 : i32, 1 : i32]}> : (tensor<1x4x14xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %252 = ttir.empty() : tensor<1x4x14x19xf32>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.div"(%247, %253, %254) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<4x14x19xf32>
    %257 = "ttir.reshape"(%255, %256) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %258 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %259 = "ttir.reshape"(%143, %258) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %260 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %261 = "ttir.broadcast"(%259, %260) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %262 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %264 = ttir.empty() : tensor<4x19x128xf32>
    %265 = "ttir.reshape"(%263, %264) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %266 = "ttir.dot_general"(%257, %265) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %267 = ttir.empty() : tensor<1x4x14x128xf32>
    %268 = "ttir.reshape"(%266, %267) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %269 = ttir.empty() : tensor<1x14x4x128xf32>
    %270 = "ttir.permute"(%268, %269) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %271 = ttir.empty() : tensor<14x512xf32>
    %272 = "ttir.reshape"(%270, %271) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %273 = ttir.empty() : tensor<512x4096xf32>
    %274 = "ttir.permute"(%26, %273) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %275 = "ttir.dot_general"(%272, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %276 = ttir.empty() : tensor<14x4096xf32>
    %277 = "ttir.all_reduce"(%275, %276) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %278 = ttir.empty() : tensor<1x14x4096xf32>
    %279 = "ttir.reshape"(%277, %278) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %280 = ttir.empty() : tensor<1x14x4096xf32>
    %281 = "ttir.add"(%60, %279, %280) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %282 = ttir.empty() : tensor<1x1x4096xf32>
    %283 = "ttir.reshape"(%34, %282) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %284 = ttir.empty() : tensor<1x14x4096xf32>
    %285 = "ttir.broadcast"(%283, %284) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.pow"(%281, %48, %286) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14xf32>
    %289 = "ttir.sum"(%287, %288) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %290 = ttir.empty() : tensor<1x14xf32>
    %291 = "ttir.multiply"(%289, %2, %290) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %292 = ttir.empty() : tensor<1x14x1xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %294 = ttir.empty() : tensor<1x14x1xf32>
    %295 = "ttir.add"(%293, %76, %294) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %296 = ttir.empty() : tensor<1x14x1xf32>
    %297 = "ttir.rsqrt"(%295, %296) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %298 = ttir.empty() : tensor<1x14x4096xf32>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %300 = ttir.empty() : tensor<1x14x4096xf32>
    %301 = "ttir.multiply"(%281, %299, %300) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %302 = ttir.empty() : tensor<1x14x4096xf32>
    %303 = "ttir.multiply"(%285, %301, %302) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %304 = ttir.empty() : tensor<14x4096xf32>
    %305 = "ttir.reshape"(%303, %304) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %306 = ttir.empty() : tensor<4096x1792xf32>
    %307 = "ttir.permute"(%36, %306) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %308 = "ttir.dot_general"(%305, %307) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.reshape"(%308, %309) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<1x14x1792xf32>
    %312 = "ttir.sigmoid"(%310, %311) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %313 = ttir.empty() : tensor<1x14x1792xf32>
    %314 = "ttir.multiply"(%310, %312, %313) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %315 = ttir.empty() : tensor<4096x1792xf32>
    %316 = "ttir.permute"(%24, %315) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %317 = "ttir.dot_general"(%305, %316) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %318 = ttir.empty() : tensor<1x14x1792xf32>
    %319 = "ttir.reshape"(%317, %318) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %320 = ttir.empty() : tensor<1x14x1792xf32>
    %321 = "ttir.multiply"(%314, %319, %320) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %322 = ttir.empty() : tensor<14x1792xf32>
    %323 = "ttir.reshape"(%321, %322) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %324 = ttir.empty() : tensor<1792x4096xf32>
    %325 = "ttir.permute"(%22, %324) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %326 = "ttir.dot_general"(%323, %325) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<14x4096xf32>
    %328 = "ttir.all_reduce"(%326, %327) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.reshape"(%328, %329) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.add"(%281, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.pow"(%332, %48, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.sum"(%334, %335) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14xf32>
    %338 = "ttir.multiply"(%336, %2, %337) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.add"(%340, %76, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x1xf32>
    %344 = "ttir.rsqrt"(%342, %343) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.broadcast"(%344, %345) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%332, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<1x14x4096xf32>
    %350 = "ttir.multiply"(%147, %348, %349) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %351 = ttir.empty() : tensor<14x4096xf32>
    %352 = "ttir.reshape"(%350, %351) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %353 = ttir.empty() : tensor<4096x16032xf32>
    %354 = "ttir.permute"(%40, %353) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %355 = "ttir.dot_general"(%352, %354) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %356 = ttir.empty() : tensor<1x14x16032xf32>
    %357 = "ttir.reshape"(%355, %356) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.mesh_shard"(%60, %358) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x8x19x128xbf16>
    %361 = "ttir.mesh_shard"(%132, %360) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %362 = ttir.empty() : tensor<1x8x19x128xbf16>
    %363 = "ttir.mesh_shard"(%143, %362) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %364 = ttir.empty() : tensor<1x14x4096xf32>
    %365 = "ttir.mesh_shard"(%350, %364) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %366 = ttir.empty() : tensor<14x128256xf32>
    %367 = "ttir.mesh_shard"(%355, %366) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %368 = ttir.empty() : tensor<1x14x128256xf32>
    %369 = "ttir.mesh_shard"(%357, %368) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %359, %361, %363, %365, %367, %369 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRFusing (ttir-fusing) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<4096x128xf32>
    %133 = "ttir.permute"(%20, %132) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %134 = "ttir.dot_general"(%88, %133) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %135 = ttir.empty() : tensor<14x128xbf16>
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %137 = ttir.empty() : tensor<1x14x1x128xbf16>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %139 = ttir.empty() : tensor<1x1x14x128xbf16>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %141 = "ttir.fill_cache"(%52, %140) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %142 = ttir.empty() : tensor<1x1x4096xf32>
    %143 = "ttir.reshape"(%38, %142) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %144 = ttir.empty() : tensor<1x14x4096xf32>
    %145 = "ttir.broadcast"(%143, %144) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %146 = ttir.empty() : tensor<4096x512xf32>
    %147 = "ttir.permute"(%32, %146) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %148 = "ttir.dot_general"(%88, %147) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %149 = ttir.empty() : tensor<1x14x4x128xf32>
    %150 = "ttir.reshape"(%148, %149) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %151 = ttir.empty() : tensor<1x4x14x128xf32>
    %152 = "ttir.permute"(%150, %151) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %153 = ttir.empty() : tensor<1x1x14x128xf32>
    %154 = "ttir.reshape"(%108, %153) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.broadcast"(%154, %155) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.multiply"(%152, %156, %157) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.slice"(%152, %159) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.neg"(%160, %161) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.slice"(%152, %163) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x128xf32>
    %166 = "ttir.concat"(%162, %164, %165) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %167 = ttir.empty() : tensor<1x1x14x128xf32>
    %168 = "ttir.reshape"(%122, %167) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.broadcast"(%168, %169) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.multiply"(%166, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.add"(%158, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<4x14x128xf32>
    %176 = "ttir.reshape"(%174, %175) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %177 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %178 = "ttir.reshape"(%131, %177) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %180 = "ttir.broadcast"(%178, %179) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x19x128xf32>
    %184 = "ttir.reshape"(%182, %183) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x128x19xf32>
    %186 = "ttir.permute"(%184, %185) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %187 = ttir.empty() : tensor<4x128x19xf32>
    %188 = "ttir.reshape"(%186, %187) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %189 = "ttir.dot_general"(%176, %188) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %190 = ttir.empty() : tensor<1x4x14x19xf32>
    %191 = "ttir.reshape"(%189, %190) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %192 = ttir.empty() : tensor<1x1x1x1xf32>
    %193 = "ttir.reshape"(%30, %192) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %194 = ttir.empty() : tensor<1x4x14x19xf32>
    %195 = "ttir.broadcast"(%193, %194) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.multiply"(%191, %195, %196) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %199 = ttir.empty() : tensor<14x1xsi32>
    %200 = "ttir.reshape"(%198, %199) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %201 = ttir.empty() : tensor<14x19xsi32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %203 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %204 = ttir.empty() : tensor<1x19xsi32>
    %205 = "ttir.reshape"(%203, %204) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %206 = ttir.empty() : tensor<14x19xsi32>
    %207 = "ttir.broadcast"(%205, %206) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %208 = ttir.empty() : tensor<14x19xbf16>
    %209 = "ttir.ge"(%202, %207, %208) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %210 = ttir.empty() : tensor<1x1xf32>
    %211 = "ttir.reshape"(%28, %210) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %212 = ttir.empty() : tensor<14x19xf32>
    %213 = "ttir.broadcast"(%211, %212) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.where"(%209, %44, %213, %214) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<1x19xsi32>
    %217 = "ttir.reshape"(%0, %216) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %218 = ttir.empty() : tensor<14x19xsi32>
    %219 = "ttir.broadcast"(%217, %218) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %220 = ttir.empty() : tensor<14x1xsi32>
    %221 = "ttir.reshape"(%10, %220) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %222 = ttir.empty() : tensor<14x19xsi32>
    %223 = "ttir.broadcast"(%221, %222) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %224 = ttir.empty() : tensor<14x19xbf16>
    %225 = "ttir.gt"(%219, %223, %224) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %226 = ttir.empty() : tensor<14x19xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.multiply"(%215, %227, %228) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<1x1x14x19xf32>
    %231 = "ttir.reshape"(%229, %230) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %232 = ttir.empty() : tensor<1x4x14x19xf32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.add"(%197, %233, %234) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x1xf32>
    %237 = "ttir.max"(%235, %236) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %238 = ttir.empty() : tensor<1x4x14x19xf32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %240 = ttir.empty() : tensor<1x4x14x19xf32>
    %241 = "ttir.subtract"(%235, %239, %240) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.softmax"(%241, %242) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<4x14x19xf32>
    %245 = "ttir.reshape"(%243, %244) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %246 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %247 = "ttir.reshape"(%141, %246) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %248 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %249 = "ttir.broadcast"(%247, %248) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %250 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %251 = "ttir.typecast"(%249, %250) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %252 = ttir.empty() : tensor<4x19x128xf32>
    %253 = "ttir.reshape"(%251, %252) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %254 = "ttir.dot_general"(%245, %253) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<512x4096xf32>
    %262 = "ttir.permute"(%26, %261) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %263 = "ttir.dot_general"(%260, %262) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %264 = ttir.empty() : tensor<14x4096xf32>
    %265 = "ttir.all_reduce"(%263, %264) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %266 = ttir.empty() : tensor<1x14x4096xf32>
    %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %268 = ttir.empty() : tensor<1x14x4096xf32>
    %269 = "ttir.add"(%60, %267, %268) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %270 = ttir.empty() : tensor<1x1x4096xf32>
    %271 = "ttir.reshape"(%34, %270) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %272 = ttir.empty() : tensor<1x14x4096xf32>
    %273 = "ttir.broadcast"(%271, %272) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %274 = ttir.empty() : tensor<1x14x4096xf32>
    %275 = "ttir.pow"(%269, %48, %274) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %276 = ttir.empty() : tensor<1x14xf32>
    %277 = "ttir.sum"(%275, %276) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %278 = ttir.empty() : tensor<1x14xf32>
    %279 = "ttir.multiply"(%277, %2, %278) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %280 = ttir.empty() : tensor<1x14x1xf32>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %282 = ttir.empty() : tensor<1x14x1xf32>
    %283 = "ttir.add"(%281, %76, %282) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %284 = ttir.empty() : tensor<1x14x1xf32>
    %285 = "ttir.rsqrt"(%283, %284) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14x4096xf32>
    %289 = "ttir.multiply"(%269, %287, %288) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %290 = ttir.empty() : tensor<1x14x4096xf32>
    %291 = "ttir.multiply"(%273, %289, %290) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %292 = ttir.empty() : tensor<14x4096xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %294 = ttir.empty() : tensor<4096x1792xf32>
    %295 = "ttir.permute"(%36, %294) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %296 = "ttir.dot_general"(%293, %295) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.sigmoid"(%298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<1x14x1792xf32>
    %302 = "ttir.multiply"(%298, %300, %301) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %303 = ttir.empty() : tensor<4096x1792xf32>
    %304 = "ttir.permute"(%24, %303) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %305 = "ttir.dot_general"(%293, %304) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %306 = ttir.empty() : tensor<1x14x1792xf32>
    %307 = "ttir.reshape"(%305, %306) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %308 = ttir.empty() : tensor<1x14x1792xf32>
    %309 = "ttir.multiply"(%302, %307, %308) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %310 = ttir.empty() : tensor<14x1792xf32>
    %311 = "ttir.reshape"(%309, %310) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %312 = ttir.empty() : tensor<1792x4096xf32>
    %313 = "ttir.permute"(%22, %312) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %314 = "ttir.dot_general"(%311, %313) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %315 = ttir.empty() : tensor<14x4096xf32>
    %316 = "ttir.all_reduce"(%314, %315) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.reshape"(%316, %317) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14x4096xf32>
    %320 = "ttir.add"(%269, %318, %319) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %321 = ttir.empty() : tensor<1x14x4096xf32>
    %322 = "ttir.pow"(%320, %48, %321) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %323 = ttir.empty() : tensor<1x14xf32>
    %324 = "ttir.sum"(%322, %323) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %325 = ttir.empty() : tensor<1x14xf32>
    %326 = "ttir.multiply"(%324, %2, %325) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.reshape"(%326, %327) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x1xf32>
    %330 = "ttir.add"(%328, %76, %329) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %331 = ttir.empty() : tensor<1x14x1xf32>
    %332 = "ttir.rsqrt"(%330, %331) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14x4096xf32>
    %336 = "ttir.multiply"(%320, %334, %335) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %337 = ttir.empty() : tensor<1x14x4096xf32>
    %338 = "ttir.multiply"(%145, %336, %337) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %339 = ttir.empty() : tensor<14x4096xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %341 = ttir.empty() : tensor<4096x16032xf32>
    %342 = "ttir.permute"(%40, %341) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %343 = "ttir.dot_general"(%340, %342) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %344 = ttir.empty() : tensor<1x14x16032xf32>
    %345 = "ttir.reshape"(%343, %344) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %346 = ttir.empty() : tensor<1x14x4096xf32>
    %347 = "ttir.mesh_shard"(%60, %346) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %348 = ttir.empty() : tensor<1x8x19x128xbf16>
    %349 = "ttir.mesh_shard"(%131, %348) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %350 = ttir.empty() : tensor<1x8x19x128xbf16>
    %351 = "ttir.mesh_shard"(%141, %350) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %352 = ttir.empty() : tensor<1x14x4096xf32>
    %353 = "ttir.mesh_shard"(%338, %352) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %354 = ttir.empty() : tensor<14x128256xf32>
    %355 = "ttir.mesh_shard"(%343, %354) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %356 = ttir.empty() : tensor<1x14x128256xf32>
    %357 = "ttir.mesh_shard"(%345, %356) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %347, %349, %351, %353, %355, %357 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRQuantDequantConversion (ttir-quant-dequant-conversion) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.gather"(%8, %56, %57) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 4096>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<128256x4096xf32>, tensor<14xui32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = "ttir.dot_general"(%88, %90) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %92 = ttir.empty() : tensor<1x14x1x128xf32>
    %93 = "ttir.reshape"(%91, %92) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %94 = ttir.empty() : tensor<1x1x14x128xf32>
    %95 = "ttir.permute"(%93, %94) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %96 = ttir.empty() : tensor<1x64x1xf32>
    %97 = "ttir.reshape"(%12, %96) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %98 = ttir.empty() : tensor<14xf32>
    %99 = "ttir.typecast"(%10, %98) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %100 = ttir.empty() : tensor<1x1x14xf32>
    %101 = "ttir.reshape"(%99, %100) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %102 = "ttir.dot_general"(%97, %101) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%95, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%95, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%95, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<4096x128xf32>
    %133 = "ttir.permute"(%20, %132) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %134 = "ttir.dot_general"(%88, %133) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x128xf32>) -> tensor<14x128xf32>
    %135 = ttir.empty() : tensor<14x128xbf16>
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %137 = ttir.empty() : tensor<1x14x1x128xbf16>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %139 = ttir.empty() : tensor<1x1x14x128xbf16>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %141 = "ttir.fill_cache"(%52, %140) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %142 = ttir.empty() : tensor<1x1x4096xf32>
    %143 = "ttir.reshape"(%38, %142) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %144 = ttir.empty() : tensor<1x14x4096xf32>
    %145 = "ttir.broadcast"(%143, %144) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %146 = ttir.empty() : tensor<4096x512xf32>
    %147 = "ttir.permute"(%32, %146) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %148 = "ttir.dot_general"(%88, %147) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x512xf32>) -> tensor<14x512xf32>
    %149 = ttir.empty() : tensor<1x14x4x128xf32>
    %150 = "ttir.reshape"(%148, %149) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %151 = ttir.empty() : tensor<1x4x14x128xf32>
    %152 = "ttir.permute"(%150, %151) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %153 = ttir.empty() : tensor<1x1x14x128xf32>
    %154 = "ttir.reshape"(%108, %153) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.broadcast"(%154, %155) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x128xf32>
    %158 = "ttir.multiply"(%152, %156, %157) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.slice"(%152, %159) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.neg"(%160, %161) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.slice"(%152, %163) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x128xf32>
    %166 = "ttir.concat"(%162, %164, %165) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %167 = ttir.empty() : tensor<1x1x14x128xf32>
    %168 = "ttir.reshape"(%122, %167) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.broadcast"(%168, %169) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.multiply"(%166, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.add"(%158, %172, %173) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<4x14x128xf32>
    %176 = "ttir.reshape"(%174, %175) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %177 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %178 = "ttir.reshape"(%131, %177) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %180 = "ttir.broadcast"(%178, %179) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %181 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x19x128xf32>
    %184 = "ttir.reshape"(%182, %183) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %185 = ttir.empty() : tensor<1x4x128x19xf32>
    %186 = "ttir.permute"(%184, %185) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %187 = ttir.empty() : tensor<4x128x19xf32>
    %188 = "ttir.reshape"(%186, %187) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %189 = "ttir.dot_general"(%176, %188) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>) -> tensor<4x14x19xf32>
    %190 = ttir.empty() : tensor<1x4x14x19xf32>
    %191 = "ttir.reshape"(%189, %190) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %192 = ttir.empty() : tensor<1x1x1x1xf32>
    %193 = "ttir.reshape"(%30, %192) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %194 = ttir.empty() : tensor<1x4x14x19xf32>
    %195 = "ttir.broadcast"(%193, %194) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %196 = ttir.empty() : tensor<1x4x14x19xf32>
    %197 = "ttir.multiply"(%191, %195, %196) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %198 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %199 = ttir.empty() : tensor<14x1xsi32>
    %200 = "ttir.reshape"(%198, %199) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %201 = ttir.empty() : tensor<14x19xsi32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %203 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %204 = ttir.empty() : tensor<1x19xsi32>
    %205 = "ttir.reshape"(%203, %204) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %206 = ttir.empty() : tensor<14x19xsi32>
    %207 = "ttir.broadcast"(%205, %206) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %208 = ttir.empty() : tensor<14x19xbf16>
    %209 = "ttir.ge"(%202, %207, %208) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %210 = ttir.empty() : tensor<1x1xf32>
    %211 = "ttir.reshape"(%28, %210) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %212 = ttir.empty() : tensor<14x19xf32>
    %213 = "ttir.broadcast"(%211, %212) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %214 = ttir.empty() : tensor<14x19xf32>
    %215 = "ttir.where"(%209, %44, %213, %214) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %216 = ttir.empty() : tensor<1x19xsi32>
    %217 = "ttir.reshape"(%0, %216) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %218 = ttir.empty() : tensor<14x19xsi32>
    %219 = "ttir.broadcast"(%217, %218) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %220 = ttir.empty() : tensor<14x1xsi32>
    %221 = "ttir.reshape"(%10, %220) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %222 = ttir.empty() : tensor<14x19xsi32>
    %223 = "ttir.broadcast"(%221, %222) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %224 = ttir.empty() : tensor<14x19xbf16>
    %225 = "ttir.gt"(%219, %223, %224) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %226 = ttir.empty() : tensor<14x19xf32>
    %227 = "ttir.typecast"(%225, %226) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %228 = ttir.empty() : tensor<14x19xf32>
    %229 = "ttir.multiply"(%215, %227, %228) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %230 = ttir.empty() : tensor<1x1x14x19xf32>
    %231 = "ttir.reshape"(%229, %230) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %232 = ttir.empty() : tensor<1x4x14x19xf32>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %234 = ttir.empty() : tensor<1x4x14x19xf32>
    %235 = "ttir.add"(%197, %233, %234) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %236 = ttir.empty() : tensor<1x4x14x1xf32>
    %237 = "ttir.max"(%235, %236) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %238 = ttir.empty() : tensor<1x4x14x19xf32>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %240 = ttir.empty() : tensor<1x4x14x19xf32>
    %241 = "ttir.subtract"(%235, %239, %240) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %242 = ttir.empty() : tensor<1x4x14x19xf32>
    %243 = "ttir.softmax"(%241, %242) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %244 = ttir.empty() : tensor<4x14x19xf32>
    %245 = "ttir.reshape"(%243, %244) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %246 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %247 = "ttir.reshape"(%141, %246) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %248 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %249 = "ttir.broadcast"(%247, %248) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %250 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %251 = "ttir.typecast"(%249, %250) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %252 = ttir.empty() : tensor<4x19x128xf32>
    %253 = "ttir.reshape"(%251, %252) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %254 = "ttir.dot_general"(%245, %253) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<512x4096xf32>
    %262 = "ttir.permute"(%26, %261) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %263 = "ttir.dot_general"(%260, %262) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x512xf32>, tensor<512x4096xf32>) -> tensor<14x4096xf32>
    %264 = ttir.empty() : tensor<14x4096xf32>
    %265 = "ttir.all_reduce"(%263, %264) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %266 = ttir.empty() : tensor<1x14x4096xf32>
    %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %268 = ttir.empty() : tensor<1x14x4096xf32>
    %269 = "ttir.add"(%60, %267, %268) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %270 = ttir.empty() : tensor<1x1x4096xf32>
    %271 = "ttir.reshape"(%34, %270) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %272 = ttir.empty() : tensor<1x14x4096xf32>
    %273 = "ttir.broadcast"(%271, %272) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %274 = ttir.empty() : tensor<1x14x4096xf32>
    %275 = "ttir.pow"(%269, %48, %274) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %276 = ttir.empty() : tensor<1x14xf32>
    %277 = "ttir.sum"(%275, %276) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %278 = ttir.empty() : tensor<1x14xf32>
    %279 = "ttir.multiply"(%277, %2, %278) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %280 = ttir.empty() : tensor<1x14x1xf32>
    %281 = "ttir.reshape"(%279, %280) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %282 = ttir.empty() : tensor<1x14x1xf32>
    %283 = "ttir.add"(%281, %76, %282) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %284 = ttir.empty() : tensor<1x14x1xf32>
    %285 = "ttir.rsqrt"(%283, %284) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %286 = ttir.empty() : tensor<1x14x4096xf32>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %288 = ttir.empty() : tensor<1x14x4096xf32>
    %289 = "ttir.multiply"(%269, %287, %288) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %290 = ttir.empty() : tensor<1x14x4096xf32>
    %291 = "ttir.multiply"(%273, %289, %290) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %292 = ttir.empty() : tensor<14x4096xf32>
    %293 = "ttir.reshape"(%291, %292) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %294 = ttir.empty() : tensor<4096x1792xf32>
    %295 = "ttir.permute"(%36, %294) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %296 = "ttir.dot_general"(%293, %295) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.reshape"(%296, %297) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.sigmoid"(%298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<1x14x1792xf32>
    %302 = "ttir.multiply"(%298, %300, %301) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %303 = ttir.empty() : tensor<4096x1792xf32>
    %304 = "ttir.permute"(%24, %303) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %305 = "ttir.dot_general"(%293, %304) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>) -> tensor<14x1792xf32>
    %306 = ttir.empty() : tensor<1x14x1792xf32>
    %307 = "ttir.reshape"(%305, %306) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %308 = ttir.empty() : tensor<1x14x1792xf32>
    %309 = "ttir.multiply"(%302, %307, %308) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %310 = ttir.empty() : tensor<14x1792xf32>
    %311 = "ttir.reshape"(%309, %310) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %312 = ttir.empty() : tensor<1792x4096xf32>
    %313 = "ttir.permute"(%22, %312) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %314 = "ttir.dot_general"(%311, %313) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>) -> tensor<14x4096xf32>
    %315 = ttir.empty() : tensor<14x4096xf32>
    %316 = "ttir.all_reduce"(%314, %315) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.reshape"(%316, %317) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14x4096xf32>
    %320 = "ttir.add"(%269, %318, %319) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %321 = ttir.empty() : tensor<1x14x4096xf32>
    %322 = "ttir.pow"(%320, %48, %321) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %323 = ttir.empty() : tensor<1x14xf32>
    %324 = "ttir.sum"(%322, %323) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %325 = ttir.empty() : tensor<1x14xf32>
    %326 = "ttir.multiply"(%324, %2, %325) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.reshape"(%326, %327) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x1xf32>
    %330 = "ttir.add"(%328, %76, %329) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %331 = ttir.empty() : tensor<1x14x1xf32>
    %332 = "ttir.rsqrt"(%330, %331) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.broadcast"(%332, %333) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<1x14x4096xf32>
    %336 = "ttir.multiply"(%320, %334, %335) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %337 = ttir.empty() : tensor<1x14x4096xf32>
    %338 = "ttir.multiply"(%145, %336, %337) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %339 = ttir.empty() : tensor<14x4096xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %341 = ttir.empty() : tensor<4096x16032xf32>
    %342 = "ttir.permute"(%40, %341) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %343 = "ttir.dot_general"(%340, %342) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>) -> tensor<14x16032xf32>
    %344 = ttir.empty() : tensor<1x14x16032xf32>
    %345 = "ttir.reshape"(%343, %344) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %346 = ttir.empty() : tensor<1x14x4096xf32>
    %347 = "ttir.mesh_shard"(%60, %346) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %348 = ttir.empty() : tensor<1x8x19x128xbf16>
    %349 = "ttir.mesh_shard"(%131, %348) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %350 = ttir.empty() : tensor<1x8x19x128xbf16>
    %351 = "ttir.mesh_shard"(%141, %350) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %352 = ttir.empty() : tensor<1x14x4096xf32>
    %353 = "ttir.mesh_shard"(%338, %352) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %354 = ttir.empty() : tensor<14x128256xf32>
    %355 = "ttir.mesh_shard"(%343, %354) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %356 = ttir.empty() : tensor<1x14x128256xf32>
    %357 = "ttir.mesh_shard"(%345, %356) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %347, %349, %351, %353, %355, %357 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRToTTIRDecomposition (ttir-to-ttir-decomposition) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = ttir.empty() : tensor<128256x4096xf32>
    %59 = "ttir.permute"(%8, %58) <{permutation = array<i64: 0, 1>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %60 = ttir.empty() : tensor<128256x4096xf32>
    %61 = "ttir.reshape"(%59, %60) <{shape = [128256 : i32, 4096 : i32]}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %62 = ttir.empty() : tensor<14x4096xf32>
    %63 = "ttir.embedding"(%56, %61, %62) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %64 = ttir.empty() : tensor<14x4096xf32>
    %65 = "ttir.reshape"(%63, %64) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %66 = ttir.empty() : tensor<14x4096xf32>
    %67 = "ttir.permute"(%65, %66) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %68 = ttir.empty() : tensor<1x14x4096xf32>
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %70 = ttir.empty() : tensor<1x1x4096xf32>
    %71 = "ttir.reshape"(%18, %70) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %72 = ttir.empty() : tensor<1x14x4096xf32>
    %73 = "ttir.broadcast"(%71, %72) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %74 = ttir.empty() : tensor<1x14x4096xf32>
    %75 = "ttir.pow"(%69, %48, %74) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %76 = ttir.empty() : tensor<1x14xf32>
    %77 = "ttir.sum"(%75, %76) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %78 = ttir.empty() : tensor<1x14xf32>
    %79 = "ttir.multiply"(%77, %2, %78) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %80 = ttir.empty() : tensor<1x14x1xf32>
    %81 = "ttir.reshape"(%79, %80) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %82 = ttir.empty() : tensor<1x1x1xf32>
    %83 = "ttir.reshape"(%16, %82) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %84 = ttir.empty() : tensor<1x14x1xf32>
    %85 = "ttir.broadcast"(%83, %84) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %86 = ttir.empty() : tensor<1x14x1xf32>
    %87 = "ttir.add"(%81, %85, %86) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %88 = ttir.empty() : tensor<1x14x1xf32>
    %89 = "ttir.rsqrt"(%87, %88) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %90 = ttir.empty() : tensor<1x14x4096xf32>
    %91 = "ttir.broadcast"(%89, %90) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %92 = ttir.empty() : tensor<1x14x4096xf32>
    %93 = "ttir.multiply"(%69, %91, %92) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %94 = ttir.empty() : tensor<1x14x4096xf32>
    %95 = "ttir.multiply"(%73, %93, %94) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %96 = ttir.empty() : tensor<14x4096xf32>
    %97 = "ttir.reshape"(%95, %96) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %98 = ttir.empty() : tensor<4096x128xf32>
    %99 = "ttir.permute"(%14, %98) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %100 = ttir.empty() : tensor<14x4096xf32>
    %101 = "ttir.permute"(%97, %100) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %102 = ttir.empty() : tensor<4096x128xf32>
    %103 = "ttir.permute"(%99, %102) <{permutation = array<i64: 0, 1>}> : (tensor<4096x128xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %104 = ttir.empty() : tensor<14x4096xf32>
    %105 = "ttir.reshape"(%101, %104) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %106 = ttir.empty() : tensor<4096x128xf32>
    %107 = "ttir.reshape"(%103, %106) <{shape = [4096 : i32, 128 : i32]}> : (tensor<4096x128xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %108 = ttir.empty() : tensor<14x128xf32>
    %109 = "ttir.matmul"(%105, %107, %108) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %110 = ttir.empty() : tensor<14x128xf32>
    %111 = "ttir.reshape"(%109, %110) <{shape = [14 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %112 = ttir.empty() : tensor<1x14x1x128xf32>
    %113 = "ttir.reshape"(%111, %112) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %114 = ttir.empty() : tensor<1x1x14x128xf32>
    %115 = "ttir.permute"(%113, %114) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %116 = ttir.empty() : tensor<1x64x1xf32>
    %117 = "ttir.reshape"(%12, %116) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %118 = ttir.empty() : tensor<14xf32>
    %119 = "ttir.typecast"(%10, %118) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %120 = ttir.empty() : tensor<1x1x14xf32>
    %121 = "ttir.reshape"(%119, %120) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %122 = ttir.empty() : tensor<1x64x1xf32>
    %123 = "ttir.permute"(%117, %122) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x64x1xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %124 = ttir.empty() : tensor<1x1x14xf32>
    %125 = "ttir.permute"(%121, %124) <{permutation = array<i64: 0, 1, 2>}> : (tensor<1x1x14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %126 = ttir.empty() : tensor<1x64x1xf32>
    %127 = "ttir.reshape"(%123, %126) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<1x64x1xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %128 = ttir.empty() : tensor<1x1x14xf32>
    %129 = "ttir.reshape"(%125, %128) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<1x1x14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %130 = ttir.empty() : tensor<1x64x14xf32>
    %131 = "ttir.matmul"(%127, %129, %130) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %132 = ttir.empty() : tensor<1x64x14xf32>
    %133 = "ttir.reshape"(%131, %132) <{shape = [1 : i32, 64 : i32, 14 : i32]}> : (tensor<1x64x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %134 = ttir.empty() : tensor<1x14x64xf32>
    %135 = "ttir.permute"(%133, %134) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %136 = ttir.empty() : tensor<1x14x128xf32>
    %137 = "ttir.concat"(%135, %135, %136) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %138 = ttir.empty() : tensor<1x14x128xf32>
    %139 = "ttir.cos"(%137, %138) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %140 = ttir.empty() : tensor<1x1x14x128xf32>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %142 = ttir.empty() : tensor<1x1x14x128xf32>
    %143 = "ttir.multiply"(%115, %141, %142) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %144 = ttir.empty() : tensor<1x1x14x64xf32>
    %145 = "ttir.slice"(%115, %144) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %146 = ttir.empty() : tensor<1x1x14x64xf32>
    %147 = "ttir.neg"(%145, %146) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %148 = ttir.empty() : tensor<1x1x14x64xf32>
    %149 = "ttir.slice"(%115, %148) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %150 = ttir.empty() : tensor<1x1x14x128xf32>
    %151 = "ttir.concat"(%147, %149, %150) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %152 = ttir.empty() : tensor<1x14x128xf32>
    %153 = "ttir.sin"(%137, %152) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %154 = ttir.empty() : tensor<1x1x14x128xf32>
    %155 = "ttir.reshape"(%153, %154) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %156 = ttir.empty() : tensor<1x1x14x128xf32>
    %157 = "ttir.multiply"(%151, %155, %156) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %158 = ttir.empty() : tensor<1x1x14x128xf32>
    %159 = "ttir.add"(%143, %157, %158) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %160 = ttir.empty() : tensor<1x1x14x128xbf16>
    %161 = "ttir.typecast"(%159, %160) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %162 = "ttir.fill_cache"(%52, %161) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %163 = ttir.empty() : tensor<4096x128xf32>
    %164 = "ttir.permute"(%20, %163) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %165 = ttir.empty() : tensor<14x4096xf32>
    %166 = "ttir.permute"(%97, %165) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %167 = ttir.empty() : tensor<4096x128xf32>
    %168 = "ttir.permute"(%164, %167) <{permutation = array<i64: 0, 1>}> : (tensor<4096x128xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %169 = ttir.empty() : tensor<14x4096xf32>
    %170 = "ttir.reshape"(%166, %169) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %171 = ttir.empty() : tensor<4096x128xf32>
    %172 = "ttir.reshape"(%168, %171) <{shape = [4096 : i32, 128 : i32]}> : (tensor<4096x128xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %173 = ttir.empty() : tensor<14x128xf32>
    %174 = "ttir.matmul"(%170, %172, %173) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %175 = ttir.empty() : tensor<14x128xf32>
    %176 = "ttir.reshape"(%174, %175) <{shape = [14 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %177 = ttir.empty() : tensor<14x128xbf16>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %179 = ttir.empty() : tensor<1x14x1x128xbf16>
    %180 = "ttir.reshape"(%178, %179) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %181 = ttir.empty() : tensor<1x1x14x128xbf16>
    %182 = "ttir.permute"(%180, %181) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %183 = "ttir.fill_cache"(%52, %182) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %184 = ttir.empty() : tensor<1x1x4096xf32>
    %185 = "ttir.reshape"(%38, %184) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %186 = ttir.empty() : tensor<1x14x4096xf32>
    %187 = "ttir.broadcast"(%185, %186) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %188 = ttir.empty() : tensor<4096x512xf32>
    %189 = "ttir.permute"(%32, %188) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %190 = ttir.empty() : tensor<14x4096xf32>
    %191 = "ttir.permute"(%97, %190) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %192 = ttir.empty() : tensor<4096x512xf32>
    %193 = "ttir.permute"(%189, %192) <{permutation = array<i64: 0, 1>}> : (tensor<4096x512xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %194 = ttir.empty() : tensor<14x4096xf32>
    %195 = "ttir.reshape"(%191, %194) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %196 = ttir.empty() : tensor<4096x512xf32>
    %197 = "ttir.reshape"(%193, %196) <{shape = [4096 : i32, 512 : i32]}> : (tensor<4096x512xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %198 = ttir.empty() : tensor<14x512xf32>
    %199 = "ttir.matmul"(%195, %197, %198) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x512xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %200 = ttir.empty() : tensor<14x512xf32>
    %201 = "ttir.reshape"(%199, %200) <{shape = [14 : i32, 512 : i32]}> : (tensor<14x512xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %202 = ttir.empty() : tensor<1x14x4x128xf32>
    %203 = "ttir.reshape"(%201, %202) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %204 = ttir.empty() : tensor<1x4x14x128xf32>
    %205 = "ttir.permute"(%203, %204) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %206 = ttir.empty() : tensor<1x1x14x128xf32>
    %207 = "ttir.reshape"(%139, %206) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %208 = ttir.empty() : tensor<1x4x14x128xf32>
    %209 = "ttir.broadcast"(%207, %208) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %210 = ttir.empty() : tensor<1x4x14x128xf32>
    %211 = "ttir.multiply"(%205, %209, %210) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %212 = ttir.empty() : tensor<1x4x14x64xf32>
    %213 = "ttir.slice"(%205, %212) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %214 = ttir.empty() : tensor<1x4x14x64xf32>
    %215 = "ttir.neg"(%213, %214) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %216 = ttir.empty() : tensor<1x4x14x64xf32>
    %217 = "ttir.slice"(%205, %216) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %218 = ttir.empty() : tensor<1x4x14x128xf32>
    %219 = "ttir.concat"(%215, %217, %218) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %220 = ttir.empty() : tensor<1x1x14x128xf32>
    %221 = "ttir.reshape"(%153, %220) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %222 = ttir.empty() : tensor<1x4x14x128xf32>
    %223 = "ttir.broadcast"(%221, %222) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %224 = ttir.empty() : tensor<1x4x14x128xf32>
    %225 = "ttir.multiply"(%219, %223, %224) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %226 = ttir.empty() : tensor<1x4x14x128xf32>
    %227 = "ttir.add"(%211, %225, %226) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %228 = ttir.empty() : tensor<4x14x128xf32>
    %229 = "ttir.reshape"(%227, %228) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %230 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %231 = "ttir.reshape"(%162, %230) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %232 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %233 = "ttir.broadcast"(%231, %232) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %234 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %236 = ttir.empty() : tensor<1x4x19x128xf32>
    %237 = "ttir.reshape"(%235, %236) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %238 = ttir.empty() : tensor<1x4x128x19xf32>
    %239 = "ttir.permute"(%237, %238) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %240 = ttir.empty() : tensor<4x128x19xf32>
    %241 = "ttir.reshape"(%239, %240) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %242 = ttir.empty() : tensor<4x14x128xf32>
    %243 = "ttir.permute"(%229, %242) <{permutation = array<i64: 0, 1, 2>}> : (tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %244 = ttir.empty() : tensor<4x128x19xf32>
    %245 = "ttir.permute"(%241, %244) <{permutation = array<i64: 0, 1, 2>}> : (tensor<4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %246 = ttir.empty() : tensor<4x14x128xf32>
    %247 = "ttir.reshape"(%243, %246) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %248 = ttir.empty() : tensor<4x128x19xf32>
    %249 = "ttir.reshape"(%245, %248) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %250 = ttir.empty() : tensor<4x14x19xf32>
    %251 = "ttir.matmul"(%247, %249, %250) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %252 = ttir.empty() : tensor<4x14x19xf32>
    %253 = "ttir.reshape"(%251, %252) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %254 = ttir.empty() : tensor<1x4x14x19xf32>
    %255 = "ttir.reshape"(%253, %254) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %256 = ttir.empty() : tensor<1x1x1x1xf32>
    %257 = "ttir.reshape"(%30, %256) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %258 = ttir.empty() : tensor<1x4x14x19xf32>
    %259 = "ttir.broadcast"(%257, %258) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %260 = ttir.empty() : tensor<1x4x14x19xf32>
    %261 = "ttir.multiply"(%255, %259, %260) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %262 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %263 = ttir.empty() : tensor<14x1xsi32>
    %264 = "ttir.reshape"(%262, %263) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %265 = ttir.empty() : tensor<14x19xsi32>
    %266 = "ttir.broadcast"(%264, %265) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %267 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %268 = ttir.empty() : tensor<1x19xsi32>
    %269 = "ttir.reshape"(%267, %268) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %270 = ttir.empty() : tensor<14x19xsi32>
    %271 = "ttir.broadcast"(%269, %270) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %272 = ttir.empty() : tensor<14x19xbf16>
    %273 = "ttir.ge"(%266, %271, %272) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %274 = ttir.empty() : tensor<1x1xf32>
    %275 = "ttir.reshape"(%28, %274) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %276 = ttir.empty() : tensor<14x19xf32>
    %277 = "ttir.broadcast"(%275, %276) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %278 = ttir.empty() : tensor<14x19xf32>
    %279 = "ttir.where"(%273, %44, %277, %278) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %280 = ttir.empty() : tensor<1x19xsi32>
    %281 = "ttir.reshape"(%0, %280) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %282 = ttir.empty() : tensor<14x19xsi32>
    %283 = "ttir.broadcast"(%281, %282) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %284 = ttir.empty() : tensor<14x1xsi32>
    %285 = "ttir.reshape"(%10, %284) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %286 = ttir.empty() : tensor<14x19xsi32>
    %287 = "ttir.broadcast"(%285, %286) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %288 = ttir.empty() : tensor<14x19xbf16>
    %289 = "ttir.gt"(%283, %287, %288) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %290 = ttir.empty() : tensor<14x19xf32>
    %291 = "ttir.typecast"(%289, %290) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %292 = ttir.empty() : tensor<14x19xf32>
    %293 = "ttir.multiply"(%279, %291, %292) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %294 = ttir.empty() : tensor<1x1x14x19xf32>
    %295 = "ttir.reshape"(%293, %294) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %296 = ttir.empty() : tensor<1x4x14x19xf32>
    %297 = "ttir.broadcast"(%295, %296) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %298 = ttir.empty() : tensor<1x4x14x19xf32>
    %299 = "ttir.add"(%261, %297, %298) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %300 = ttir.empty() : tensor<1x4x14x1xf32>
    %301 = "ttir.max"(%299, %300) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %302 = ttir.empty() : tensor<1x4x14x19xf32>
    %303 = "ttir.broadcast"(%301, %302) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %304 = ttir.empty() : tensor<1x4x14x19xf32>
    %305 = "ttir.subtract"(%299, %303, %304) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %306 = ttir.empty() : tensor<1x4x14x19xf32>
    %307 = "ttir.softmax"(%305, %306) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %308 = ttir.empty() : tensor<4x14x19xf32>
    %309 = "ttir.reshape"(%307, %308) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %310 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %311 = "ttir.reshape"(%183, %310) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %312 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %313 = "ttir.broadcast"(%311, %312) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %314 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %315 = "ttir.typecast"(%313, %314) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %316 = ttir.empty() : tensor<4x19x128xf32>
    %317 = "ttir.reshape"(%315, %316) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %318 = ttir.empty() : tensor<4x14x19xf32>
    %319 = "ttir.permute"(%309, %318) <{permutation = array<i64: 0, 1, 2>}> : (tensor<4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %320 = ttir.empty() : tensor<4x19x128xf32>
    %321 = "ttir.permute"(%317, %320) <{permutation = array<i64: 0, 1, 2>}> : (tensor<4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %322 = ttir.empty() : tensor<4x14x19xf32>
    %323 = "ttir.reshape"(%319, %322) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %324 = ttir.empty() : tensor<4x19x128xf32>
    %325 = "ttir.reshape"(%321, %324) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %326 = ttir.empty() : tensor<4x14x128xf32>
    %327 = "ttir.matmul"(%323, %325, %326) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %328 = ttir.empty() : tensor<4x14x128xf32>
    %329 = "ttir.reshape"(%327, %328) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %330 = ttir.empty() : tensor<1x4x14x128xf32>
    %331 = "ttir.reshape"(%329, %330) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %332 = ttir.empty() : tensor<1x14x4x128xf32>
    %333 = "ttir.permute"(%331, %332) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %334 = ttir.empty() : tensor<14x512xf32>
    %335 = "ttir.reshape"(%333, %334) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %336 = ttir.empty() : tensor<512x4096xf32>
    %337 = "ttir.permute"(%26, %336) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %338 = ttir.empty() : tensor<14x512xf32>
    %339 = "ttir.permute"(%335, %338) <{permutation = array<i64: 0, 1>}> : (tensor<14x512xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %340 = ttir.empty() : tensor<512x4096xf32>
    %341 = "ttir.permute"(%337, %340) <{permutation = array<i64: 0, 1>}> : (tensor<512x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %342 = ttir.empty() : tensor<14x512xf32>
    %343 = "ttir.reshape"(%339, %342) <{shape = [14 : i32, 512 : i32]}> : (tensor<14x512xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %344 = ttir.empty() : tensor<512x4096xf32>
    %345 = "ttir.reshape"(%341, %344) <{shape = [512 : i32, 4096 : i32]}> : (tensor<512x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %346 = ttir.empty() : tensor<14x4096xf32>
    %347 = "ttir.matmul"(%343, %345, %346) <{transpose_a = false, transpose_b = false}> : (tensor<14x512xf32>, tensor<512x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %348 = ttir.empty() : tensor<14x4096xf32>
    %349 = "ttir.reshape"(%347, %348) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %350 = ttir.empty() : tensor<14x4096xf32>
    %351 = "ttir.all_reduce"(%349, %350) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %352 = ttir.empty() : tensor<1x14x4096xf32>
    %353 = "ttir.reshape"(%351, %352) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %354 = ttir.empty() : tensor<1x14x4096xf32>
    %355 = "ttir.add"(%69, %353, %354) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %356 = ttir.empty() : tensor<1x1x4096xf32>
    %357 = "ttir.reshape"(%34, %356) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %358 = ttir.empty() : tensor<1x14x4096xf32>
    %359 = "ttir.broadcast"(%357, %358) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %360 = ttir.empty() : tensor<1x14x4096xf32>
    %361 = "ttir.pow"(%355, %48, %360) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %362 = ttir.empty() : tensor<1x14xf32>
    %363 = "ttir.sum"(%361, %362) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %364 = ttir.empty() : tensor<1x14xf32>
    %365 = "ttir.multiply"(%363, %2, %364) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %366 = ttir.empty() : tensor<1x14x1xf32>
    %367 = "ttir.reshape"(%365, %366) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %368 = ttir.empty() : tensor<1x14x1xf32>
    %369 = "ttir.add"(%367, %85, %368) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %370 = ttir.empty() : tensor<1x14x1xf32>
    %371 = "ttir.rsqrt"(%369, %370) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %372 = ttir.empty() : tensor<1x14x4096xf32>
    %373 = "ttir.broadcast"(%371, %372) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %374 = ttir.empty() : tensor<1x14x4096xf32>
    %375 = "ttir.multiply"(%355, %373, %374) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %376 = ttir.empty() : tensor<1x14x4096xf32>
    %377 = "ttir.multiply"(%359, %375, %376) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %378 = ttir.empty() : tensor<14x4096xf32>
    %379 = "ttir.reshape"(%377, %378) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %380 = ttir.empty() : tensor<4096x1792xf32>
    %381 = "ttir.permute"(%36, %380) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %382 = ttir.empty() : tensor<14x4096xf32>
    %383 = "ttir.permute"(%379, %382) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %384 = ttir.empty() : tensor<4096x1792xf32>
    %385 = "ttir.permute"(%381, %384) <{permutation = array<i64: 0, 1>}> : (tensor<4096x1792xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %386 = ttir.empty() : tensor<14x4096xf32>
    %387 = "ttir.reshape"(%383, %386) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %388 = ttir.empty() : tensor<4096x1792xf32>
    %389 = "ttir.reshape"(%385, %388) <{shape = [4096 : i32, 1792 : i32]}> : (tensor<4096x1792xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %390 = ttir.empty() : tensor<14x1792xf32>
    %391 = "ttir.matmul"(%387, %389, %390) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %392 = ttir.empty() : tensor<14x1792xf32>
    %393 = "ttir.reshape"(%391, %392) <{shape = [14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %394 = ttir.empty() : tensor<1x14x1792xf32>
    %395 = "ttir.reshape"(%393, %394) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %396 = ttir.empty() : tensor<1x14x1792xf32>
    %397 = "ttir.sigmoid"(%395, %396) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %398 = ttir.empty() : tensor<1x14x1792xf32>
    %399 = "ttir.multiply"(%395, %397, %398) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %400 = ttir.empty() : tensor<4096x1792xf32>
    %401 = "ttir.permute"(%24, %400) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %402 = ttir.empty() : tensor<14x4096xf32>
    %403 = "ttir.permute"(%379, %402) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %404 = ttir.empty() : tensor<4096x1792xf32>
    %405 = "ttir.permute"(%401, %404) <{permutation = array<i64: 0, 1>}> : (tensor<4096x1792xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %406 = ttir.empty() : tensor<14x4096xf32>
    %407 = "ttir.reshape"(%403, %406) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %408 = ttir.empty() : tensor<4096x1792xf32>
    %409 = "ttir.reshape"(%405, %408) <{shape = [4096 : i32, 1792 : i32]}> : (tensor<4096x1792xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %410 = ttir.empty() : tensor<14x1792xf32>
    %411 = "ttir.matmul"(%407, %409, %410) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %412 = ttir.empty() : tensor<14x1792xf32>
    %413 = "ttir.reshape"(%411, %412) <{shape = [14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %414 = ttir.empty() : tensor<1x14x1792xf32>
    %415 = "ttir.reshape"(%413, %414) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %416 = ttir.empty() : tensor<1x14x1792xf32>
    %417 = "ttir.multiply"(%399, %415, %416) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %418 = ttir.empty() : tensor<14x1792xf32>
    %419 = "ttir.reshape"(%417, %418) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %420 = ttir.empty() : tensor<1792x4096xf32>
    %421 = "ttir.permute"(%22, %420) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %422 = ttir.empty() : tensor<14x1792xf32>
    %423 = "ttir.permute"(%419, %422) <{permutation = array<i64: 0, 1>}> : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %424 = ttir.empty() : tensor<1792x4096xf32>
    %425 = "ttir.permute"(%421, %424) <{permutation = array<i64: 0, 1>}> : (tensor<1792x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %426 = ttir.empty() : tensor<14x1792xf32>
    %427 = "ttir.reshape"(%423, %426) <{shape = [14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %428 = ttir.empty() : tensor<1792x4096xf32>
    %429 = "ttir.reshape"(%425, %428) <{shape = [1792 : i32, 4096 : i32]}> : (tensor<1792x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %430 = ttir.empty() : tensor<14x4096xf32>
    %431 = "ttir.matmul"(%427, %429, %430) <{transpose_a = false, transpose_b = false}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %432 = ttir.empty() : tensor<14x4096xf32>
    %433 = "ttir.reshape"(%431, %432) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %434 = ttir.empty() : tensor<14x4096xf32>
    %435 = "ttir.all_reduce"(%433, %434) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %436 = ttir.empty() : tensor<1x14x4096xf32>
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %438 = ttir.empty() : tensor<1x14x4096xf32>
    %439 = "ttir.add"(%355, %437, %438) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %440 = ttir.empty() : tensor<1x14x4096xf32>
    %441 = "ttir.pow"(%439, %48, %440) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %442 = ttir.empty() : tensor<1x14xf32>
    %443 = "ttir.sum"(%441, %442) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %444 = ttir.empty() : tensor<1x14xf32>
    %445 = "ttir.multiply"(%443, %2, %444) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %446 = ttir.empty() : tensor<1x14x1xf32>
    %447 = "ttir.reshape"(%445, %446) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %448 = ttir.empty() : tensor<1x14x1xf32>
    %449 = "ttir.add"(%447, %85, %448) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %450 = ttir.empty() : tensor<1x14x1xf32>
    %451 = "ttir.rsqrt"(%449, %450) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %452 = ttir.empty() : tensor<1x14x4096xf32>
    %453 = "ttir.broadcast"(%451, %452) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %454 = ttir.empty() : tensor<1x14x4096xf32>
    %455 = "ttir.multiply"(%439, %453, %454) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %456 = ttir.empty() : tensor<1x14x4096xf32>
    %457 = "ttir.multiply"(%187, %455, %456) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %458 = ttir.empty() : tensor<14x4096xf32>
    %459 = "ttir.reshape"(%457, %458) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %460 = ttir.empty() : tensor<4096x16032xf32>
    %461 = "ttir.permute"(%40, %460) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %462 = ttir.empty() : tensor<14x4096xf32>
    %463 = "ttir.permute"(%459, %462) <{permutation = array<i64: 0, 1>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %464 = ttir.empty() : tensor<4096x16032xf32>
    %465 = "ttir.permute"(%461, %464) <{permutation = array<i64: 0, 1>}> : (tensor<4096x16032xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %466 = ttir.empty() : tensor<14x4096xf32>
    %467 = "ttir.reshape"(%463, %466) <{shape = [14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %468 = ttir.empty() : tensor<4096x16032xf32>
    %469 = "ttir.reshape"(%465, %468) <{shape = [4096 : i32, 16032 : i32]}> : (tensor<4096x16032xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %470 = ttir.empty() : tensor<14x16032xf32>
    %471 = "ttir.matmul"(%467, %469, %470) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %472 = ttir.empty() : tensor<14x16032xf32>
    %473 = "ttir.reshape"(%471, %472) <{shape = [14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %474 = ttir.empty() : tensor<1x14x16032xf32>
    %475 = "ttir.reshape"(%473, %474) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %476 = ttir.empty() : tensor<1x14x4096xf32>
    %477 = "ttir.mesh_shard"(%69, %476) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %478 = ttir.empty() : tensor<1x8x19x128xbf16>
    %479 = "ttir.mesh_shard"(%162, %478) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %480 = ttir.empty() : tensor<1x8x19x128xbf16>
    %481 = "ttir.mesh_shard"(%183, %480) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %482 = ttir.empty() : tensor<1x14x4096xf32>
    %483 = "ttir.mesh_shard"(%457, %482) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %484 = ttir.empty() : tensor<14x128256xf32>
    %485 = "ttir.mesh_shard"(%473, %484) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %486 = ttir.empty() : tensor<1x14x128256xf32>
    %487 = "ttir.mesh_shard"(%475, %486) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %477, %479, %481, %483, %485, %487 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRFusing (ttir-fusing) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<4096x128xf32>
    %90 = "ttir.permute"(%14, %89) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %91 = ttir.empty() : tensor<14x128xf32>
    %92 = "ttir.matmul"(%88, %90, %91) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %93 = ttir.empty() : tensor<1x14x1x128xf32>
    %94 = "ttir.reshape"(%92, %93) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %95 = ttir.empty() : tensor<1x1x14x128xf32>
    %96 = "ttir.permute"(%94, %95) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %97 = ttir.empty() : tensor<1x64x1xf32>
    %98 = "ttir.reshape"(%12, %97) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %99 = ttir.empty() : tensor<14xf32>
    %100 = "ttir.typecast"(%10, %99) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %101 = ttir.empty() : tensor<1x1x14xf32>
    %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %103 = ttir.empty() : tensor<1x64x14xf32>
    %104 = "ttir.matmul"(%98, %102, %103) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %105 = ttir.empty() : tensor<1x14x64xf32>
    %106 = "ttir.permute"(%104, %105) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.concat"(%106, %106, %107) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x14x128xf32>
    %110 = "ttir.cos"(%108, %109) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.reshape"(%110, %111) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x128xf32>
    %114 = "ttir.multiply"(%96, %112, %113) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.slice"(%96, %115) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.neg"(%116, %117) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x64xf32>
    %120 = "ttir.slice"(%96, %119) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %121 = ttir.empty() : tensor<1x1x14x128xf32>
    %122 = "ttir.concat"(%118, %120, %121) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %123 = ttir.empty() : tensor<1x14x128xf32>
    %124 = "ttir.sin"(%108, %123) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.reshape"(%124, %125) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.multiply"(%122, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xf32>
    %130 = "ttir.add"(%114, %128, %129) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %131 = ttir.empty() : tensor<1x1x14x128xbf16>
    %132 = "ttir.typecast"(%130, %131) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %133 = "ttir.fill_cache"(%52, %132) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %134 = ttir.empty() : tensor<4096x128xf32>
    %135 = "ttir.permute"(%20, %134) <{permutation = array<i64: 1, 0>}> : (tensor<128x4096xf32>, tensor<4096x128xf32>) -> tensor<4096x128xf32>
    %136 = ttir.empty() : tensor<14x128xf32>
    %137 = "ttir.matmul"(%88, %135, %136) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x128xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %138 = ttir.empty() : tensor<14x128xbf16>
    %139 = "ttir.typecast"(%137, %138) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %140 = ttir.empty() : tensor<1x14x1x128xbf16>
    %141 = "ttir.reshape"(%139, %140) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %142 = ttir.empty() : tensor<1x1x14x128xbf16>
    %143 = "ttir.permute"(%141, %142) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %144 = "ttir.fill_cache"(%52, %143) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %145 = ttir.empty() : tensor<1x1x4096xf32>
    %146 = "ttir.reshape"(%38, %145) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %147 = ttir.empty() : tensor<1x14x4096xf32>
    %148 = "ttir.broadcast"(%146, %147) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %149 = ttir.empty() : tensor<4096x512xf32>
    %150 = "ttir.permute"(%32, %149) <{permutation = array<i64: 1, 0>}> : (tensor<512x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %151 = ttir.empty() : tensor<14x512xf32>
    %152 = "ttir.matmul"(%88, %150, %151) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x512xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %153 = ttir.empty() : tensor<1x14x4x128xf32>
    %154 = "ttir.reshape"(%152, %153) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.permute"(%154, %155) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x1x14x128xf32>
    %158 = "ttir.reshape"(%110, %157) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %159 = ttir.empty() : tensor<1x4x14x128xf32>
    %160 = "ttir.broadcast"(%158, %159) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %161 = ttir.empty() : tensor<1x4x14x128xf32>
    %162 = "ttir.multiply"(%156, %160, %161) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.slice"(%156, %163) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<1x4x14x64xf32>
    %166 = "ttir.neg"(%164, %165) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %167 = ttir.empty() : tensor<1x4x14x64xf32>
    %168 = "ttir.slice"(%156, %167) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.concat"(%166, %168, %169) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x1x14x128xf32>
    %172 = "ttir.reshape"(%124, %171) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %173 = ttir.empty() : tensor<1x4x14x128xf32>
    %174 = "ttir.broadcast"(%172, %173) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %175 = ttir.empty() : tensor<1x4x14x128xf32>
    %176 = "ttir.multiply"(%170, %174, %175) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %177 = ttir.empty() : tensor<1x4x14x128xf32>
    %178 = "ttir.add"(%162, %176, %177) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %179 = ttir.empty() : tensor<4x14x128xf32>
    %180 = "ttir.reshape"(%178, %179) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %181 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %182 = "ttir.reshape"(%133, %181) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %183 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %184 = "ttir.broadcast"(%182, %183) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %185 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %186 = "ttir.typecast"(%184, %185) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %187 = ttir.empty() : tensor<1x4x19x128xf32>
    %188 = "ttir.reshape"(%186, %187) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %189 = ttir.empty() : tensor<1x4x128x19xf32>
    %190 = "ttir.permute"(%188, %189) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %191 = ttir.empty() : tensor<4x128x19xf32>
    %192 = "ttir.reshape"(%190, %191) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %193 = ttir.empty() : tensor<4x14x19xf32>
    %194 = "ttir.matmul"(%180, %192, %193) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %195 = ttir.empty() : tensor<1x4x14x19xf32>
    %196 = "ttir.reshape"(%194, %195) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %197 = ttir.empty() : tensor<1x1x1x1xf32>
    %198 = "ttir.reshape"(%30, %197) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %199 = ttir.empty() : tensor<1x4x14x19xf32>
    %200 = "ttir.broadcast"(%198, %199) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %201 = ttir.empty() : tensor<1x4x14x19xf32>
    %202 = "ttir.multiply"(%196, %200, %201) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %203 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %204 = ttir.empty() : tensor<14x1xsi32>
    %205 = "ttir.reshape"(%203, %204) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %206 = ttir.empty() : tensor<14x19xsi32>
    %207 = "ttir.broadcast"(%205, %206) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %208 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %209 = ttir.empty() : tensor<1x19xsi32>
    %210 = "ttir.reshape"(%208, %209) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %211 = ttir.empty() : tensor<14x19xsi32>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %213 = ttir.empty() : tensor<14x19xbf16>
    %214 = "ttir.ge"(%207, %212, %213) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %215 = ttir.empty() : tensor<1x1xf32>
    %216 = "ttir.reshape"(%28, %215) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %217 = ttir.empty() : tensor<14x19xf32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %219 = ttir.empty() : tensor<14x19xf32>
    %220 = "ttir.where"(%214, %44, %218, %219) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %221 = ttir.empty() : tensor<1x19xsi32>
    %222 = "ttir.reshape"(%0, %221) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %223 = ttir.empty() : tensor<14x19xsi32>
    %224 = "ttir.broadcast"(%222, %223) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %225 = ttir.empty() : tensor<14x1xsi32>
    %226 = "ttir.reshape"(%10, %225) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %227 = ttir.empty() : tensor<14x19xsi32>
    %228 = "ttir.broadcast"(%226, %227) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %229 = ttir.empty() : tensor<14x19xbf16>
    %230 = "ttir.gt"(%224, %228, %229) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %231 = ttir.empty() : tensor<14x19xf32>
    %232 = "ttir.typecast"(%230, %231) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %233 = ttir.empty() : tensor<14x19xf32>
    %234 = "ttir.multiply"(%220, %232, %233) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %235 = ttir.empty() : tensor<1x1x14x19xf32>
    %236 = "ttir.reshape"(%234, %235) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<1x4x14x19xf32>
    %240 = "ttir.add"(%202, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %241 = ttir.empty() : tensor<1x4x14x1xf32>
    %242 = "ttir.max"(%240, %241) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %243 = ttir.empty() : tensor<1x4x14x19xf32>
    %244 = "ttir.broadcast"(%242, %243) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %245 = ttir.empty() : tensor<1x4x14x19xf32>
    %246 = "ttir.subtract"(%240, %244, %245) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %247 = ttir.empty() : tensor<1x4x14x19xf32>
    %248 = "ttir.softmax"(%246, %247) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %249 = ttir.empty() : tensor<4x14x19xf32>
    %250 = "ttir.reshape"(%248, %249) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %251 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %252 = "ttir.reshape"(%144, %251) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %253 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %254 = "ttir.broadcast"(%252, %253) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %255 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %256 = "ttir.typecast"(%254, %255) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %257 = ttir.empty() : tensor<4x19x128xf32>
    %258 = "ttir.reshape"(%256, %257) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %259 = ttir.empty() : tensor<4x14x128xf32>
    %260 = "ttir.matmul"(%250, %258, %259) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %261 = ttir.empty() : tensor<1x4x14x128xf32>
    %262 = "ttir.reshape"(%260, %261) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %263 = ttir.empty() : tensor<1x14x4x128xf32>
    %264 = "ttir.permute"(%262, %263) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %265 = ttir.empty() : tensor<14x512xf32>
    %266 = "ttir.reshape"(%264, %265) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %267 = ttir.empty() : tensor<512x4096xf32>
    %268 = "ttir.permute"(%26, %267) <{permutation = array<i64: 1, 0>}> : (tensor<4096x512xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %269 = ttir.empty() : tensor<14x4096xf32>
    %270 = "ttir.matmul"(%266, %268, %269) <{transpose_a = false, transpose_b = false}> : (tensor<14x512xf32>, tensor<512x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %271 = ttir.empty() : tensor<14x4096xf32>
    %272 = "ttir.all_reduce"(%270, %271) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %273 = ttir.empty() : tensor<1x14x4096xf32>
    %274 = "ttir.reshape"(%272, %273) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %275 = ttir.empty() : tensor<1x14x4096xf32>
    %276 = "ttir.add"(%60, %274, %275) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %277 = ttir.empty() : tensor<1x1x4096xf32>
    %278 = "ttir.reshape"(%34, %277) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %279 = ttir.empty() : tensor<1x14x4096xf32>
    %280 = "ttir.broadcast"(%278, %279) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %281 = ttir.empty() : tensor<1x14x4096xf32>
    %282 = "ttir.pow"(%276, %48, %281) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %283 = ttir.empty() : tensor<1x14xf32>
    %284 = "ttir.sum"(%282, %283) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %285 = ttir.empty() : tensor<1x14xf32>
    %286 = "ttir.multiply"(%284, %2, %285) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %287 = ttir.empty() : tensor<1x14x1xf32>
    %288 = "ttir.reshape"(%286, %287) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %289 = ttir.empty() : tensor<1x14x1xf32>
    %290 = "ttir.add"(%288, %76, %289) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %291 = ttir.empty() : tensor<1x14x1xf32>
    %292 = "ttir.rsqrt"(%290, %291) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %293 = ttir.empty() : tensor<1x14x4096xf32>
    %294 = "ttir.broadcast"(%292, %293) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %295 = ttir.empty() : tensor<1x14x4096xf32>
    %296 = "ttir.multiply"(%276, %294, %295) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %297 = ttir.empty() : tensor<1x14x4096xf32>
    %298 = "ttir.multiply"(%280, %296, %297) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %299 = ttir.empty() : tensor<14x4096xf32>
    %300 = "ttir.reshape"(%298, %299) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %301 = ttir.empty() : tensor<4096x1792xf32>
    %302 = "ttir.permute"(%36, %301) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %303 = ttir.empty() : tensor<14x1792xf32>
    %304 = "ttir.matmul"(%300, %302, %303) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %305 = ttir.empty() : tensor<1x14x1792xf32>
    %306 = "ttir.reshape"(%304, %305) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %307 = ttir.empty() : tensor<1x14x1792xf32>
    %308 = "ttir.sigmoid"(%306, %307) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %309 = ttir.empty() : tensor<1x14x1792xf32>
    %310 = "ttir.multiply"(%306, %308, %309) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %311 = ttir.empty() : tensor<4096x1792xf32>
    %312 = "ttir.permute"(%24, %311) <{permutation = array<i64: 1, 0>}> : (tensor<1792x4096xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %313 = ttir.empty() : tensor<14x1792xf32>
    %314 = "ttir.matmul"(%300, %312, %313) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %315 = ttir.empty() : tensor<1x14x1792xf32>
    %316 = "ttir.reshape"(%314, %315) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %317 = ttir.empty() : tensor<1x14x1792xf32>
    %318 = "ttir.multiply"(%310, %316, %317) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %319 = ttir.empty() : tensor<14x1792xf32>
    %320 = "ttir.reshape"(%318, %319) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %321 = ttir.empty() : tensor<1792x4096xf32>
    %322 = "ttir.permute"(%22, %321) <{permutation = array<i64: 1, 0>}> : (tensor<4096x1792xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %323 = ttir.empty() : tensor<14x4096xf32>
    %324 = "ttir.matmul"(%320, %322, %323) <{transpose_a = false, transpose_b = false}> : (tensor<14x1792xf32>, tensor<1792x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %325 = ttir.empty() : tensor<14x4096xf32>
    %326 = "ttir.all_reduce"(%324, %325) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %327 = ttir.empty() : tensor<1x14x4096xf32>
    %328 = "ttir.reshape"(%326, %327) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.add"(%276, %328, %329) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.pow"(%330, %48, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14xf32>
    %334 = "ttir.sum"(%332, %333) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %335 = ttir.empty() : tensor<1x14xf32>
    %336 = "ttir.multiply"(%334, %2, %335) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %337 = ttir.empty() : tensor<1x14x1xf32>
    %338 = "ttir.reshape"(%336, %337) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %339 = ttir.empty() : tensor<1x14x1xf32>
    %340 = "ttir.add"(%338, %76, %339) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %341 = ttir.empty() : tensor<1x14x1xf32>
    %342 = "ttir.rsqrt"(%340, %341) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %343 = ttir.empty() : tensor<1x14x4096xf32>
    %344 = "ttir.broadcast"(%342, %343) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %345 = ttir.empty() : tensor<1x14x4096xf32>
    %346 = "ttir.multiply"(%330, %344, %345) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.multiply"(%148, %346, %347) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<14x4096xf32>
    %350 = "ttir.reshape"(%348, %349) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %351 = ttir.empty() : tensor<4096x16032xf32>
    %352 = "ttir.permute"(%40, %351) <{permutation = array<i64: 1, 0>}> : (tensor<16032x4096xf32>, tensor<4096x16032xf32>) -> tensor<4096x16032xf32>
    %353 = ttir.empty() : tensor<14x16032xf32>
    %354 = "ttir.matmul"(%350, %352, %353) <{transpose_a = false, transpose_b = false}> : (tensor<14x4096xf32>, tensor<4096x16032xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %355 = ttir.empty() : tensor<1x14x16032xf32>
    %356 = "ttir.reshape"(%354, %355) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %357 = ttir.empty() : tensor<1x14x4096xf32>
    %358 = "ttir.mesh_shard"(%60, %357) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %359 = ttir.empty() : tensor<1x8x19x128xbf16>
    %360 = "ttir.mesh_shard"(%133, %359) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %361 = ttir.empty() : tensor<1x8x19x128xbf16>
    %362 = "ttir.mesh_shard"(%144, %361) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %363 = ttir.empty() : tensor<1x14x4096xf32>
    %364 = "ttir.mesh_shard"(%348, %363) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %365 = ttir.empty() : tensor<14x128256xf32>
    %366 = "ttir.mesh_shard"(%354, %365) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %367 = ttir.empty() : tensor<1x14x128256xf32>
    %368 = "ttir.mesh_shard"(%356, %367) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %358, %360, %362, %364, %366, %368 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<1x1x14x128xf32>
    %152 = "ttir.reshape"(%108, %151) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.multiply"(%150, %154, %155) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x128xf32>
    %164 = "ttir.concat"(%160, %162, %163) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %165 = ttir.empty() : tensor<1x1x14x128xf32>
    %166 = "ttir.reshape"(%122, %165) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.multiply"(%164, %168, %169) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.add"(%156, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.reshape"(%172, %173) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %176 = "ttir.reshape"(%131, %175) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %177 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %178 = "ttir.broadcast"(%176, %177) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %180 = "ttir.typecast"(%178, %179) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %181 = ttir.empty() : tensor<1x4x19x128xf32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x128x19xf32>
    %184 = "ttir.permute"(%182, %183) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %185 = ttir.empty() : tensor<4x128x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %187 = ttir.empty() : tensor<4x14x19xf32>
    %188 = "ttir.matmul"(%174, %186, %187) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x1x1x1xf32>
    %192 = "ttir.reshape"(%30, %191) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %193 = ttir.empty() : tensor<1x4x14x19xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %195 = ttir.empty() : tensor<1x4x14x19xf32>
    %196 = "ttir.multiply"(%190, %194, %195) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %197 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %198 = ttir.empty() : tensor<14x1xsi32>
    %199 = "ttir.reshape"(%197, %198) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %200 = ttir.empty() : tensor<14x19xsi32>
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %202 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %203 = ttir.empty() : tensor<1x19xsi32>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %205 = ttir.empty() : tensor<14x19xsi32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %207 = ttir.empty() : tensor<14x19xbf16>
    %208 = "ttir.ge"(%201, %206, %207) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %209 = ttir.empty() : tensor<1x1xf32>
    %210 = "ttir.reshape"(%28, %209) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %211 = ttir.empty() : tensor<14x19xf32>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %213 = ttir.empty() : tensor<14x19xf32>
    %214 = "ttir.where"(%208, %44, %212, %213) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %215 = ttir.empty() : tensor<1x19xsi32>
    %216 = "ttir.reshape"(%0, %215) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %217 = ttir.empty() : tensor<14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %219 = ttir.empty() : tensor<14x1xsi32>
    %220 = "ttir.reshape"(%10, %219) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %221 = ttir.empty() : tensor<14x19xsi32>
    %222 = "ttir.broadcast"(%220, %221) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %223 = ttir.empty() : tensor<14x19xbf16>
    %224 = "ttir.gt"(%218, %222, %223) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %225 = ttir.empty() : tensor<14x19xf32>
    %226 = "ttir.typecast"(%224, %225) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %227 = ttir.empty() : tensor<14x19xf32>
    %228 = "ttir.multiply"(%214, %226, %227) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %229 = ttir.empty() : tensor<1x1x14x19xf32>
    %230 = "ttir.reshape"(%228, %229) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x19xf32>
    %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.add"(%196, %232, %233) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x1xf32>
    %236 = "ttir.max"(%234, %235) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<1x4x14x19xf32>
    %240 = "ttir.subtract"(%234, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %241 = ttir.empty() : tensor<1x4x14x19xf32>
    %242 = "ttir.softmax"(%240, %241) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %243 = ttir.empty() : tensor<4x14x19xf32>
    %244 = "ttir.reshape"(%242, %243) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %245 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %246 = "ttir.reshape"(%140, %245) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %247 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %249 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %250 = "ttir.typecast"(%248, %249) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %251 = ttir.empty() : tensor<4x19x128xf32>
    %252 = "ttir.reshape"(%250, %251) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %253 = ttir.empty() : tensor<4x14x128xf32>
    %254 = "ttir.matmul"(%244, %252, %253) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<14x4096xf32>
    %262 = "ttir.matmul"(%260, %26, %261) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.all_reduce"(%262, %263) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<1x14x4096xf32>
    %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.add"(%60, %266, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x1x4096xf32>
    %270 = "ttir.reshape"(%34, %269) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.broadcast"(%270, %271) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14x4096xf32>
    %274 = "ttir.pow"(%268, %48, %273) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.sum"(%274, %275) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.multiply"(%276, %2, %277) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.add"(%280, %76, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x1xf32>
    %284 = "ttir.rsqrt"(%282, %283) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<1x14x4096xf32>
    %288 = "ttir.multiply"(%268, %286, %287) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %289 = ttir.empty() : tensor<1x14x4096xf32>
    %290 = "ttir.multiply"(%272, %288, %289) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %291 = ttir.empty() : tensor<14x4096xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%292, %36, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<1x14x1792xf32>
    %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.sigmoid"(%296, %297) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.multiply"(%296, %298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<14x1792xf32>
    %302 = "ttir.matmul"(%292, %24, %301) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %303 = ttir.empty() : tensor<1x14x1792xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %305 = ttir.empty() : tensor<1x14x1792xf32>
    %306 = "ttir.multiply"(%300, %304, %305) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %307 = ttir.empty() : tensor<14x1792xf32>
    %308 = "ttir.reshape"(%306, %307) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<14x4096xf32>
    %310 = "ttir.matmul"(%308, %22, %309) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %311 = ttir.empty() : tensor<14x4096xf32>
    %312 = "ttir.all_reduce"(%310, %311) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %313 = ttir.empty() : tensor<1x14x4096xf32>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %315 = ttir.empty() : tensor<1x14x4096xf32>
    %316 = "ttir.add"(%268, %314, %315) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.pow"(%316, %48, %317) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14xf32>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %321 = ttir.empty() : tensor<1x14xf32>
    %322 = "ttir.multiply"(%320, %2, %321) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %323 = ttir.empty() : tensor<1x14x1xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %325 = ttir.empty() : tensor<1x14x1xf32>
    %326 = "ttir.add"(%324, %76, %325) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.rsqrt"(%326, %327) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.multiply"(%316, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.multiply"(%144, %332, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<14x4096xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %337 = ttir.empty() : tensor<14x16032xf32>
    %338 = "ttir.matmul"(%336, %40, %337) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %339 = ttir.empty() : tensor<1x14x16032xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %341 = ttir.empty() : tensor<1x14x4096xf32>
    %342 = "ttir.mesh_shard"(%60, %341) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %343 = ttir.empty() : tensor<1x8x19x128xbf16>
    %344 = "ttir.mesh_shard"(%131, %343) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %345 = ttir.empty() : tensor<1x8x19x128xbf16>
    %346 = "ttir.mesh_shard"(%140, %345) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.mesh_shard"(%334, %347) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<14x128256xf32>
    %350 = "ttir.mesh_shard"(%338, %349) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %351 = ttir.empty() : tensor<1x14x128256xf32>
    %352 = "ttir.mesh_shard"(%340, %351) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %342, %344, %346, %348, %350, %352 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
  %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
  %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
  %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
  %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
  %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
  %5 = ttir.empty() : tensor<1x14xsi32>
  %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
  %7 = ttir.empty() : tensor<128256x4096xf32>
  %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
  %9 = ttir.empty() : tensor<14xsi32>
  %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
  %11 = ttir.empty() : tensor<64xf32>
  %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
  %13 = ttir.empty() : tensor<128x4096xf32>
  %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
  %15 = ttir.empty() : tensor<f32>
  %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
  %17 = ttir.empty() : tensor<4096xf32>
  %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
  %19 = ttir.empty() : tensor<128x4096xf32>
  %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
  %21 = ttir.empty() : tensor<4096x1792xf32>
  %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
  %23 = ttir.empty() : tensor<1792x4096xf32>
  %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
  %25 = ttir.empty() : tensor<4096x512xf32>
  %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
  %27 = ttir.empty() : tensor<f32>
  %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
  %29 = ttir.empty() : tensor<f32>
  %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
  %31 = ttir.empty() : tensor<512x4096xf32>
  %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
  %33 = ttir.empty() : tensor<4096xf32>
  %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
  %35 = ttir.empty() : tensor<1792x4096xf32>
  %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
  %37 = ttir.empty() : tensor<4096xf32>
  %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
  %39 = ttir.empty() : tensor<16032x4096xf32>
  %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
  %41 = ttir.empty() : tensor<1x1xf32>
  %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
  %43 = ttir.empty() : tensor<14x19xf32>
  %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
  %45 = ttir.empty() : tensor<1x1x1xf32>
  %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
  %47 = ttir.empty() : tensor<1x14x4096xf32>
  %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %49 = ttir.empty() : tensor<1x1x1x1xbf16>
  %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
  %51 = ttir.empty() : tensor<1x1x19x128xbf16>
  %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
  %53 = ttir.empty() : tensor<1x14xui32>
  %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
  %55 = ttir.empty() : tensor<14xui32>
  %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
  %57 = ttir.empty() : tensor<14x4096xf32>
  %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %59 = ttir.empty() : tensor<1x14x4096xf32>
  %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %61 = ttir.empty() : tensor<1x1x4096xf32>
  %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
  %63 = ttir.empty() : tensor<1x14x4096xf32>
  %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %65 = ttir.empty() : tensor<1x14x4096xf32>
  %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %67 = ttir.empty() : tensor<1x14xf32>
  %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %69 = ttir.empty() : tensor<1x14xf32>
  %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %71 = ttir.empty() : tensor<1x14x1xf32>
  %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %73 = ttir.empty() : tensor<1x1x1xf32>
  %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
  %75 = ttir.empty() : tensor<1x14x1xf32>
  %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %77 = ttir.empty() : tensor<1x14x1xf32>
  %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %79 = ttir.empty() : tensor<1x14x1xf32>
  %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %81 = ttir.empty() : tensor<1x14x4096xf32>
  %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %83 = ttir.empty() : tensor<1x14x4096xf32>
  %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %85 = ttir.empty() : tensor<1x14x4096xf32>
  %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %87 = ttir.empty() : tensor<14x4096xf32>
  %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %89 = ttir.empty() : tensor<14x128xf32>
  %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
  %91 = ttir.empty() : tensor<1x14x1x128xf32>
  %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
  %93 = ttir.empty() : tensor<1x1x14x128xf32>
  %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %95 = ttir.empty() : tensor<1x64x1xf32>
  %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
  %97 = ttir.empty() : tensor<14xf32>
  %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
  %99 = ttir.empty() : tensor<1x1x14xf32>
  %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
  %101 = ttir.empty() : tensor<1x64x14xf32>
  %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
  %103 = ttir.empty() : tensor<1x14x64xf32>
  %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
  %105 = ttir.empty() : tensor<1x14x128xf32>
  %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
  %107 = ttir.empty() : tensor<1x14x128xf32>
  %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
  %109 = ttir.empty() : tensor<1x1x14x128xf32>
  %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %111 = ttir.empty() : tensor<1x1x14x128xf32>
  %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %113 = ttir.empty() : tensor<1x1x14x64xf32>
  %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
  %115 = ttir.empty() : tensor<1x1x14x64xf32>
  %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
  %117 = ttir.empty() : tensor<1x1x14x64xf32>
  %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
  %119 = ttir.empty() : tensor<1x1x14x128xf32>
  %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %121 = ttir.empty() : tensor<1x14x128xf32>
  %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
  %123 = ttir.empty() : tensor<1x1x14x128xf32>
  %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %125 = ttir.empty() : tensor<1x1x14x128xf32>
  %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %127 = ttir.empty() : tensor<1x1x14x128xf32>
  %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %129 = ttir.empty() : tensor<1x1x14x128xbf16>
  %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
  %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
  %132 = ttir.empty() : tensor<14x128xf32>
  %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
  %134 = ttir.empty() : tensor<14x128xbf16>
  %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
  %136 = ttir.empty() : tensor<1x14x1x128xbf16>
  %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
  %138 = ttir.empty() : tensor<1x1x14x128xbf16>
  %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
  %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
  %141 = ttir.empty() : tensor<1x1x4096xf32>
  %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
  %143 = ttir.empty() : tensor<1x14x4096xf32>
  %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %145 = ttir.empty() : tensor<14x512xf32>
  %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
  %147 = ttir.empty() : tensor<1x14x4x128xf32>
  %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
  %149 = ttir.empty() : tensor<1x4x14x128xf32>
  %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %151 = ttir.empty() : tensor<1x1x14x128xf32>
  %152 = "ttir.reshape"(%108, %151) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %153 = ttir.empty() : tensor<1x4x14x128xf32>
  %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %155 = ttir.empty() : tensor<1x4x14x128xf32>
  %156 = "ttir.multiply"(%150, %154, %155) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %157 = ttir.empty() : tensor<1x4x14x64xf32>
  %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
  %159 = ttir.empty() : tensor<1x4x14x64xf32>
  %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
  %161 = ttir.empty() : tensor<1x4x14x64xf32>
  %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
  %163 = ttir.empty() : tensor<1x4x14x128xf32>
  %164 = "ttir.concat"(%160, %162, %163) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %165 = ttir.empty() : tensor<1x1x14x128xf32>
  %166 = "ttir.reshape"(%122, %165) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
  %167 = ttir.empty() : tensor<1x4x14x128xf32>
  %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %169 = ttir.empty() : tensor<1x4x14x128xf32>
  %170 = "ttir.multiply"(%164, %168, %169) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %171 = ttir.empty() : tensor<1x4x14x128xf32>
  %172 = "ttir.add"(%156, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %173 = ttir.empty() : tensor<4x14x128xf32>
  %174 = "ttir.reshape"(%172, %173) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
  %175 = ttir.empty() : tensor<1x1x1x19x128xbf16>
  %176 = "ttir.reshape"(%131, %175) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
  %177 = ttir.empty() : tensor<1x1x4x19x128xbf16>
  %178 = "ttir.broadcast"(%176, %177) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
  %179 = ttir.empty() : tensor<1x1x4x19x128xf32>
  %180 = "ttir.typecast"(%178, %179) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
  %181 = ttir.empty() : tensor<1x4x19x128xf32>
  %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
  %183 = ttir.empty() : tensor<1x4x128x19xf32>
  %184 = "ttir.permute"(%182, %183) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
  %185 = ttir.empty() : tensor<4x128x19xf32>
  %186 = "ttir.reshape"(%184, %185) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
  %187 = ttir.empty() : tensor<4x14x19xf32>
  %188 = "ttir.matmul"(%174, %186, %187) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
  %189 = ttir.empty() : tensor<1x4x14x19xf32>
  %190 = "ttir.reshape"(%188, %189) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %191 = ttir.empty() : tensor<1x1x1x1xf32>
  %192 = "ttir.reshape"(%30, %191) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
  %193 = ttir.empty() : tensor<1x4x14x19xf32>
  %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %195 = ttir.empty() : tensor<1x4x14x19xf32>
  %196 = "ttir.multiply"(%190, %194, %195) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %197 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
  %198 = ttir.empty() : tensor<14x1xsi32>
  %199 = "ttir.reshape"(%197, %198) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
  %200 = ttir.empty() : tensor<14x19xsi32>
  %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
  %202 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
  %203 = ttir.empty() : tensor<1x19xsi32>
  %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
  %205 = ttir.empty() : tensor<14x19xsi32>
  %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
  %207 = ttir.empty() : tensor<14x19xbf16>
  %208 = "ttir.ge"(%201, %206, %207) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
  %209 = ttir.empty() : tensor<1x1xf32>
  %210 = "ttir.reshape"(%28, %209) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
  %211 = ttir.empty() : tensor<14x19xf32>
  %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
  %213 = ttir.empty() : tensor<14x19xf32>
  %214 = "ttir.where"(%208, %44, %212, %213) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
  %215 = ttir.empty() : tensor<1x19xsi32>
  %216 = "ttir.reshape"(%0, %215) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
  %217 = ttir.empty() : tensor<14x19xsi32>
  %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
  %219 = ttir.empty() : tensor<14x1xsi32>
  %220 = "ttir.reshape"(%10, %219) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
  %221 = ttir.empty() : tensor<14x19xsi32>
  %222 = "ttir.broadcast"(%220, %221) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
  %223 = ttir.empty() : tensor<14x19xbf16>
  %224 = "ttir.gt"(%218, %222, %223) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
  %225 = ttir.empty() : tensor<14x19xf32>
  %226 = "ttir.typecast"(%224, %225) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
  %227 = ttir.empty() : tensor<14x19xf32>
  %228 = "ttir.multiply"(%214, %226, %227) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
  %229 = ttir.empty() : tensor<1x1x14x19xf32>
  %230 = "ttir.reshape"(%228, %229) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
  %231 = ttir.empty() : tensor<1x4x14x19xf32>
  %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %233 = ttir.empty() : tensor<1x4x14x19xf32>
  %234 = "ttir.add"(%196, %232, %233) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %235 = ttir.empty() : tensor<1x4x14x1xf32>
  %236 = "ttir.max"(%234, %235) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
  %237 = ttir.empty() : tensor<1x4x14x19xf32>
  %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %239 = ttir.empty() : tensor<1x4x14x19xf32>
  %240 = "ttir.subtract"(%234, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %241 = ttir.empty() : tensor<1x4x14x19xf32>
  %242 = "ttir.softmax"(%240, %241) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
  %243 = ttir.empty() : tensor<4x14x19xf32>
  %244 = "ttir.reshape"(%242, %243) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
  %245 = ttir.empty() : tensor<1x1x1x19x128xbf16>
  %246 = "ttir.reshape"(%140, %245) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
  %247 = ttir.empty() : tensor<1x1x4x19x128xbf16>
  %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
  %249 = ttir.empty() : tensor<1x1x4x19x128xf32>
  %250 = "ttir.typecast"(%248, %249) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
  %251 = ttir.empty() : tensor<4x19x128xf32>
  %252 = "ttir.reshape"(%250, %251) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
  %253 = ttir.empty() : tensor<4x14x128xf32>
  %254 = "ttir.matmul"(%244, %252, %253) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
  %255 = ttir.empty() : tensor<1x4x14x128xf32>
  %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
  %257 = ttir.empty() : tensor<1x14x4x128xf32>
  %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
  %259 = ttir.empty() : tensor<14x512xf32>
  %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
  %261 = ttir.empty() : tensor<14x4096xf32>
  %262 = "ttir.matmul"(%260, %26, %261) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %263 = ttir.empty() : tensor<14x4096xf32>
  %264 = "ttir.all_reduce"(%262, %263) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %265 = ttir.empty() : tensor<1x14x4096xf32>
  %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %267 = ttir.empty() : tensor<1x14x4096xf32>
  %268 = "ttir.add"(%60, %266, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %269 = ttir.empty() : tensor<1x1x4096xf32>
  %270 = "ttir.reshape"(%34, %269) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
  %271 = ttir.empty() : tensor<1x14x4096xf32>
  %272 = "ttir.broadcast"(%270, %271) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %273 = ttir.empty() : tensor<1x14x4096xf32>
  %274 = "ttir.pow"(%268, %48, %273) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %275 = ttir.empty() : tensor<1x14xf32>
  %276 = "ttir.sum"(%274, %275) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %277 = ttir.empty() : tensor<1x14xf32>
  %278 = "ttir.multiply"(%276, %2, %277) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %279 = ttir.empty() : tensor<1x14x1xf32>
  %280 = "ttir.reshape"(%278, %279) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %281 = ttir.empty() : tensor<1x14x1xf32>
  %282 = "ttir.add"(%280, %76, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %283 = ttir.empty() : tensor<1x14x1xf32>
  %284 = "ttir.rsqrt"(%282, %283) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %285 = ttir.empty() : tensor<1x14x4096xf32>
  %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %287 = ttir.empty() : tensor<1x14x4096xf32>
  %288 = "ttir.multiply"(%268, %286, %287) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %289 = ttir.empty() : tensor<1x14x4096xf32>
  %290 = "ttir.multiply"(%272, %288, %289) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %291 = ttir.empty() : tensor<14x4096xf32>
  %292 = "ttir.reshape"(%290, %291) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %293 = ttir.empty() : tensor<14x1792xf32>
  %294 = "ttir.matmul"(%292, %36, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
  %295 = ttir.empty() : tensor<1x14x1792xf32>
  %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
  %297 = ttir.empty() : tensor<1x14x1792xf32>
  %298 = "ttir.sigmoid"(%296, %297) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
  %299 = ttir.empty() : tensor<1x14x1792xf32>
  %300 = "ttir.multiply"(%296, %298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
  %301 = ttir.empty() : tensor<14x1792xf32>
  %302 = "ttir.matmul"(%292, %24, %301) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
  %303 = ttir.empty() : tensor<1x14x1792xf32>
  %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
  %305 = ttir.empty() : tensor<1x14x1792xf32>
  %306 = "ttir.multiply"(%300, %304, %305) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
  %307 = ttir.empty() : tensor<14x1792xf32>
  %308 = "ttir.reshape"(%306, %307) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
  %309 = ttir.empty() : tensor<14x4096xf32>
  %310 = "ttir.matmul"(%308, %22, %309) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %311 = ttir.empty() : tensor<14x4096xf32>
  %312 = "ttir.all_reduce"(%310, %311) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %313 = ttir.empty() : tensor<1x14x4096xf32>
  %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %315 = ttir.empty() : tensor<1x14x4096xf32>
  %316 = "ttir.add"(%268, %314, %315) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %317 = ttir.empty() : tensor<1x14x4096xf32>
  %318 = "ttir.pow"(%316, %48, %317) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %319 = ttir.empty() : tensor<1x14xf32>
  %320 = "ttir.sum"(%318, %319) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %321 = ttir.empty() : tensor<1x14xf32>
  %322 = "ttir.multiply"(%320, %2, %321) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
  %323 = ttir.empty() : tensor<1x14x1xf32>
  %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %325 = ttir.empty() : tensor<1x14x1xf32>
  %326 = "ttir.add"(%324, %76, %325) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %327 = ttir.empty() : tensor<1x14x1xf32>
  %328 = "ttir.rsqrt"(%326, %327) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
  %329 = ttir.empty() : tensor<1x14x4096xf32>
  %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %331 = ttir.empty() : tensor<1x14x4096xf32>
  %332 = "ttir.multiply"(%316, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %333 = ttir.empty() : tensor<1x14x4096xf32>
  %334 = "ttir.multiply"(%144, %332, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %335 = ttir.empty() : tensor<14x4096xf32>
  %336 = "ttir.reshape"(%334, %335) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
  %337 = ttir.empty() : tensor<14x16032xf32>
  %338 = "ttir.matmul"(%336, %40, %337) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
  %339 = ttir.empty() : tensor<1x14x16032xf32>
  %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
  %341 = ttir.empty() : tensor<1x14x4096xf32>
  %342 = "ttir.mesh_shard"(%60, %341) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %343 = ttir.empty() : tensor<1x8x19x128xbf16>
  %344 = "ttir.mesh_shard"(%131, %343) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
  %345 = ttir.empty() : tensor<1x8x19x128xbf16>
  %346 = "ttir.mesh_shard"(%140, %345) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
  %347 = ttir.empty() : tensor<1x14x4096xf32>
  %348 = "ttir.mesh_shard"(%334, %347) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
  %349 = ttir.empty() : tensor<14x128256xf32>
  %350 = "ttir.mesh_shard"(%338, %349) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
  %351 = ttir.empty() : tensor<1x14x128256xf32>
  %352 = "ttir.mesh_shard"(%340, %351) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
  return %342, %344, %346, %348, %350, %352 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
}

// -----// IR Dump After Inliner (inline) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<1x1x14x128xf32>
    %152 = "ttir.reshape"(%108, %151) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.multiply"(%150, %154, %155) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x128xf32>
    %164 = "ttir.concat"(%160, %162, %163) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %165 = ttir.empty() : tensor<1x1x14x128xf32>
    %166 = "ttir.reshape"(%122, %165) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.multiply"(%164, %168, %169) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.add"(%156, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.reshape"(%172, %173) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %176 = "ttir.reshape"(%131, %175) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %177 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %178 = "ttir.broadcast"(%176, %177) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %180 = "ttir.typecast"(%178, %179) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %181 = ttir.empty() : tensor<1x4x19x128xf32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x128x19xf32>
    %184 = "ttir.permute"(%182, %183) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %185 = ttir.empty() : tensor<4x128x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %187 = ttir.empty() : tensor<4x14x19xf32>
    %188 = "ttir.matmul"(%174, %186, %187) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x1x1x1xf32>
    %192 = "ttir.reshape"(%30, %191) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %193 = ttir.empty() : tensor<1x4x14x19xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %195 = ttir.empty() : tensor<1x4x14x19xf32>
    %196 = "ttir.multiply"(%190, %194, %195) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %197 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %198 = ttir.empty() : tensor<14x1xsi32>
    %199 = "ttir.reshape"(%197, %198) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %200 = ttir.empty() : tensor<14x19xsi32>
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %202 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %203 = ttir.empty() : tensor<1x19xsi32>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %205 = ttir.empty() : tensor<14x19xsi32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %207 = ttir.empty() : tensor<14x19xbf16>
    %208 = "ttir.ge"(%201, %206, %207) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %209 = ttir.empty() : tensor<1x1xf32>
    %210 = "ttir.reshape"(%28, %209) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %211 = ttir.empty() : tensor<14x19xf32>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %213 = ttir.empty() : tensor<14x19xf32>
    %214 = "ttir.where"(%208, %44, %212, %213) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %215 = ttir.empty() : tensor<1x19xsi32>
    %216 = "ttir.reshape"(%0, %215) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %217 = ttir.empty() : tensor<14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %219 = ttir.empty() : tensor<14x1xsi32>
    %220 = "ttir.reshape"(%10, %219) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %221 = ttir.empty() : tensor<14x19xsi32>
    %222 = "ttir.broadcast"(%220, %221) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %223 = ttir.empty() : tensor<14x19xbf16>
    %224 = "ttir.gt"(%218, %222, %223) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %225 = ttir.empty() : tensor<14x19xf32>
    %226 = "ttir.typecast"(%224, %225) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %227 = ttir.empty() : tensor<14x19xf32>
    %228 = "ttir.multiply"(%214, %226, %227) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %229 = ttir.empty() : tensor<1x1x14x19xf32>
    %230 = "ttir.reshape"(%228, %229) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x19xf32>
    %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.add"(%196, %232, %233) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x1xf32>
    %236 = "ttir.max"(%234, %235) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<1x4x14x19xf32>
    %240 = "ttir.subtract"(%234, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %241 = ttir.empty() : tensor<1x4x14x19xf32>
    %242 = "ttir.softmax"(%240, %241) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %243 = ttir.empty() : tensor<4x14x19xf32>
    %244 = "ttir.reshape"(%242, %243) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %245 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %246 = "ttir.reshape"(%140, %245) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %247 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %249 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %250 = "ttir.typecast"(%248, %249) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %251 = ttir.empty() : tensor<4x19x128xf32>
    %252 = "ttir.reshape"(%250, %251) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %253 = ttir.empty() : tensor<4x14x128xf32>
    %254 = "ttir.matmul"(%244, %252, %253) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<14x4096xf32>
    %262 = "ttir.matmul"(%260, %26, %261) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.all_reduce"(%262, %263) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<1x14x4096xf32>
    %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.add"(%60, %266, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x1x4096xf32>
    %270 = "ttir.reshape"(%34, %269) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.broadcast"(%270, %271) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14x4096xf32>
    %274 = "ttir.pow"(%268, %48, %273) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.sum"(%274, %275) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.multiply"(%276, %2, %277) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.add"(%280, %76, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x1xf32>
    %284 = "ttir.rsqrt"(%282, %283) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<1x14x4096xf32>
    %288 = "ttir.multiply"(%268, %286, %287) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %289 = ttir.empty() : tensor<1x14x4096xf32>
    %290 = "ttir.multiply"(%272, %288, %289) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %291 = ttir.empty() : tensor<14x4096xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%292, %36, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<1x14x1792xf32>
    %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.sigmoid"(%296, %297) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.multiply"(%296, %298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<14x1792xf32>
    %302 = "ttir.matmul"(%292, %24, %301) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %303 = ttir.empty() : tensor<1x14x1792xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %305 = ttir.empty() : tensor<1x14x1792xf32>
    %306 = "ttir.multiply"(%300, %304, %305) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %307 = ttir.empty() : tensor<14x1792xf32>
    %308 = "ttir.reshape"(%306, %307) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<14x4096xf32>
    %310 = "ttir.matmul"(%308, %22, %309) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %311 = ttir.empty() : tensor<14x4096xf32>
    %312 = "ttir.all_reduce"(%310, %311) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %313 = ttir.empty() : tensor<1x14x4096xf32>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %315 = ttir.empty() : tensor<1x14x4096xf32>
    %316 = "ttir.add"(%268, %314, %315) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.pow"(%316, %48, %317) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14xf32>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %321 = ttir.empty() : tensor<1x14xf32>
    %322 = "ttir.multiply"(%320, %2, %321) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %323 = ttir.empty() : tensor<1x14x1xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %325 = ttir.empty() : tensor<1x14x1xf32>
    %326 = "ttir.add"(%324, %76, %325) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.rsqrt"(%326, %327) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.multiply"(%316, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.multiply"(%144, %332, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<14x4096xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %337 = ttir.empty() : tensor<14x16032xf32>
    %338 = "ttir.matmul"(%336, %40, %337) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %339 = ttir.empty() : tensor<1x14x16032xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %341 = ttir.empty() : tensor<1x14x4096xf32>
    %342 = "ttir.mesh_shard"(%60, %341) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %343 = ttir.empty() : tensor<1x8x19x128xbf16>
    %344 = "ttir.mesh_shard"(%131, %343) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %345 = ttir.empty() : tensor<1x8x19x128xbf16>
    %346 = "ttir.mesh_shard"(%140, %345) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.mesh_shard"(%334, %347) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<14x128256xf32>
    %350 = "ttir.mesh_shard"(%338, %349) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %351 = ttir.empty() : tensor<1x14x128256xf32>
    %352 = "ttir.mesh_shard"(%340, %351) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %342, %344, %346, %348, %350, %352 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRFlattenSlidingWindow (ttir-flatten-sliding-window) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<1x1x14x128xf32>
    %152 = "ttir.reshape"(%108, %151) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.multiply"(%150, %154, %155) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x128xf32>
    %164 = "ttir.concat"(%160, %162, %163) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %165 = ttir.empty() : tensor<1x1x14x128xf32>
    %166 = "ttir.reshape"(%122, %165) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.multiply"(%164, %168, %169) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.add"(%156, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.reshape"(%172, %173) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %176 = "ttir.reshape"(%131, %175) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %177 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %178 = "ttir.broadcast"(%176, %177) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %180 = "ttir.typecast"(%178, %179) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %181 = ttir.empty() : tensor<1x4x19x128xf32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x128x19xf32>
    %184 = "ttir.permute"(%182, %183) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %185 = ttir.empty() : tensor<4x128x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %187 = ttir.empty() : tensor<4x14x19xf32>
    %188 = "ttir.matmul"(%174, %186, %187) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x1x1x1xf32>
    %192 = "ttir.reshape"(%30, %191) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %193 = ttir.empty() : tensor<1x4x14x19xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %195 = ttir.empty() : tensor<1x4x14x19xf32>
    %196 = "ttir.multiply"(%190, %194, %195) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %197 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %198 = ttir.empty() : tensor<14x1xsi32>
    %199 = "ttir.reshape"(%197, %198) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %200 = ttir.empty() : tensor<14x19xsi32>
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %202 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %203 = ttir.empty() : tensor<1x19xsi32>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %205 = ttir.empty() : tensor<14x19xsi32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %207 = ttir.empty() : tensor<14x19xbf16>
    %208 = "ttir.ge"(%201, %206, %207) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %209 = ttir.empty() : tensor<1x1xf32>
    %210 = "ttir.reshape"(%28, %209) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %211 = ttir.empty() : tensor<14x19xf32>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %213 = ttir.empty() : tensor<14x19xf32>
    %214 = "ttir.where"(%208, %44, %212, %213) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %215 = ttir.empty() : tensor<1x19xsi32>
    %216 = "ttir.reshape"(%0, %215) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %217 = ttir.empty() : tensor<14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %219 = ttir.empty() : tensor<14x1xsi32>
    %220 = "ttir.reshape"(%10, %219) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %221 = ttir.empty() : tensor<14x19xsi32>
    %222 = "ttir.broadcast"(%220, %221) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %223 = ttir.empty() : tensor<14x19xbf16>
    %224 = "ttir.gt"(%218, %222, %223) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %225 = ttir.empty() : tensor<14x19xf32>
    %226 = "ttir.typecast"(%224, %225) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %227 = ttir.empty() : tensor<14x19xf32>
    %228 = "ttir.multiply"(%214, %226, %227) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %229 = ttir.empty() : tensor<1x1x14x19xf32>
    %230 = "ttir.reshape"(%228, %229) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x19xf32>
    %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.add"(%196, %232, %233) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x1xf32>
    %236 = "ttir.max"(%234, %235) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<1x4x14x19xf32>
    %240 = "ttir.subtract"(%234, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %241 = ttir.empty() : tensor<1x4x14x19xf32>
    %242 = "ttir.softmax"(%240, %241) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %243 = ttir.empty() : tensor<4x14x19xf32>
    %244 = "ttir.reshape"(%242, %243) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %245 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %246 = "ttir.reshape"(%140, %245) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %247 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %249 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %250 = "ttir.typecast"(%248, %249) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %251 = ttir.empty() : tensor<4x19x128xf32>
    %252 = "ttir.reshape"(%250, %251) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %253 = ttir.empty() : tensor<4x14x128xf32>
    %254 = "ttir.matmul"(%244, %252, %253) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<14x4096xf32>
    %262 = "ttir.matmul"(%260, %26, %261) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.all_reduce"(%262, %263) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<1x14x4096xf32>
    %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.add"(%60, %266, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x1x4096xf32>
    %270 = "ttir.reshape"(%34, %269) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.broadcast"(%270, %271) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14x4096xf32>
    %274 = "ttir.pow"(%268, %48, %273) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.sum"(%274, %275) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.multiply"(%276, %2, %277) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.add"(%280, %76, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x1xf32>
    %284 = "ttir.rsqrt"(%282, %283) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<1x14x4096xf32>
    %288 = "ttir.multiply"(%268, %286, %287) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %289 = ttir.empty() : tensor<1x14x4096xf32>
    %290 = "ttir.multiply"(%272, %288, %289) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %291 = ttir.empty() : tensor<14x4096xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%292, %36, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<1x14x1792xf32>
    %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.sigmoid"(%296, %297) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.multiply"(%296, %298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<14x1792xf32>
    %302 = "ttir.matmul"(%292, %24, %301) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %303 = ttir.empty() : tensor<1x14x1792xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %305 = ttir.empty() : tensor<1x14x1792xf32>
    %306 = "ttir.multiply"(%300, %304, %305) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %307 = ttir.empty() : tensor<14x1792xf32>
    %308 = "ttir.reshape"(%306, %307) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<14x4096xf32>
    %310 = "ttir.matmul"(%308, %22, %309) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %311 = ttir.empty() : tensor<14x4096xf32>
    %312 = "ttir.all_reduce"(%310, %311) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %313 = ttir.empty() : tensor<1x14x4096xf32>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %315 = ttir.empty() : tensor<1x14x4096xf32>
    %316 = "ttir.add"(%268, %314, %315) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.pow"(%316, %48, %317) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14xf32>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %321 = ttir.empty() : tensor<1x14xf32>
    %322 = "ttir.multiply"(%320, %2, %321) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %323 = ttir.empty() : tensor<1x14x1xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %325 = ttir.empty() : tensor<1x14x1xf32>
    %326 = "ttir.add"(%324, %76, %325) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.rsqrt"(%326, %327) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.multiply"(%316, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.multiply"(%144, %332, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<14x4096xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %337 = ttir.empty() : tensor<14x16032xf32>
    %338 = "ttir.matmul"(%336, %40, %337) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %339 = ttir.empty() : tensor<1x14x16032xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %341 = ttir.empty() : tensor<1x14x4096xf32>
    %342 = "ttir.mesh_shard"(%60, %341) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %343 = ttir.empty() : tensor<1x8x19x128xbf16>
    %344 = "ttir.mesh_shard"(%131, %343) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %345 = ttir.empty() : tensor<1x8x19x128xbf16>
    %346 = "ttir.mesh_shard"(%140, %345) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.mesh_shard"(%334, %347) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<14x128256xf32>
    %350 = "ttir.mesh_shard"(%338, %349) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %351 = ttir.empty() : tensor<1x14x128256xf32>
    %352 = "ttir.mesh_shard"(%340, %351) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %342, %344, %346, %348, %350, %352 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRExplicateTMs (ttir-explicate-tms) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %63 = ttir.empty() : tensor<1x14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14x1xf32>
    %72 = "ttir.reshape"(%70, %71) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %73 = ttir.empty() : tensor<1x1x1xf32>
    %74 = "ttir.reshape"(%16, %73) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %75 = ttir.empty() : tensor<1x14x1xf32>
    %76 = "ttir.broadcast"(%74, %75) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %77 = ttir.empty() : tensor<1x14x1xf32>
    %78 = "ttir.add"(%72, %76, %77) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %79 = ttir.empty() : tensor<1x14x1xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %81 = ttir.empty() : tensor<1x14x4096xf32>
    %82 = "ttir.broadcast"(%80, %81) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %83 = ttir.empty() : tensor<1x14x4096xf32>
    %84 = "ttir.multiply"(%60, %82, %83) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %85 = ttir.empty() : tensor<1x14x4096xf32>
    %86 = "ttir.multiply"(%64, %84, %85) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<1x1x14x128xf32>
    %152 = "ttir.reshape"(%108, %151) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %153 = ttir.empty() : tensor<1x4x14x128xf32>
    %154 = "ttir.broadcast"(%152, %153) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %155 = ttir.empty() : tensor<1x4x14x128xf32>
    %156 = "ttir.multiply"(%150, %154, %155) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<1x4x14x64xf32>
    %162 = "ttir.slice"(%150, %161) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x128xf32>
    %164 = "ttir.concat"(%160, %162, %163) <{dim = 3 : si32}> : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %165 = ttir.empty() : tensor<1x1x14x128xf32>
    %166 = "ttir.reshape"(%122, %165) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %167 = ttir.empty() : tensor<1x4x14x128xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %169 = ttir.empty() : tensor<1x4x14x128xf32>
    %170 = "ttir.multiply"(%164, %168, %169) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %171 = ttir.empty() : tensor<1x4x14x128xf32>
    %172 = "ttir.add"(%156, %170, %171) : (tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.reshape"(%172, %173) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %176 = "ttir.reshape"(%131, %175) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %177 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %178 = "ttir.broadcast"(%176, %177) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %179 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %180 = "ttir.typecast"(%178, %179) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %181 = ttir.empty() : tensor<1x4x19x128xf32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<1x4x19x128xf32>) -> tensor<1x4x19x128xf32>
    %183 = ttir.empty() : tensor<1x4x128x19xf32>
    %184 = "ttir.permute"(%182, %183) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x4x19x128xf32>, tensor<1x4x128x19xf32>) -> tensor<1x4x128x19xf32>
    %185 = ttir.empty() : tensor<4x128x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [4 : i32, 128 : i32, 19 : i32]}> : (tensor<1x4x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %187 = ttir.empty() : tensor<4x14x19xf32>
    %188 = "ttir.matmul"(%174, %186, %187) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.reshape"(%188, %189) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x1x1x1xf32>
    %192 = "ttir.reshape"(%30, %191) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %193 = ttir.empty() : tensor<1x4x14x19xf32>
    %194 = "ttir.broadcast"(%192, %193) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %195 = ttir.empty() : tensor<1x4x14x19xf32>
    %196 = "ttir.multiply"(%190, %194, %195) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %197 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %198 = ttir.empty() : tensor<14x1xsi32>
    %199 = "ttir.reshape"(%197, %198) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %200 = ttir.empty() : tensor<14x19xsi32>
    %201 = "ttir.broadcast"(%199, %200) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %202 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %203 = ttir.empty() : tensor<1x19xsi32>
    %204 = "ttir.reshape"(%202, %203) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %205 = ttir.empty() : tensor<14x19xsi32>
    %206 = "ttir.broadcast"(%204, %205) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %207 = ttir.empty() : tensor<14x19xbf16>
    %208 = "ttir.ge"(%201, %206, %207) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %209 = ttir.empty() : tensor<1x1xf32>
    %210 = "ttir.reshape"(%28, %209) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %211 = ttir.empty() : tensor<14x19xf32>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %213 = ttir.empty() : tensor<14x19xf32>
    %214 = "ttir.where"(%208, %44, %212, %213) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %215 = ttir.empty() : tensor<1x19xsi32>
    %216 = "ttir.reshape"(%0, %215) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %217 = ttir.empty() : tensor<14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %219 = ttir.empty() : tensor<14x1xsi32>
    %220 = "ttir.reshape"(%10, %219) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %221 = ttir.empty() : tensor<14x19xsi32>
    %222 = "ttir.broadcast"(%220, %221) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %223 = ttir.empty() : tensor<14x19xbf16>
    %224 = "ttir.gt"(%218, %222, %223) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %225 = ttir.empty() : tensor<14x19xf32>
    %226 = "ttir.typecast"(%224, %225) <{conservative_folding = false}> : (tensor<14x19xbf16>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %227 = ttir.empty() : tensor<14x19xf32>
    %228 = "ttir.multiply"(%214, %226, %227) : (tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %229 = ttir.empty() : tensor<1x1x14x19xf32>
    %230 = "ttir.reshape"(%228, %229) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x19xf32>
    %232 = "ttir.broadcast"(%230, %231) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.add"(%196, %232, %233) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x1xf32>
    %236 = "ttir.max"(%234, %235) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.broadcast"(%236, %237) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<1x4x14x19xf32>
    %240 = "ttir.subtract"(%234, %238, %239) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %241 = ttir.empty() : tensor<1x4x14x19xf32>
    %242 = "ttir.softmax"(%240, %241) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %243 = ttir.empty() : tensor<4x14x19xf32>
    %244 = "ttir.reshape"(%242, %243) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %245 = ttir.empty() : tensor<1x1x1x19x128xbf16>
    %246 = "ttir.reshape"(%140, %245) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x1x1x19x128xbf16>) -> tensor<1x1x1x19x128xbf16>
    %247 = ttir.empty() : tensor<1x1x4x19x128xbf16>
    %248 = "ttir.broadcast"(%246, %247) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<1x1x1x19x128xbf16>, tensor<1x1x4x19x128xbf16>) -> tensor<1x1x4x19x128xbf16>
    %249 = ttir.empty() : tensor<1x1x4x19x128xf32>
    %250 = "ttir.typecast"(%248, %249) <{conservative_folding = false}> : (tensor<1x1x4x19x128xbf16>, tensor<1x1x4x19x128xf32>) -> tensor<1x1x4x19x128xf32>
    %251 = ttir.empty() : tensor<4x19x128xf32>
    %252 = "ttir.reshape"(%250, %251) <{shape = [4 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x4x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %253 = ttir.empty() : tensor<4x14x128xf32>
    %254 = "ttir.matmul"(%244, %252, %253) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %255 = ttir.empty() : tensor<1x4x14x128xf32>
    %256 = "ttir.reshape"(%254, %255) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %257 = ttir.empty() : tensor<1x14x4x128xf32>
    %258 = "ttir.permute"(%256, %257) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %259 = ttir.empty() : tensor<14x512xf32>
    %260 = "ttir.reshape"(%258, %259) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %261 = ttir.empty() : tensor<14x4096xf32>
    %262 = "ttir.matmul"(%260, %26, %261) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.all_reduce"(%262, %263) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<1x14x4096xf32>
    %266 = "ttir.reshape"(%264, %265) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.add"(%60, %266, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x1x4096xf32>
    %270 = "ttir.reshape"(%34, %269) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.broadcast"(%270, %271) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14x4096xf32>
    %274 = "ttir.pow"(%268, %48, %273) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.sum"(%274, %275) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.multiply"(%276, %2, %277) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.add"(%280, %76, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x1xf32>
    %284 = "ttir.rsqrt"(%282, %283) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.broadcast"(%284, %285) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<1x14x4096xf32>
    %288 = "ttir.multiply"(%268, %286, %287) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %289 = ttir.empty() : tensor<1x14x4096xf32>
    %290 = "ttir.multiply"(%272, %288, %289) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %291 = ttir.empty() : tensor<14x4096xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%292, %36, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<1x14x1792xf32>
    %296 = "ttir.reshape"(%294, %295) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %297 = ttir.empty() : tensor<1x14x1792xf32>
    %298 = "ttir.sigmoid"(%296, %297) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %299 = ttir.empty() : tensor<1x14x1792xf32>
    %300 = "ttir.multiply"(%296, %298, %299) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %301 = ttir.empty() : tensor<14x1792xf32>
    %302 = "ttir.matmul"(%292, %24, %301) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %303 = ttir.empty() : tensor<1x14x1792xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 1792 : i32]}> : (tensor<14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %305 = ttir.empty() : tensor<1x14x1792xf32>
    %306 = "ttir.multiply"(%300, %304, %305) : (tensor<1x14x1792xf32>, tensor<1x14x1792xf32>, tensor<1x14x1792xf32>) -> tensor<1x14x1792xf32>
    %307 = ttir.empty() : tensor<14x1792xf32>
    %308 = "ttir.reshape"(%306, %307) <{shape = [14 : i32, 1792 : i32]}> : (tensor<1x14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %309 = ttir.empty() : tensor<14x4096xf32>
    %310 = "ttir.matmul"(%308, %22, %309) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %311 = ttir.empty() : tensor<14x4096xf32>
    %312 = "ttir.all_reduce"(%310, %311) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %313 = ttir.empty() : tensor<1x14x4096xf32>
    %314 = "ttir.reshape"(%312, %313) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %315 = ttir.empty() : tensor<1x14x4096xf32>
    %316 = "ttir.add"(%268, %314, %315) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.pow"(%316, %48, %317) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14xf32>
    %320 = "ttir.sum"(%318, %319) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %321 = ttir.empty() : tensor<1x14xf32>
    %322 = "ttir.multiply"(%320, %2, %321) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %323 = ttir.empty() : tensor<1x14x1xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %325 = ttir.empty() : tensor<1x14x1xf32>
    %326 = "ttir.add"(%324, %76, %325) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %327 = ttir.empty() : tensor<1x14x1xf32>
    %328 = "ttir.rsqrt"(%326, %327) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.broadcast"(%328, %329) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x14x4096xf32>
    %332 = "ttir.multiply"(%316, %330, %331) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %333 = ttir.empty() : tensor<1x14x4096xf32>
    %334 = "ttir.multiply"(%144, %332, %333) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %335 = ttir.empty() : tensor<14x4096xf32>
    %336 = "ttir.reshape"(%334, %335) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %337 = ttir.empty() : tensor<14x16032xf32>
    %338 = "ttir.matmul"(%336, %40, %337) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %339 = ttir.empty() : tensor<1x14x16032xf32>
    %340 = "ttir.reshape"(%338, %339) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %341 = ttir.empty() : tensor<1x14x4096xf32>
    %342 = "ttir.mesh_shard"(%60, %341) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %343 = ttir.empty() : tensor<1x8x19x128xbf16>
    %344 = "ttir.mesh_shard"(%131, %343) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %345 = ttir.empty() : tensor<1x8x19x128xbf16>
    %346 = "ttir.mesh_shard"(%140, %345) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %347 = ttir.empty() : tensor<1x14x4096xf32>
    %348 = "ttir.mesh_shard"(%334, %347) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %349 = ttir.empty() : tensor<14x128256xf32>
    %350 = "ttir.mesh_shard"(%338, %349) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %351 = ttir.empty() : tensor<1x14x128256xf32>
    %352 = "ttir.mesh_shard"(%340, %351) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %342, %344, %346, %348, %350, %352 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIREraseInverseOps (ttir-erase-inverse-ops) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %63 = ttir.empty() : tensor<14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x1x1xf32>
    %72 = "ttir.reshape"(%16, %71) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %73 = ttir.empty() : tensor<1x14x1xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %75 = ttir.empty() : tensor<1x14xf32>
    %76 = "ttir.reshape"(%74, %75) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %77 = ttir.empty() : tensor<1x14xf32>
    %78 = "ttir.add"(%70, %76, %77) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %79 = ttir.empty() : tensor<1x14xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %81 = ttir.empty() : tensor<14x1xf32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %83 = ttir.empty() : tensor<14x4096xf32>
    %84 = "ttir.broadcast"(%82, %83) <{broadcast_dimensions = array<i64: 1, 4096>}> : (tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %85 = ttir.empty() : tensor<14x4096xf32>
    %86 = "ttir.multiply"(%58, %84, %85) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.multiply"(%64, %86, %87) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<4x14x128xf32>
    %152 = "ttir.broadcast"(%108, %151) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %153 = ttir.empty() : tensor<4x14x128xf32>
    %154 = "ttir.reshape"(%150, %153) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %155 = ttir.empty() : tensor<4x14x128xf32>
    %156 = "ttir.multiply"(%154, %152, %155) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<4x14x64xf32>
    %162 = "ttir.reshape"(%160, %161) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.slice"(%150, %163) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<4x14x64xf32>
    %166 = "ttir.reshape"(%164, %165) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %167 = ttir.empty() : tensor<4x14x128xf32>
    %168 = "ttir.concat"(%162, %166, %167) <{dim = 2 : si32}> : (tensor<4x14x64xf32>, tensor<4x14x64xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %169 = ttir.empty() : tensor<4x14x128xf32>
    %170 = "ttir.broadcast"(%122, %169) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %171 = ttir.empty() : tensor<4x14x128xf32>
    %172 = "ttir.multiply"(%168, %170, %171) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.add"(%156, %172, %173) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x128x19xbf16>
    %176 = "ttir.permute"(%131, %175) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16>, tensor<1x1x128x19xbf16>) -> tensor<1x1x128x19xbf16>
    %177 = ttir.empty() : tensor<1x128x19xbf16>
    %178 = "ttir.reshape"(%176, %177) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16>, tensor<1x128x19xbf16>) -> tensor<1x128x19xbf16>
    %179 = ttir.empty() : tensor<4x128x19xbf16>
    %180 = "ttir.broadcast"(%178, %179) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xbf16>, tensor<4x128x19xbf16>) -> tensor<4x128x19xbf16>
    %181 = ttir.empty() : tensor<4x128x19xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<4x128x19xbf16>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %183 = ttir.empty() : tensor<4x14x19xf32>
    %184 = "ttir.matmul"(%174, %182, %183) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %185 = ttir.empty() : tensor<1x4x14x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %187 = ttir.empty() : tensor<1x1x1x1xf32>
    %188 = "ttir.reshape"(%30, %187) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.broadcast"(%188, %189) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x4x14x19xf32>
    %192 = "ttir.multiply"(%186, %190, %191) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %193 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %194 = ttir.empty() : tensor<14x1xsi32>
    %195 = "ttir.reshape"(%193, %194) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %196 = ttir.empty() : tensor<14x19xsi32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %198 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %199 = ttir.empty() : tensor<1x19xsi32>
    %200 = "ttir.reshape"(%198, %199) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %201 = ttir.empty() : tensor<14x19xsi32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %203 = ttir.empty() : tensor<14x19xbf16>
    %204 = "ttir.ge"(%197, %202, %203) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %205 = ttir.empty() : tensor<1x1xf32>
    %206 = "ttir.reshape"(%28, %205) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %207 = ttir.empty() : tensor<14x19xf32>
    %208 = "ttir.broadcast"(%206, %207) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %209 = ttir.empty() : tensor<14x19xf32>
    %210 = "ttir.where"(%204, %44, %208, %209) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %211 = ttir.empty() : tensor<1x1x1x19xsi32>
    %212 = "ttir.reshape"(%0, %211) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x1x1x19xsi32>) -> tensor<1x1x1x19xsi32>
    %213 = ttir.empty() : tensor<1x1x14x19xsi32>
    %214 = "ttir.broadcast"(%212, %213) <{broadcast_dimensions = array<i64: 1, 1, 14, 1>}> : (tensor<1x1x1x19xsi32>, tensor<1x1x14x19xsi32>) -> tensor<1x1x14x19xsi32>
    %215 = ttir.empty() : tensor<1x1x14x1xsi32>
    %216 = "ttir.reshape"(%10, %215) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<1x1x14x1xsi32>) -> tensor<1x1x14x1xsi32>
    %217 = ttir.empty() : tensor<1x1x14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x1x14x1xsi32>, tensor<1x1x14x19xsi32>) -> tensor<1x1x14x19xsi32>
    %219 = ttir.empty() : tensor<1x1x14x19xbf16>
    %220 = "ttir.gt"(%214, %218, %219) : (tensor<1x1x14x19xsi32>, tensor<1x1x14x19xsi32>, tensor<1x1x14x19xbf16>) -> tensor<1x1x14x19xbf16>
    %221 = ttir.empty() : tensor<1x1x14x19xf32>
    %222 = "ttir.typecast"(%220, %221) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %223 = ttir.empty() : tensor<1x1x14x19xf32>
    %224 = "ttir.reshape"(%210, %223) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %225 = ttir.empty() : tensor<1x1x14x19xf32>
    %226 = "ttir.multiply"(%224, %222, %225) : (tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %227 = ttir.empty() : tensor<1x4x14x19xf32>
    %228 = "ttir.broadcast"(%226, %227) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %229 = ttir.empty() : tensor<1x4x14x19xf32>
    %230 = "ttir.add"(%192, %228, %229) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x1xf32>
    %232 = "ttir.max"(%230, %231) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.broadcast"(%232, %233) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x19xf32>
    %236 = "ttir.subtract"(%230, %234, %235) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.softmax"(%236, %237) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<4x14x19xf32>
    %240 = "ttir.reshape"(%238, %239) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %241 = ttir.empty() : tensor<1x19x128xbf16>
    %242 = "ttir.reshape"(%140, %241) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x19x128xbf16>) -> tensor<1x19x128xbf16>
    %243 = ttir.empty() : tensor<4x19x128xbf16>
    %244 = "ttir.broadcast"(%242, %243) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xbf16>, tensor<4x19x128xbf16>) -> tensor<4x19x128xbf16>
    %245 = ttir.empty() : tensor<4x19x128xf32>
    %246 = "ttir.typecast"(%244, %245) <{conservative_folding = false}> : (tensor<4x19x128xbf16>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %247 = ttir.empty() : tensor<4x14x128xf32>
    %248 = "ttir.matmul"(%240, %246, %247) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %249 = ttir.empty() : tensor<1x4x14x128xf32>
    %250 = "ttir.reshape"(%248, %249) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %251 = ttir.empty() : tensor<1x14x4x128xf32>
    %252 = "ttir.permute"(%250, %251) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %253 = ttir.empty() : tensor<14x512xf32>
    %254 = "ttir.reshape"(%252, %253) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %255 = ttir.empty() : tensor<14x4096xf32>
    %256 = "ttir.matmul"(%254, %26, %255) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %257 = ttir.empty() : tensor<14x4096xf32>
    %258 = "ttir.all_reduce"(%256, %257) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %259 = ttir.empty() : tensor<14x4096xf32>
    %260 = "ttir.add"(%58, %258, %259) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %261 = ttir.empty() : tensor<1x14x4096xf32>
    %262 = "ttir.reshape"(%260, %261) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %263 = ttir.empty() : tensor<1x4096xf32>
    %264 = "ttir.reshape"(%34, %263) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %265 = ttir.empty() : tensor<14x4096xf32>
    %266 = "ttir.broadcast"(%264, %265) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.pow"(%262, %48, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x14xf32>
    %270 = "ttir.sum"(%268, %269) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %271 = ttir.empty() : tensor<1x14xf32>
    %272 = "ttir.multiply"(%270, %2, %271) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %273 = ttir.empty() : tensor<1x14xf32>
    %274 = "ttir.reshape"(%74, %273) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.add"(%272, %274, %275) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.rsqrt"(%276, %277) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %281 = ttir.empty() : tensor<14x4096xf32>
    %282 = "ttir.broadcast"(%280, %281) <{broadcast_dimensions = array<i64: 1, 4096>}> : (tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %283 = ttir.empty() : tensor<14x4096xf32>
    %284 = "ttir.multiply"(%260, %282, %283) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %285 = ttir.empty() : tensor<14x4096xf32>
    %286 = "ttir.multiply"(%266, %284, %285) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %287 = ttir.empty() : tensor<14x1792xf32>
    %288 = "ttir.matmul"(%286, %36, %287) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %289 = ttir.empty() : tensor<14x1792xf32>
    %290 = "ttir.sigmoid"(%288, %289) : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %291 = ttir.empty() : tensor<14x1792xf32>
    %292 = "ttir.multiply"(%288, %290, %291) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%286, %24, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<14x1792xf32>
    %296 = "ttir.multiply"(%292, %294, %295) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %297 = ttir.empty() : tensor<14x4096xf32>
    %298 = "ttir.matmul"(%296, %22, %297) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %299 = ttir.empty() : tensor<14x4096xf32>
    %300 = "ttir.all_reduce"(%298, %299) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %301 = ttir.empty() : tensor<14x4096xf32>
    %302 = "ttir.add"(%260, %300, %301) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %303 = ttir.empty() : tensor<1x14x4096xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %305 = ttir.empty() : tensor<1x14x4096xf32>
    %306 = "ttir.pow"(%304, %48, %305) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %307 = ttir.empty() : tensor<1x14xf32>
    %308 = "ttir.sum"(%306, %307) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %309 = ttir.empty() : tensor<1x14xf32>
    %310 = "ttir.multiply"(%308, %2, %309) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %311 = ttir.empty() : tensor<1x14x1xf32>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %313 = ttir.empty() : tensor<1x14x1xf32>
    %314 = "ttir.add"(%312, %74, %313) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %315 = ttir.empty() : tensor<1x14x1xf32>
    %316 = "ttir.rsqrt"(%314, %315) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.broadcast"(%316, %317) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14x4096xf32>
    %320 = "ttir.multiply"(%304, %318, %319) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %321 = ttir.empty() : tensor<1x14x4096xf32>
    %322 = "ttir.multiply"(%144, %320, %321) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %323 = ttir.empty() : tensor<14x4096xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %325 = ttir.empty() : tensor<14x16032xf32>
    %326 = "ttir.matmul"(%324, %40, %325) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %327 = ttir.empty() : tensor<1x14x16032xf32>
    %328 = "ttir.reshape"(%326, %327) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.mesh_shard"(%60, %329) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x8x19x128xbf16>
    %332 = "ttir.mesh_shard"(%131, %331) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %333 = ttir.empty() : tensor<1x8x19x128xbf16>
    %334 = "ttir.mesh_shard"(%140, %333) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %335 = ttir.empty() : tensor<1x14x4096xf32>
    %336 = "ttir.mesh_shard"(%322, %335) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %337 = ttir.empty() : tensor<14x128256xf32>
    %338 = "ttir.mesh_shard"(%326, %337) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %339 = ttir.empty() : tensor<1x14x128256xf32>
    %340 = "ttir.mesh_shard"(%328, %339) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %330, %332, %334, %336, %338, %340 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRFusing (ttir-fusing) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<14x19xf32>
    %44 = "ttir.broadcast"(%42, %43) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %45 = ttir.empty() : tensor<1x1x1xf32>
    %46 = "ttir.reshape"(%3, %45) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %47 = ttir.empty() : tensor<1x14x4096xf32>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 14, 4096>}> : (tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %49 = ttir.empty() : tensor<1x1x1x1xbf16>
    %50 = "ttir.reshape"(%4, %49) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %51 = ttir.empty() : tensor<1x1x19x128xbf16>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %53 = ttir.empty() : tensor<1x14xui32>
    %54 = "ttir.typecast"(%6, %53) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %55 = ttir.empty() : tensor<14xui32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %57 = ttir.empty() : tensor<14x4096xf32>
    %58 = "ttir.embedding"(%56, %8, %57) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.reshape"(%58, %59) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x4096xf32>
    %62 = "ttir.reshape"(%18, %61) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %63 = ttir.empty() : tensor<14x4096xf32>
    %64 = "ttir.broadcast"(%62, %63) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %65 = ttir.empty() : tensor<1x14x4096xf32>
    %66 = "ttir.pow"(%60, %48, %65) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %67 = ttir.empty() : tensor<1x14xf32>
    %68 = "ttir.sum"(%66, %67) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.multiply"(%68, %2, %69) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x1x1xf32>
    %72 = "ttir.reshape"(%16, %71) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %73 = ttir.empty() : tensor<1x14x1xf32>
    %74 = "ttir.broadcast"(%72, %73) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %75 = ttir.empty() : tensor<1x14xf32>
    %76 = "ttir.reshape"(%74, %75) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %77 = ttir.empty() : tensor<1x14xf32>
    %78 = "ttir.add"(%70, %76, %77) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %79 = ttir.empty() : tensor<1x14xf32>
    %80 = "ttir.rsqrt"(%78, %79) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %81 = ttir.empty() : tensor<14x1xf32>
    %82 = "ttir.reshape"(%80, %81) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %83 = ttir.empty() : tensor<14x4096xf32>
    %84 = "ttir.broadcast"(%82, %83) <{broadcast_dimensions = array<i64: 1, 4096>}> : (tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %85 = ttir.empty() : tensor<14x4096xf32>
    %86 = "ttir.multiply"(%58, %84, %85) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %87 = ttir.empty() : tensor<14x4096xf32>
    %88 = "ttir.multiply"(%64, %86, %87) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %89 = ttir.empty() : tensor<14x128xf32>
    %90 = "ttir.matmul"(%88, %14, %89) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %91 = ttir.empty() : tensor<1x14x1x128xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %93 = ttir.empty() : tensor<1x1x14x128xf32>
    %94 = "ttir.permute"(%92, %93) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %95 = ttir.empty() : tensor<1x64x1xf32>
    %96 = "ttir.reshape"(%12, %95) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %97 = ttir.empty() : tensor<14xf32>
    %98 = "ttir.typecast"(%10, %97) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %99 = ttir.empty() : tensor<1x1x14xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %101 = ttir.empty() : tensor<1x64x14xf32>
    %102 = "ttir.matmul"(%96, %100, %101) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %103 = ttir.empty() : tensor<1x14x64xf32>
    %104 = "ttir.permute"(%102, %103) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %105 = ttir.empty() : tensor<1x14x128xf32>
    %106 = "ttir.concat"(%104, %104, %105) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %107 = ttir.empty() : tensor<1x14x128xf32>
    %108 = "ttir.cos"(%106, %107) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %109 = ttir.empty() : tensor<1x1x14x128xf32>
    %110 = "ttir.reshape"(%108, %109) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.multiply"(%94, %110, %111) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x1x14x64xf32>
    %114 = "ttir.slice"(%94, %113) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %115 = ttir.empty() : tensor<1x1x14x64xf32>
    %116 = "ttir.neg"(%114, %115) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %117 = ttir.empty() : tensor<1x1x14x64xf32>
    %118 = "ttir.slice"(%94, %117) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.concat"(%116, %118, %119) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x14x128xf32>
    %122 = "ttir.sin"(%106, %121) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %123 = ttir.empty() : tensor<1x1x14x128xf32>
    %124 = "ttir.reshape"(%122, %123) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %125 = ttir.empty() : tensor<1x1x14x128xf32>
    %126 = "ttir.multiply"(%120, %124, %125) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %127 = ttir.empty() : tensor<1x1x14x128xf32>
    %128 = "ttir.add"(%112, %126, %127) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %129 = ttir.empty() : tensor<1x1x14x128xbf16>
    %130 = "ttir.typecast"(%128, %129) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %131 = "ttir.fill_cache"(%52, %130) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %132 = ttir.empty() : tensor<14x128xf32>
    %133 = "ttir.matmul"(%88, %20, %132) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %134 = ttir.empty() : tensor<14x128xbf16>
    %135 = "ttir.typecast"(%133, %134) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %136 = ttir.empty() : tensor<1x14x1x128xbf16>
    %137 = "ttir.reshape"(%135, %136) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %138 = ttir.empty() : tensor<1x1x14x128xbf16>
    %139 = "ttir.permute"(%137, %138) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %140 = "ttir.fill_cache"(%52, %139) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %141 = ttir.empty() : tensor<1x1x4096xf32>
    %142 = "ttir.reshape"(%38, %141) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %143 = ttir.empty() : tensor<1x14x4096xf32>
    %144 = "ttir.broadcast"(%142, %143) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %145 = ttir.empty() : tensor<14x512xf32>
    %146 = "ttir.matmul"(%88, %32, %145) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %147 = ttir.empty() : tensor<1x14x4x128xf32>
    %148 = "ttir.reshape"(%146, %147) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %149 = ttir.empty() : tensor<1x4x14x128xf32>
    %150 = "ttir.permute"(%148, %149) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %151 = ttir.empty() : tensor<4x14x128xf32>
    %152 = "ttir.broadcast"(%108, %151) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %153 = ttir.empty() : tensor<4x14x128xf32>
    %154 = "ttir.reshape"(%150, %153) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %155 = ttir.empty() : tensor<4x14x128xf32>
    %156 = "ttir.multiply"(%154, %152, %155) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %157 = ttir.empty() : tensor<1x4x14x64xf32>
    %158 = "ttir.slice"(%150, %157) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %159 = ttir.empty() : tensor<1x4x14x64xf32>
    %160 = "ttir.neg"(%158, %159) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %161 = ttir.empty() : tensor<4x14x64xf32>
    %162 = "ttir.reshape"(%160, %161) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %163 = ttir.empty() : tensor<1x4x14x64xf32>
    %164 = "ttir.slice"(%150, %163) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %165 = ttir.empty() : tensor<4x14x64xf32>
    %166 = "ttir.reshape"(%164, %165) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %167 = ttir.empty() : tensor<4x14x128xf32>
    %168 = "ttir.concat"(%162, %166, %167) <{dim = 2 : si32}> : (tensor<4x14x64xf32>, tensor<4x14x64xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %169 = ttir.empty() : tensor<4x14x128xf32>
    %170 = "ttir.broadcast"(%122, %169) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %171 = ttir.empty() : tensor<4x14x128xf32>
    %172 = "ttir.multiply"(%168, %170, %171) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %173 = ttir.empty() : tensor<4x14x128xf32>
    %174 = "ttir.add"(%156, %172, %173) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %175 = ttir.empty() : tensor<1x1x128x19xbf16>
    %176 = "ttir.permute"(%131, %175) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16>, tensor<1x1x128x19xbf16>) -> tensor<1x1x128x19xbf16>
    %177 = ttir.empty() : tensor<1x128x19xbf16>
    %178 = "ttir.reshape"(%176, %177) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16>, tensor<1x128x19xbf16>) -> tensor<1x128x19xbf16>
    %179 = ttir.empty() : tensor<4x128x19xbf16>
    %180 = "ttir.broadcast"(%178, %179) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xbf16>, tensor<4x128x19xbf16>) -> tensor<4x128x19xbf16>
    %181 = ttir.empty() : tensor<4x128x19xf32>
    %182 = "ttir.typecast"(%180, %181) <{conservative_folding = false}> : (tensor<4x128x19xbf16>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %183 = ttir.empty() : tensor<4x14x19xf32>
    %184 = "ttir.matmul"(%174, %182, %183) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %185 = ttir.empty() : tensor<1x4x14x19xf32>
    %186 = "ttir.reshape"(%184, %185) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %187 = ttir.empty() : tensor<1x1x1x1xf32>
    %188 = "ttir.reshape"(%30, %187) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %189 = ttir.empty() : tensor<1x4x14x19xf32>
    %190 = "ttir.broadcast"(%188, %189) <{broadcast_dimensions = array<i64: 1, 4, 14, 19>}> : (tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %191 = ttir.empty() : tensor<1x4x14x19xf32>
    %192 = "ttir.multiply"(%186, %190, %191) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %193 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %194 = ttir.empty() : tensor<14x1xsi32>
    %195 = "ttir.reshape"(%193, %194) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %196 = ttir.empty() : tensor<14x19xsi32>
    %197 = "ttir.broadcast"(%195, %196) <{broadcast_dimensions = array<i64: 1, 19>}> : (tensor<14x1xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %198 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %199 = ttir.empty() : tensor<1x19xsi32>
    %200 = "ttir.reshape"(%198, %199) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %201 = ttir.empty() : tensor<14x19xsi32>
    %202 = "ttir.broadcast"(%200, %201) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x19xsi32>, tensor<14x19xsi32>) -> tensor<14x19xsi32>
    %203 = ttir.empty() : tensor<14x19xbf16>
    %204 = "ttir.ge"(%197, %202, %203) : (tensor<14x19xsi32>, tensor<14x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %205 = ttir.empty() : tensor<1x1xf32>
    %206 = "ttir.reshape"(%28, %205) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %207 = ttir.empty() : tensor<14x19xf32>
    %208 = "ttir.broadcast"(%206, %207) <{broadcast_dimensions = array<i64: 14, 19>}> : (tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %209 = ttir.empty() : tensor<14x19xf32>
    %210 = "ttir.where"(%204, %44, %208, %209) : (tensor<14x19xbf16>, tensor<14x19xf32>, tensor<14x19xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %211 = ttir.empty() : tensor<1x1x1x19xsi32>
    %212 = "ttir.reshape"(%0, %211) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x1x1x19xsi32>) -> tensor<1x1x1x19xsi32>
    %213 = ttir.empty() : tensor<1x1x14x19xsi32>
    %214 = "ttir.broadcast"(%212, %213) <{broadcast_dimensions = array<i64: 1, 1, 14, 1>}> : (tensor<1x1x1x19xsi32>, tensor<1x1x14x19xsi32>) -> tensor<1x1x14x19xsi32>
    %215 = ttir.empty() : tensor<1x1x14x1xsi32>
    %216 = "ttir.reshape"(%10, %215) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<1x1x14x1xsi32>) -> tensor<1x1x14x1xsi32>
    %217 = ttir.empty() : tensor<1x1x14x19xsi32>
    %218 = "ttir.broadcast"(%216, %217) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x1x14x1xsi32>, tensor<1x1x14x19xsi32>) -> tensor<1x1x14x19xsi32>
    %219 = ttir.empty() : tensor<1x1x14x19xbf16>
    %220 = "ttir.gt"(%214, %218, %219) : (tensor<1x1x14x19xsi32>, tensor<1x1x14x19xsi32>, tensor<1x1x14x19xbf16>) -> tensor<1x1x14x19xbf16>
    %221 = ttir.empty() : tensor<1x1x14x19xf32>
    %222 = "ttir.typecast"(%220, %221) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %223 = ttir.empty() : tensor<1x1x14x19xf32>
    %224 = "ttir.reshape"(%210, %223) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %225 = ttir.empty() : tensor<1x1x14x19xf32>
    %226 = "ttir.multiply"(%224, %222, %225) : (tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %227 = ttir.empty() : tensor<1x4x14x19xf32>
    %228 = "ttir.broadcast"(%226, %227) <{broadcast_dimensions = array<i64: 1, 4, 1, 1>}> : (tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %229 = ttir.empty() : tensor<1x4x14x19xf32>
    %230 = "ttir.add"(%192, %228, %229) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %231 = ttir.empty() : tensor<1x4x14x1xf32>
    %232 = "ttir.max"(%230, %231) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %233 = ttir.empty() : tensor<1x4x14x19xf32>
    %234 = "ttir.broadcast"(%232, %233) <{broadcast_dimensions = array<i64: 1, 1, 1, 19>}> : (tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %235 = ttir.empty() : tensor<1x4x14x19xf32>
    %236 = "ttir.subtract"(%230, %234, %235) : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %237 = ttir.empty() : tensor<1x4x14x19xf32>
    %238 = "ttir.softmax"(%236, %237) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %239 = ttir.empty() : tensor<4x14x19xf32>
    %240 = "ttir.reshape"(%238, %239) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %241 = ttir.empty() : tensor<1x19x128xbf16>
    %242 = "ttir.reshape"(%140, %241) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x19x128xbf16>) -> tensor<1x19x128xbf16>
    %243 = ttir.empty() : tensor<4x19x128xbf16>
    %244 = "ttir.broadcast"(%242, %243) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xbf16>, tensor<4x19x128xbf16>) -> tensor<4x19x128xbf16>
    %245 = ttir.empty() : tensor<4x19x128xf32>
    %246 = "ttir.typecast"(%244, %245) <{conservative_folding = false}> : (tensor<4x19x128xbf16>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %247 = ttir.empty() : tensor<4x14x128xf32>
    %248 = "ttir.matmul"(%240, %246, %247) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %249 = ttir.empty() : tensor<1x4x14x128xf32>
    %250 = "ttir.reshape"(%248, %249) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %251 = ttir.empty() : tensor<1x14x4x128xf32>
    %252 = "ttir.permute"(%250, %251) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %253 = ttir.empty() : tensor<14x512xf32>
    %254 = "ttir.reshape"(%252, %253) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %255 = ttir.empty() : tensor<14x4096xf32>
    %256 = "ttir.matmul"(%254, %26, %255) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %257 = ttir.empty() : tensor<14x4096xf32>
    %258 = "ttir.all_reduce"(%256, %257) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %259 = ttir.empty() : tensor<14x4096xf32>
    %260 = "ttir.add"(%58, %258, %259) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %261 = ttir.empty() : tensor<1x14x4096xf32>
    %262 = "ttir.reshape"(%260, %261) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %263 = ttir.empty() : tensor<1x4096xf32>
    %264 = "ttir.reshape"(%34, %263) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %265 = ttir.empty() : tensor<14x4096xf32>
    %266 = "ttir.broadcast"(%264, %265) <{broadcast_dimensions = array<i64: 14, 1>}> : (tensor<1x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %267 = ttir.empty() : tensor<1x14x4096xf32>
    %268 = "ttir.pow"(%262, %48, %267) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %269 = ttir.empty() : tensor<1x14xf32>
    %270 = "ttir.sum"(%268, %269) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %271 = ttir.empty() : tensor<1x14xf32>
    %272 = "ttir.multiply"(%270, %2, %271) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %273 = ttir.empty() : tensor<1x14xf32>
    %274 = "ttir.reshape"(%74, %273) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.add"(%272, %274, %275) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14xf32>
    %278 = "ttir.rsqrt"(%276, %277) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %279 = ttir.empty() : tensor<14x1xf32>
    %280 = "ttir.reshape"(%278, %279) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %281 = ttir.empty() : tensor<14x4096xf32>
    %282 = "ttir.broadcast"(%280, %281) <{broadcast_dimensions = array<i64: 1, 4096>}> : (tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %283 = ttir.empty() : tensor<14x4096xf32>
    %284 = "ttir.multiply"(%260, %282, %283) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %285 = ttir.empty() : tensor<14x4096xf32>
    %286 = "ttir.multiply"(%266, %284, %285) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %287 = ttir.empty() : tensor<14x1792xf32>
    %288 = "ttir.matmul"(%286, %36, %287) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %289 = ttir.empty() : tensor<14x1792xf32>
    %290 = "ttir.sigmoid"(%288, %289) : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %291 = ttir.empty() : tensor<14x1792xf32>
    %292 = "ttir.multiply"(%288, %290, %291) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %293 = ttir.empty() : tensor<14x1792xf32>
    %294 = "ttir.matmul"(%286, %24, %293) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %295 = ttir.empty() : tensor<14x1792xf32>
    %296 = "ttir.multiply"(%292, %294, %295) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %297 = ttir.empty() : tensor<14x4096xf32>
    %298 = "ttir.matmul"(%296, %22, %297) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %299 = ttir.empty() : tensor<14x4096xf32>
    %300 = "ttir.all_reduce"(%298, %299) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %301 = ttir.empty() : tensor<14x4096xf32>
    %302 = "ttir.add"(%260, %300, %301) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %303 = ttir.empty() : tensor<1x14x4096xf32>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %305 = ttir.empty() : tensor<1x14x4096xf32>
    %306 = "ttir.pow"(%304, %48, %305) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %307 = ttir.empty() : tensor<1x14xf32>
    %308 = "ttir.sum"(%306, %307) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %309 = ttir.empty() : tensor<1x14xf32>
    %310 = "ttir.multiply"(%308, %2, %309) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %311 = ttir.empty() : tensor<1x14x1xf32>
    %312 = "ttir.reshape"(%310, %311) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %313 = ttir.empty() : tensor<1x14x1xf32>
    %314 = "ttir.add"(%312, %74, %313) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %315 = ttir.empty() : tensor<1x14x1xf32>
    %316 = "ttir.rsqrt"(%314, %315) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %317 = ttir.empty() : tensor<1x14x4096xf32>
    %318 = "ttir.broadcast"(%316, %317) <{broadcast_dimensions = array<i64: 1, 1, 4096>}> : (tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %319 = ttir.empty() : tensor<1x14x4096xf32>
    %320 = "ttir.multiply"(%304, %318, %319) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %321 = ttir.empty() : tensor<1x14x4096xf32>
    %322 = "ttir.multiply"(%144, %320, %321) : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %323 = ttir.empty() : tensor<14x4096xf32>
    %324 = "ttir.reshape"(%322, %323) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %325 = ttir.empty() : tensor<14x16032xf32>
    %326 = "ttir.matmul"(%324, %40, %325) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %327 = ttir.empty() : tensor<1x14x16032xf32>
    %328 = "ttir.reshape"(%326, %327) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %329 = ttir.empty() : tensor<1x14x4096xf32>
    %330 = "ttir.mesh_shard"(%60, %329) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %331 = ttir.empty() : tensor<1x8x19x128xbf16>
    %332 = "ttir.mesh_shard"(%131, %331) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %333 = ttir.empty() : tensor<1x8x19x128xbf16>
    %334 = "ttir.mesh_shard"(%140, %333) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %335 = ttir.empty() : tensor<1x14x4096xf32>
    %336 = "ttir.mesh_shard"(%322, %335) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %337 = ttir.empty() : tensor<14x128256xf32>
    %338 = "ttir.mesh_shard"(%326, %337) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %339 = ttir.empty() : tensor<1x14x128256xf32>
    %340 = "ttir.mesh_shard"(%328, %339) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %330, %332, %334, %336, %338, %340 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRImplicitBroadcastFold (ttir-implicit-broadcast-fold) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<1x1x1xf32>
    %44 = "ttir.reshape"(%3, %43) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %45 = ttir.empty() : tensor<1x1x1x1xbf16>
    %46 = "ttir.reshape"(%4, %45) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %47 = ttir.empty() : tensor<1x1x19x128xbf16>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %49 = ttir.empty() : tensor<1x14xui32>
    %50 = "ttir.typecast"(%6, %49) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %51 = ttir.empty() : tensor<14xui32>
    %52 = "ttir.reshape"(%50, %51) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %53 = ttir.empty() : tensor<14x4096xf32>
    %54 = "ttir.embedding"(%52, %8, %53) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %55 = ttir.empty() : tensor<1x14x4096xf32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %57 = ttir.empty() : tensor<1x4096xf32>
    %58 = "ttir.reshape"(%18, %57) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.pow"(%56, %44, %59) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x14xf32>
    %62 = "ttir.sum"(%60, %61) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %63 = ttir.empty() : tensor<1x14xf32>
    %64 = "ttir.multiply"(%62, %2, %63) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %65 = ttir.empty() : tensor<1x1x1xf32>
    %66 = "ttir.reshape"(%16, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %67 = ttir.empty() : tensor<1x14x1xf32>
    %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14xf32>
    %72 = "ttir.add"(%64, %70, %71) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %73 = ttir.empty() : tensor<1x14xf32>
    %74 = "ttir.rsqrt"(%72, %73) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %75 = ttir.empty() : tensor<14x1xf32>
    %76 = "ttir.reshape"(%74, %75) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %77 = ttir.empty() : tensor<14x4096xf32>
    %78 = "ttir.multiply"(%54, %76, %77) : (tensor<14x4096xf32>, tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %79 = ttir.empty() : tensor<14x4096xf32>
    %80 = "ttir.multiply"(%58, %78, %79) : (tensor<1x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %81 = ttir.empty() : tensor<14x128xf32>
    %82 = "ttir.matmul"(%80, %14, %81) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %83 = ttir.empty() : tensor<1x14x1x128xf32>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %85 = ttir.empty() : tensor<1x1x14x128xf32>
    %86 = "ttir.permute"(%84, %85) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %87 = ttir.empty() : tensor<1x64x1xf32>
    %88 = "ttir.reshape"(%12, %87) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %89 = ttir.empty() : tensor<14xf32>
    %90 = "ttir.typecast"(%10, %89) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %91 = ttir.empty() : tensor<1x1x14xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %93 = ttir.empty() : tensor<1x64x14xf32>
    %94 = "ttir.matmul"(%88, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %95 = ttir.empty() : tensor<1x14x64xf32>
    %96 = "ttir.permute"(%94, %95) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %97 = ttir.empty() : tensor<1x14x128xf32>
    %98 = "ttir.concat"(%96, %96, %97) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %99 = ttir.empty() : tensor<1x14x128xf32>
    %100 = "ttir.cos"(%98, %99) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %101 = ttir.empty() : tensor<1x1x14x128xf32>
    %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %103 = ttir.empty() : tensor<1x1x14x128xf32>
    %104 = "ttir.multiply"(%86, %102, %103) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %105 = ttir.empty() : tensor<1x1x14x64xf32>
    %106 = "ttir.slice"(%86, %105) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %107 = ttir.empty() : tensor<1x1x14x64xf32>
    %108 = "ttir.neg"(%106, %107) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %109 = ttir.empty() : tensor<1x1x14x64xf32>
    %110 = "ttir.slice"(%86, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.concat"(%108, %110, %111) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x14x128xf32>
    %114 = "ttir.sin"(%98, %113) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %115 = ttir.empty() : tensor<1x1x14x128xf32>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %117 = ttir.empty() : tensor<1x1x14x128xf32>
    %118 = "ttir.multiply"(%112, %116, %117) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.add"(%104, %118, %119) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x1x14x128xbf16>
    %122 = "ttir.typecast"(%120, %121) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %123 = "ttir.fill_cache"(%48, %122) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %124 = ttir.empty() : tensor<14x128xf32>
    %125 = "ttir.matmul"(%80, %20, %124) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %126 = ttir.empty() : tensor<14x128xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %128 = ttir.empty() : tensor<1x14x1x128xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %130 = ttir.empty() : tensor<1x1x14x128xbf16>
    %131 = "ttir.permute"(%129, %130) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %132 = "ttir.fill_cache"(%48, %131) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<1x1x4096xf32>
    %134 = "ttir.reshape"(%38, %133) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %135 = ttir.empty() : tensor<14x512xf32>
    %136 = "ttir.matmul"(%80, %32, %135) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %137 = ttir.empty() : tensor<1x14x4x128xf32>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %139 = ttir.empty() : tensor<1x4x14x128xf32>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %141 = ttir.empty() : tensor<4x14x128xf32>
    %142 = "ttir.reshape"(%140, %141) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %143 = ttir.empty() : tensor<4x14x128xf32>
    %144 = "ttir.multiply"(%142, %100, %143) : (tensor<4x14x128xf32>, tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %145 = ttir.empty() : tensor<1x4x14x64xf32>
    %146 = "ttir.slice"(%140, %145) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %147 = ttir.empty() : tensor<1x4x14x64xf32>
    %148 = "ttir.neg"(%146, %147) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %149 = ttir.empty() : tensor<4x14x64xf32>
    %150 = "ttir.reshape"(%148, %149) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %151 = ttir.empty() : tensor<1x4x14x64xf32>
    %152 = "ttir.slice"(%140, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %153 = ttir.empty() : tensor<4x14x64xf32>
    %154 = "ttir.reshape"(%152, %153) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %155 = ttir.empty() : tensor<4x14x128xf32>
    %156 = "ttir.concat"(%150, %154, %155) <{dim = 2 : si32}> : (tensor<4x14x64xf32>, tensor<4x14x64xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %157 = ttir.empty() : tensor<4x14x128xf32>
    %158 = "ttir.multiply"(%156, %114, %157) : (tensor<4x14x128xf32>, tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %159 = ttir.empty() : tensor<4x14x128xf32>
    %160 = "ttir.add"(%144, %158, %159) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %161 = ttir.empty() : tensor<1x1x128x19xbf16>
    %162 = "ttir.permute"(%123, %161) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16>, tensor<1x1x128x19xbf16>) -> tensor<1x1x128x19xbf16>
    %163 = ttir.empty() : tensor<1x128x19xbf16>
    %164 = "ttir.reshape"(%162, %163) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16>, tensor<1x128x19xbf16>) -> tensor<1x128x19xbf16>
    %165 = ttir.empty() : tensor<1x128x19xf32>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x128x19xbf16>, tensor<1x128x19xf32>) -> tensor<1x128x19xf32>
    %167 = ttir.empty() : tensor<4x128x19xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %169 = ttir.empty() : tensor<4x14x19xf32>
    %170 = "ttir.matmul"(%160, %168, %169) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %171 = ttir.empty() : tensor<1x4x14x19xf32>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %173 = ttir.empty() : tensor<1x1x1x1xf32>
    %174 = "ttir.reshape"(%30, %173) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %175 = ttir.empty() : tensor<1x4x14x19xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x4x14x19xf32>, tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %177 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %178 = ttir.empty() : tensor<14x1xsi32>
    %179 = "ttir.reshape"(%177, %178) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %180 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %181 = ttir.empty() : tensor<1x19xsi32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %183 = ttir.empty() : tensor<14x19xbf16>
    %184 = "ttir.ge"(%179, %182, %183) : (tensor<14x1xsi32>, tensor<1x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %185 = ttir.empty() : tensor<1x1xf32>
    %186 = "ttir.reshape"(%28, %185) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %187 = ttir.empty() : tensor<14x19xf32>
    %188 = "ttir.where"(%184, %42, %186, %187) : (tensor<14x19xbf16>, tensor<1x1xf32>, tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %189 = ttir.empty() : tensor<1x1x1x19xsi32>
    %190 = "ttir.reshape"(%0, %189) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x1x1x19xsi32>) -> tensor<1x1x1x19xsi32>
    %191 = ttir.empty() : tensor<1x1x14x1xsi32>
    %192 = "ttir.reshape"(%10, %191) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<1x1x14x1xsi32>) -> tensor<1x1x14x1xsi32>
    %193 = ttir.empty() : tensor<1x1x14x19xbf16>
    %194 = "ttir.gt"(%190, %192, %193) : (tensor<1x1x1x19xsi32>, tensor<1x1x14x1xsi32>, tensor<1x1x14x19xbf16>) -> tensor<1x1x14x19xbf16>
    %195 = ttir.empty() : tensor<1x1x14x19xf32>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %197 = ttir.empty() : tensor<1x1x14x19xf32>
    %198 = "ttir.reshape"(%188, %197) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %199 = ttir.empty() : tensor<1x1x14x19xf32>
    %200 = "ttir.multiply"(%198, %196, %199) : (tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %201 = ttir.empty() : tensor<1x4x14x19xf32>
    %202 = "ttir.add"(%176, %200, %201) : (tensor<1x4x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %203 = ttir.empty() : tensor<1x4x14x1xf32>
    %204 = "ttir.max"(%202, %203) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %205 = ttir.empty() : tensor<1x4x14x19xf32>
    %206 = "ttir.subtract"(%202, %204, %205) : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %207 = ttir.empty() : tensor<1x4x14x19xf32>
    %208 = "ttir.softmax"(%206, %207) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %209 = ttir.empty() : tensor<4x14x19xf32>
    %210 = "ttir.reshape"(%208, %209) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %211 = ttir.empty() : tensor<1x19x128xbf16>
    %212 = "ttir.reshape"(%132, %211) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x19x128xbf16>) -> tensor<1x19x128xbf16>
    %213 = ttir.empty() : tensor<1x19x128xf32>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x19x128xbf16>, tensor<1x19x128xf32>) -> tensor<1x19x128xf32>
    %215 = ttir.empty() : tensor<4x19x128xf32>
    %216 = "ttir.broadcast"(%214, %215) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %217 = ttir.empty() : tensor<4x14x128xf32>
    %218 = "ttir.matmul"(%210, %216, %217) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %219 = ttir.empty() : tensor<1x4x14x128xf32>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %221 = ttir.empty() : tensor<1x14x4x128xf32>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %223 = ttir.empty() : tensor<14x512xf32>
    %224 = "ttir.reshape"(%222, %223) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %225 = ttir.empty() : tensor<14x4096xf32>
    %226 = "ttir.matmul"(%224, %26, %225) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %227 = ttir.empty() : tensor<14x4096xf32>
    %228 = "ttir.all_reduce"(%226, %227) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %229 = ttir.empty() : tensor<14x4096xf32>
    %230 = "ttir.add"(%54, %228, %229) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %231 = ttir.empty() : tensor<1x14x4096xf32>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %233 = ttir.empty() : tensor<1x4096xf32>
    %234 = "ttir.reshape"(%34, %233) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %235 = ttir.empty() : tensor<1x14x4096xf32>
    %236 = "ttir.pow"(%232, %44, %235) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %237 = ttir.empty() : tensor<1x14xf32>
    %238 = "ttir.sum"(%236, %237) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %239 = ttir.empty() : tensor<1x14xf32>
    %240 = "ttir.multiply"(%238, %2, %239) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %241 = ttir.empty() : tensor<1x14xf32>
    %242 = "ttir.reshape"(%68, %241) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %243 = ttir.empty() : tensor<1x14xf32>
    %244 = "ttir.add"(%240, %242, %243) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %245 = ttir.empty() : tensor<1x14xf32>
    %246 = "ttir.rsqrt"(%244, %245) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %247 = ttir.empty() : tensor<14x1xf32>
    %248 = "ttir.reshape"(%246, %247) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %249 = ttir.empty() : tensor<14x4096xf32>
    %250 = "ttir.multiply"(%230, %248, %249) : (tensor<14x4096xf32>, tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %251 = ttir.empty() : tensor<14x4096xf32>
    %252 = "ttir.multiply"(%234, %250, %251) : (tensor<1x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %253 = ttir.empty() : tensor<14x1792xf32>
    %254 = "ttir.matmul"(%252, %36, %253) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %255 = ttir.empty() : tensor<14x1792xf32>
    %256 = "ttir.sigmoid"(%254, %255) : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %257 = ttir.empty() : tensor<14x1792xf32>
    %258 = "ttir.multiply"(%254, %256, %257) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %259 = ttir.empty() : tensor<14x1792xf32>
    %260 = "ttir.matmul"(%252, %24, %259) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %261 = ttir.empty() : tensor<14x1792xf32>
    %262 = "ttir.multiply"(%258, %260, %261) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.matmul"(%262, %22, %263) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<14x4096xf32>
    %266 = "ttir.all_reduce"(%264, %265) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %267 = ttir.empty() : tensor<14x4096xf32>
    %268 = "ttir.add"(%230, %266, %267) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %269 = ttir.empty() : tensor<1x14x4096xf32>
    %270 = "ttir.reshape"(%268, %269) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.pow"(%270, %44, %271) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14xf32>
    %274 = "ttir.sum"(%272, %273) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.multiply"(%274, %2, %275) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14x1xf32>
    %278 = "ttir.reshape"(%276, %277) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.add"(%278, %66, %279) : (tensor<1x14x1xf32>, tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.rsqrt"(%280, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x4096xf32>
    %284 = "ttir.multiply"(%270, %282, %283) : (tensor<1x14x4096xf32>, tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.multiply"(%134, %284, %285) : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<14x4096xf32>
    %288 = "ttir.reshape"(%286, %287) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %289 = ttir.empty() : tensor<14x16032xf32>
    %290 = "ttir.matmul"(%288, %40, %289) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %291 = ttir.empty() : tensor<1x14x16032xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %293 = ttir.empty() : tensor<1x14x4096xf32>
    %294 = "ttir.mesh_shard"(%56, %293) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %295 = ttir.empty() : tensor<1x8x19x128xbf16>
    %296 = "ttir.mesh_shard"(%123, %295) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %297 = ttir.empty() : tensor<1x8x19x128xbf16>
    %298 = "ttir.mesh_shard"(%132, %297) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %299 = ttir.empty() : tensor<1x14x4096xf32>
    %300 = "ttir.mesh_shard"(%286, %299) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %301 = ttir.empty() : tensor<14x128256xf32>
    %302 = "ttir.mesh_shard"(%290, %301) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %303 = ttir.empty() : tensor<1x14x128256xf32>
    %304 = "ttir.mesh_shard"(%292, %303) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %294, %296, %298, %300, %302, %304 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTIRQuantDataTypeConversionPass (ttir-quant-data-type-conversion) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16>
    %5 = ttir.empty() : tensor<1x14xsi32>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32>, tensor<1x14xsi32>) -> tensor<1x14xsi32>
    %7 = ttir.empty() : tensor<128256x4096xf32>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<128256x4096xf32>) -> tensor<128256x4096xf32>
    %9 = ttir.empty() : tensor<14xsi32>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32>, tensor<14xsi32>) -> tensor<14xsi32>
    %11 = ttir.empty() : tensor<64xf32>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32>, tensor<64xf32>) -> tensor<64xf32>
    %13 = ttir.empty() : tensor<128x4096xf32>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %15 = ttir.empty() : tensor<f32>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %17 = ttir.empty() : tensor<4096xf32>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %19 = ttir.empty() : tensor<128x4096xf32>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32>, tensor<128x4096xf32>) -> tensor<128x4096xf32>
    %21 = ttir.empty() : tensor<4096x1792xf32>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32>, tensor<4096x1792xf32>) -> tensor<4096x1792xf32>
    %23 = ttir.empty() : tensor<1792x4096xf32>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %25 = ttir.empty() : tensor<4096x512xf32>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<4096x512xf32>) -> tensor<4096x512xf32>
    %27 = ttir.empty() : tensor<f32>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %29 = ttir.empty() : tensor<f32>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %31 = ttir.empty() : tensor<512x4096xf32>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32>, tensor<512x4096xf32>) -> tensor<512x4096xf32>
    %33 = ttir.empty() : tensor<4096xf32>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %35 = ttir.empty() : tensor<1792x4096xf32>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32>, tensor<1792x4096xf32>) -> tensor<1792x4096xf32>
    %37 = ttir.empty() : tensor<4096xf32>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32>, tensor<4096xf32>) -> tensor<4096xf32>
    %39 = ttir.empty() : tensor<16032x4096xf32>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32>, tensor<16032x4096xf32>) -> tensor<16032x4096xf32>
    %41 = ttir.empty() : tensor<1x1xf32>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %43 = ttir.empty() : tensor<1x1x1xf32>
    %44 = "ttir.reshape"(%3, %43) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %45 = ttir.empty() : tensor<1x1x1x1xbf16>
    %46 = "ttir.reshape"(%4, %45) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16>, tensor<1x1x1x1xbf16>) -> tensor<1x1x1x1xbf16>
    %47 = ttir.empty() : tensor<1x1x19x128xbf16>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
    %49 = ttir.empty() : tensor<1x14xui32>
    %50 = "ttir.typecast"(%6, %49) <{conservative_folding = false}> : (tensor<1x14xsi32>, tensor<1x14xui32>) -> tensor<1x14xui32>
    %51 = ttir.empty() : tensor<14xui32>
    %52 = "ttir.reshape"(%50, %51) <{shape = [14 : i32]}> : (tensor<1x14xui32>, tensor<14xui32>) -> tensor<14xui32>
    %53 = ttir.empty() : tensor<14x4096xf32>
    %54 = "ttir.embedding"(%52, %8, %53) : (tensor<14xui32>, tensor<128256x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %55 = ttir.empty() : tensor<1x14x4096xf32>
    %56 = "ttir.reshape"(%54, %55) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %57 = ttir.empty() : tensor<1x4096xf32>
    %58 = "ttir.reshape"(%18, %57) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %59 = ttir.empty() : tensor<1x14x4096xf32>
    %60 = "ttir.pow"(%56, %44, %59) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %61 = ttir.empty() : tensor<1x14xf32>
    %62 = "ttir.sum"(%60, %61) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %63 = ttir.empty() : tensor<1x14xf32>
    %64 = "ttir.multiply"(%62, %2, %63) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %65 = ttir.empty() : tensor<1x1x1xf32>
    %66 = "ttir.reshape"(%16, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %67 = ttir.empty() : tensor<1x14x1xf32>
    %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %69 = ttir.empty() : tensor<1x14xf32>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %71 = ttir.empty() : tensor<1x14xf32>
    %72 = "ttir.add"(%64, %70, %71) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %73 = ttir.empty() : tensor<1x14xf32>
    %74 = "ttir.rsqrt"(%72, %73) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %75 = ttir.empty() : tensor<14x1xf32>
    %76 = "ttir.reshape"(%74, %75) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %77 = ttir.empty() : tensor<14x4096xf32>
    %78 = "ttir.multiply"(%54, %76, %77) : (tensor<14x4096xf32>, tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %79 = ttir.empty() : tensor<14x4096xf32>
    %80 = "ttir.multiply"(%58, %78, %79) : (tensor<1x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %81 = ttir.empty() : tensor<14x128xf32>
    %82 = "ttir.matmul"(%80, %14, %81) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %83 = ttir.empty() : tensor<1x14x1x128xf32>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32>, tensor<1x14x1x128xf32>) -> tensor<1x14x1x128xf32>
    %85 = ttir.empty() : tensor<1x1x14x128xf32>
    %86 = "ttir.permute"(%84, %85) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %87 = ttir.empty() : tensor<1x64x1xf32>
    %88 = "ttir.reshape"(%12, %87) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %89 = ttir.empty() : tensor<14xf32>
    %90 = "ttir.typecast"(%10, %89) <{conservative_folding = false}> : (tensor<14xsi32>, tensor<14xf32>) -> tensor<14xf32>
    %91 = ttir.empty() : tensor<1x1x14xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32>, tensor<1x1x14xf32>) -> tensor<1x1x14xf32>
    %93 = ttir.empty() : tensor<1x64x14xf32>
    %94 = "ttir.matmul"(%88, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32>, tensor<1x1x14xf32>, tensor<1x64x14xf32>) -> tensor<1x64x14xf32>
    %95 = ttir.empty() : tensor<1x14x64xf32>
    %96 = "ttir.permute"(%94, %95) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32>, tensor<1x14x64xf32>) -> tensor<1x14x64xf32>
    %97 = ttir.empty() : tensor<1x14x128xf32>
    %98 = "ttir.concat"(%96, %96, %97) <{dim = 2 : si32}> : (tensor<1x14x64xf32>, tensor<1x14x64xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %99 = ttir.empty() : tensor<1x14x128xf32>
    %100 = "ttir.cos"(%98, %99) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %101 = ttir.empty() : tensor<1x1x14x128xf32>
    %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %103 = ttir.empty() : tensor<1x1x14x128xf32>
    %104 = "ttir.multiply"(%86, %102, %103) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %105 = ttir.empty() : tensor<1x1x14x64xf32>
    %106 = "ttir.slice"(%86, %105) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %107 = ttir.empty() : tensor<1x1x14x64xf32>
    %108 = "ttir.neg"(%106, %107) : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %109 = ttir.empty() : tensor<1x1x14x64xf32>
    %110 = "ttir.slice"(%86, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x64xf32>) -> tensor<1x1x14x64xf32>
    %111 = ttir.empty() : tensor<1x1x14x128xf32>
    %112 = "ttir.concat"(%108, %110, %111) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32>, tensor<1x1x14x64xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %113 = ttir.empty() : tensor<1x14x128xf32>
    %114 = "ttir.sin"(%98, %113) : (tensor<1x14x128xf32>, tensor<1x14x128xf32>) -> tensor<1x14x128xf32>
    %115 = ttir.empty() : tensor<1x1x14x128xf32>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %117 = ttir.empty() : tensor<1x1x14x128xf32>
    %118 = "ttir.multiply"(%112, %116, %117) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %119 = ttir.empty() : tensor<1x1x14x128xf32>
    %120 = "ttir.add"(%104, %118, %119) : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>, tensor<1x1x14x128xf32>) -> tensor<1x1x14x128xf32>
    %121 = ttir.empty() : tensor<1x1x14x128xbf16>
    %122 = "ttir.typecast"(%120, %121) <{conservative_folding = false}> : (tensor<1x1x14x128xf32>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %123 = "ttir.fill_cache"(%48, %122) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %124 = ttir.empty() : tensor<14x128xf32>
    %125 = "ttir.matmul"(%80, %20, %124) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<128x4096xf32>, tensor<14x128xf32>) -> tensor<14x128xf32>
    %126 = ttir.empty() : tensor<14x128xbf16>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<14x128xf32>, tensor<14x128xbf16>) -> tensor<14x128xbf16>
    %128 = ttir.empty() : tensor<1x14x1x128xbf16>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16>, tensor<1x14x1x128xbf16>) -> tensor<1x14x1x128xbf16>
    %130 = ttir.empty() : tensor<1x1x14x128xbf16>
    %131 = "ttir.permute"(%129, %130) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x14x128xbf16>
    %132 = "ttir.fill_cache"(%48, %131) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16>, tensor<1x1x14x128xbf16>) -> tensor<1x1x19x128xbf16>
    %133 = ttir.empty() : tensor<1x1x4096xf32>
    %134 = "ttir.reshape"(%38, %133) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x1x4096xf32>) -> tensor<1x1x4096xf32>
    %135 = ttir.empty() : tensor<14x512xf32>
    %136 = "ttir.matmul"(%80, %32, %135) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<512x4096xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %137 = ttir.empty() : tensor<1x14x4x128xf32>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %139 = ttir.empty() : tensor<1x4x14x128xf32>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %141 = ttir.empty() : tensor<4x14x128xf32>
    %142 = "ttir.reshape"(%140, %141) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %143 = ttir.empty() : tensor<4x14x128xf32>
    %144 = "ttir.multiply"(%142, %100, %143) : (tensor<4x14x128xf32>, tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %145 = ttir.empty() : tensor<1x4x14x64xf32>
    %146 = "ttir.slice"(%140, %145) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %147 = ttir.empty() : tensor<1x4x14x64xf32>
    %148 = "ttir.neg"(%146, %147) : (tensor<1x4x14x64xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %149 = ttir.empty() : tensor<4x14x64xf32>
    %150 = "ttir.reshape"(%148, %149) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %151 = ttir.empty() : tensor<1x4x14x64xf32>
    %152 = "ttir.slice"(%140, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32>, tensor<1x4x14x64xf32>) -> tensor<1x4x14x64xf32>
    %153 = ttir.empty() : tensor<4x14x64xf32>
    %154 = "ttir.reshape"(%152, %153) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32>, tensor<4x14x64xf32>) -> tensor<4x14x64xf32>
    %155 = ttir.empty() : tensor<4x14x128xf32>
    %156 = "ttir.concat"(%150, %154, %155) <{dim = 2 : si32}> : (tensor<4x14x64xf32>, tensor<4x14x64xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %157 = ttir.empty() : tensor<4x14x128xf32>
    %158 = "ttir.multiply"(%156, %114, %157) : (tensor<4x14x128xf32>, tensor<1x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %159 = ttir.empty() : tensor<4x14x128xf32>
    %160 = "ttir.add"(%144, %158, %159) : (tensor<4x14x128xf32>, tensor<4x14x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %161 = ttir.empty() : tensor<1x1x128x19xbf16>
    %162 = "ttir.permute"(%123, %161) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16>, tensor<1x1x128x19xbf16>) -> tensor<1x1x128x19xbf16>
    %163 = ttir.empty() : tensor<1x128x19xbf16>
    %164 = "ttir.reshape"(%162, %163) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16>, tensor<1x128x19xbf16>) -> tensor<1x128x19xbf16>
    %165 = ttir.empty() : tensor<1x128x19xf32>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x128x19xbf16>, tensor<1x128x19xf32>) -> tensor<1x128x19xf32>
    %167 = ttir.empty() : tensor<4x128x19xf32>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xf32>, tensor<4x128x19xf32>) -> tensor<4x128x19xf32>
    %169 = ttir.empty() : tensor<4x14x19xf32>
    %170 = "ttir.matmul"(%160, %168, %169) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32>, tensor<4x128x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %171 = ttir.empty() : tensor<1x4x14x19xf32>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %173 = ttir.empty() : tensor<1x1x1x1xf32>
    %174 = "ttir.reshape"(%30, %173) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %175 = ttir.empty() : tensor<1x4x14x19xf32>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x4x14x19xf32>, tensor<1x1x1x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %177 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32>
    %178 = ttir.empty() : tensor<14x1xsi32>
    %179 = "ttir.reshape"(%177, %178) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<14x1xsi32>) -> tensor<14x1xsi32>
    %180 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32>
    %181 = ttir.empty() : tensor<1x19xsi32>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x19xsi32>) -> tensor<1x19xsi32>
    %183 = ttir.empty() : tensor<14x19xbf16>
    %184 = "ttir.ge"(%179, %182, %183) : (tensor<14x1xsi32>, tensor<1x19xsi32>, tensor<14x19xbf16>) -> tensor<14x19xbf16>
    %185 = ttir.empty() : tensor<1x1xf32>
    %186 = "ttir.reshape"(%28, %185) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %187 = ttir.empty() : tensor<14x19xf32>
    %188 = "ttir.where"(%184, %42, %186, %187) : (tensor<14x19xbf16>, tensor<1x1xf32>, tensor<1x1xf32>, tensor<14x19xf32>) -> tensor<14x19xf32>
    %189 = ttir.empty() : tensor<1x1x1x19xsi32>
    %190 = "ttir.reshape"(%0, %189) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32>, tensor<1x1x1x19xsi32>) -> tensor<1x1x1x19xsi32>
    %191 = ttir.empty() : tensor<1x1x14x1xsi32>
    %192 = "ttir.reshape"(%10, %191) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32>, tensor<1x1x14x1xsi32>) -> tensor<1x1x14x1xsi32>
    %193 = ttir.empty() : tensor<1x1x14x19xbf16>
    %194 = "ttir.gt"(%190, %192, %193) : (tensor<1x1x1x19xsi32>, tensor<1x1x14x1xsi32>, tensor<1x1x14x19xbf16>) -> tensor<1x1x14x19xbf16>
    %195 = ttir.empty() : tensor<1x1x14x19xf32>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %197 = ttir.empty() : tensor<1x1x14x19xf32>
    %198 = "ttir.reshape"(%188, %197) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %199 = ttir.empty() : tensor<1x1x14x19xf32>
    %200 = "ttir.multiply"(%198, %196, %199) : (tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x1x14x19xf32>) -> tensor<1x1x14x19xf32>
    %201 = ttir.empty() : tensor<1x4x14x19xf32>
    %202 = "ttir.add"(%176, %200, %201) : (tensor<1x4x14x19xf32>, tensor<1x1x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %203 = ttir.empty() : tensor<1x4x14x1xf32>
    %204 = "ttir.max"(%202, %203) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>) -> tensor<1x4x14x1xf32>
    %205 = ttir.empty() : tensor<1x4x14x19xf32>
    %206 = "ttir.subtract"(%202, %204, %205) : (tensor<1x4x14x19xf32>, tensor<1x4x14x1xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %207 = ttir.empty() : tensor<1x4x14x19xf32>
    %208 = "ttir.softmax"(%206, %207) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32>, tensor<1x4x14x19xf32>) -> tensor<1x4x14x19xf32>
    %209 = ttir.empty() : tensor<4x14x19xf32>
    %210 = "ttir.reshape"(%208, %209) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32>, tensor<4x14x19xf32>) -> tensor<4x14x19xf32>
    %211 = ttir.empty() : tensor<1x19x128xbf16>
    %212 = "ttir.reshape"(%132, %211) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16>, tensor<1x19x128xbf16>) -> tensor<1x19x128xbf16>
    %213 = ttir.empty() : tensor<1x19x128xf32>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x19x128xbf16>, tensor<1x19x128xf32>) -> tensor<1x19x128xf32>
    %215 = ttir.empty() : tensor<4x19x128xf32>
    %216 = "ttir.broadcast"(%214, %215) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xf32>, tensor<4x19x128xf32>) -> tensor<4x19x128xf32>
    %217 = ttir.empty() : tensor<4x14x128xf32>
    %218 = "ttir.matmul"(%210, %216, %217) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32>, tensor<4x19x128xf32>, tensor<4x14x128xf32>) -> tensor<4x14x128xf32>
    %219 = ttir.empty() : tensor<1x4x14x128xf32>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32>, tensor<1x4x14x128xf32>) -> tensor<1x4x14x128xf32>
    %221 = ttir.empty() : tensor<1x14x4x128xf32>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32>, tensor<1x14x4x128xf32>) -> tensor<1x14x4x128xf32>
    %223 = ttir.empty() : tensor<14x512xf32>
    %224 = "ttir.reshape"(%222, %223) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32>, tensor<14x512xf32>) -> tensor<14x512xf32>
    %225 = ttir.empty() : tensor<14x4096xf32>
    %226 = "ttir.matmul"(%224, %26, %225) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32>, tensor<4096x512xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %227 = ttir.empty() : tensor<14x4096xf32>
    %228 = "ttir.all_reduce"(%226, %227) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %229 = ttir.empty() : tensor<14x4096xf32>
    %230 = "ttir.add"(%54, %228, %229) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %231 = ttir.empty() : tensor<1x14x4096xf32>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %233 = ttir.empty() : tensor<1x4096xf32>
    %234 = "ttir.reshape"(%34, %233) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32>, tensor<1x4096xf32>) -> tensor<1x4096xf32>
    %235 = ttir.empty() : tensor<1x14x4096xf32>
    %236 = "ttir.pow"(%232, %44, %235) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %237 = ttir.empty() : tensor<1x14xf32>
    %238 = "ttir.sum"(%236, %237) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %239 = ttir.empty() : tensor<1x14xf32>
    %240 = "ttir.multiply"(%238, %2, %239) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %241 = ttir.empty() : tensor<1x14xf32>
    %242 = "ttir.reshape"(%68, %241) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %243 = ttir.empty() : tensor<1x14xf32>
    %244 = "ttir.add"(%240, %242, %243) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %245 = ttir.empty() : tensor<1x14xf32>
    %246 = "ttir.rsqrt"(%244, %245) : (tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %247 = ttir.empty() : tensor<14x1xf32>
    %248 = "ttir.reshape"(%246, %247) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<14x1xf32>) -> tensor<14x1xf32>
    %249 = ttir.empty() : tensor<14x4096xf32>
    %250 = "ttir.multiply"(%230, %248, %249) : (tensor<14x4096xf32>, tensor<14x1xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %251 = ttir.empty() : tensor<14x4096xf32>
    %252 = "ttir.multiply"(%234, %250, %251) : (tensor<1x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %253 = ttir.empty() : tensor<14x1792xf32>
    %254 = "ttir.matmul"(%252, %36, %253) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %255 = ttir.empty() : tensor<14x1792xf32>
    %256 = "ttir.sigmoid"(%254, %255) : (tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %257 = ttir.empty() : tensor<14x1792xf32>
    %258 = "ttir.multiply"(%254, %256, %257) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %259 = ttir.empty() : tensor<14x1792xf32>
    %260 = "ttir.matmul"(%252, %24, %259) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<1792x4096xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %261 = ttir.empty() : tensor<14x1792xf32>
    %262 = "ttir.multiply"(%258, %260, %261) : (tensor<14x1792xf32>, tensor<14x1792xf32>, tensor<14x1792xf32>) -> tensor<14x1792xf32>
    %263 = ttir.empty() : tensor<14x4096xf32>
    %264 = "ttir.matmul"(%262, %22, %263) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32>, tensor<4096x1792xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %265 = ttir.empty() : tensor<14x4096xf32>
    %266 = "ttir.all_reduce"(%264, %265) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %267 = ttir.empty() : tensor<14x4096xf32>
    %268 = "ttir.add"(%230, %266, %267) : (tensor<14x4096xf32>, tensor<14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %269 = ttir.empty() : tensor<1x14x4096xf32>
    %270 = "ttir.reshape"(%268, %269) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %271 = ttir.empty() : tensor<1x14x4096xf32>
    %272 = "ttir.pow"(%270, %44, %271) : (tensor<1x14x4096xf32>, tensor<1x1x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %273 = ttir.empty() : tensor<1x14xf32>
    %274 = "ttir.sum"(%272, %273) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %275 = ttir.empty() : tensor<1x14xf32>
    %276 = "ttir.multiply"(%274, %2, %275) : (tensor<1x14xf32>, tensor<1x14xf32>, tensor<1x14xf32>) -> tensor<1x14xf32>
    %277 = ttir.empty() : tensor<1x14x1xf32>
    %278 = "ttir.reshape"(%276, %277) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %279 = ttir.empty() : tensor<1x14x1xf32>
    %280 = "ttir.add"(%278, %66, %279) : (tensor<1x14x1xf32>, tensor<1x1x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %281 = ttir.empty() : tensor<1x14x1xf32>
    %282 = "ttir.rsqrt"(%280, %281) : (tensor<1x14x1xf32>, tensor<1x14x1xf32>) -> tensor<1x14x1xf32>
    %283 = ttir.empty() : tensor<1x14x4096xf32>
    %284 = "ttir.multiply"(%270, %282, %283) : (tensor<1x14x4096xf32>, tensor<1x14x1xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %285 = ttir.empty() : tensor<1x14x4096xf32>
    %286 = "ttir.multiply"(%134, %284, %285) : (tensor<1x1x4096xf32>, tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %287 = ttir.empty() : tensor<14x4096xf32>
    %288 = "ttir.reshape"(%286, %287) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32>, tensor<14x4096xf32>) -> tensor<14x4096xf32>
    %289 = ttir.empty() : tensor<14x16032xf32>
    %290 = "ttir.matmul"(%288, %40, %289) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32>, tensor<16032x4096xf32>, tensor<14x16032xf32>) -> tensor<14x16032xf32>
    %291 = ttir.empty() : tensor<1x14x16032xf32>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32>, tensor<1x14x16032xf32>) -> tensor<1x14x16032xf32>
    %293 = ttir.empty() : tensor<1x14x4096xf32>
    %294 = "ttir.mesh_shard"(%56, %293) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %295 = ttir.empty() : tensor<1x8x19x128xbf16>
    %296 = "ttir.mesh_shard"(%123, %295) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %297 = ttir.empty() : tensor<1x8x19x128xbf16>
    %298 = "ttir.mesh_shard"(%132, %297) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16>, tensor<1x8x19x128xbf16>) -> tensor<1x8x19x128xbf16>
    %299 = ttir.empty() : tensor<1x14x4096xf32>
    %300 = "ttir.mesh_shard"(%286, %299) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32>, tensor<1x14x4096xf32>) -> tensor<1x14x4096xf32>
    %301 = ttir.empty() : tensor<14x128256xf32>
    %302 = "ttir.mesh_shard"(%290, %301) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32>, tensor<14x128256xf32>) -> tensor<14x128256xf32>
    %303 = ttir.empty() : tensor<1x14x128256xf32>
    %304 = "ttir.mesh_shard"(%292, %303) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32>, tensor<1x14x128256xf32>) -> tensor<1x14x128256xf32>
    return %294, %296, %298, %300, %302, %304 : tensor<1x14x4096xf32>, tensor<1x8x19x128xbf16>, tensor<1x8x19x128xbf16>, tensor<1x14x4096xf32>, tensor<14x128256xf32>, tensor<1x14x128256xf32>
  }
}

// -----// IR Dump After TTNNLayout (ttnn-layout) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x448x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %5 = ttir.empty() : tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %7 = ttir.empty() : tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %9 = ttir.empty() : tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %11 = ttir.empty() : tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %13 = ttir.empty() : tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %15 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %17 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %19 = ttir.empty() : tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %21 = ttir.empty() : tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x448x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %23 = ttir.empty() : tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %25 = ttir.empty() : tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %27 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %29 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %31 = ttir.empty() : tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %33 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %35 = ttir.empty() : tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %37 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %39 = ttir.empty() : tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %41 = ttir.empty() : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %43 = ttir.empty() : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %44 = "ttir.reshape"(%3, %43) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %45 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %46 = "ttir.reshape"(%4, %45) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %47 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %49 = ttir.empty() : tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %50 = "ttir.typecast"(%6, %49) <{conservative_folding = false}> : (tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %51 = ttir.empty() : tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %52 = "ttir.reshape"(%50, %51) <{shape = [14 : i32]}> : (tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %53 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %54 = "ttir.embedding"(%52, %8, %53) : (tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %55 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %56 = "ttir.reshape"(%54, %55) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %57 = ttir.empty() : tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %58 = "ttir.reshape"(%18, %57) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %59 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %60 = "ttir.pow"(%56, %44, %59) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %61 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %62 = "ttir.sum"(%60, %61) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %63 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %64 = "ttir.multiply"(%62, %2, %63) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %65 = ttir.empty() : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %66 = "ttir.reshape"(%16, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %67 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %69 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %71 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %72 = "ttir.add"(%64, %70, %71) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %73 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %74 = "ttir.rsqrt"(%72, %73) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %75 = ttir.empty() : tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %76 = "ttir.reshape"(%74, %75) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %77 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %78 = "ttir.multiply"(%54, %76, %77) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %79 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %80 = "ttir.multiply"(%58, %78, %79) : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %81 = ttir.empty() : tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %82 = "ttir.matmul"(%80, %14, %81) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %83 = ttir.empty() : tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %85 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %86 = "ttir.permute"(%84, %85) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %87 = ttir.empty() : tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %88 = "ttir.reshape"(%12, %87) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %89 = ttir.empty() : tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %90 = "ttir.typecast"(%10, %89) <{conservative_folding = false}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %91 = ttir.empty() : tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %93 = ttir.empty() : tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %94 = "ttir.matmul"(%88, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %95 = ttir.empty() : tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %96 = "ttir.permute"(%94, %95) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %97 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %98 = "ttir.concat"(%96, %96, %97) <{dim = 2 : si32}> : (tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %99 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %100 = "ttir.cos"(%98, %99) : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %101 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %103 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %104 = "ttir.multiply"(%86, %102, %103) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %105 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %106 = "ttir.slice"(%86, %105) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %107 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %108 = "ttir.neg"(%106, %107) : (tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %109 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %110 = "ttir.slice"(%86, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %111 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %112 = "ttir.concat"(%108, %110, %111) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %113 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %114 = "ttir.sin"(%98, %113) : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %115 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %117 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %118 = "ttir.multiply"(%112, %116, %117) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %119 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %120 = "ttir.add"(%104, %118, %119) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %121 = ttir.empty() : tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %122 = "ttir.typecast"(%120, %121) <{conservative_folding = false}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %123 = "ttir.fill_cache"(%48, %122) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %124 = ttir.empty() : tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %125 = "ttir.matmul"(%80, %20, %124) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %126 = ttir.empty() : tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %128 = ttir.empty() : tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %130 = ttir.empty() : tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %131 = "ttir.permute"(%129, %130) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %132 = "ttir.fill_cache"(%48, %131) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %133 = ttir.empty() : tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %134 = "ttir.reshape"(%38, %133) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %135 = ttir.empty() : tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %136 = "ttir.matmul"(%80, %32, %135) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %137 = ttir.empty() : tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %139 = ttir.empty() : tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %141 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %142 = "ttir.reshape"(%140, %141) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %143 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %144 = "ttir.multiply"(%142, %100, %143) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %145 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %146 = "ttir.slice"(%140, %145) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %147 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %148 = "ttir.neg"(%146, %147) : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %149 = ttir.empty() : tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %150 = "ttir.reshape"(%148, %149) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %151 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %152 = "ttir.slice"(%140, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %153 = ttir.empty() : tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %154 = "ttir.reshape"(%152, %153) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %155 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %156 = "ttir.concat"(%150, %154, %155) <{dim = 2 : si32}> : (tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %157 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %158 = "ttir.multiply"(%156, %114, %157) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %159 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %160 = "ttir.add"(%144, %158, %159) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %161 = ttir.empty() : tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %162 = "ttir.permute"(%123, %161) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %163 = ttir.empty() : tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %164 = "ttir.reshape"(%162, %163) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %165 = ttir.empty() : tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %167 = ttir.empty() : tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %169 = ttir.empty() : tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %170 = "ttir.matmul"(%160, %168, %169) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %171 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %173 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %174 = "ttir.reshape"(%30, %173) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %175 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %177 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %178 = ttir.empty() : tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %179 = "ttir.reshape"(%177, %178) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %180 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %181 = ttir.empty() : tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %183 = ttir.empty() : tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %184 = "ttir.ge"(%179, %182, %183) : (tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %185 = ttir.empty() : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %186 = "ttir.reshape"(%28, %185) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %187 = ttir.empty() : tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %188 = "ttir.where"(%184, %42, %186, %187) : (tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %189 = ttir.empty() : tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %190 = "ttir.reshape"(%0, %189) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %191 = ttir.empty() : tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %192 = "ttir.reshape"(%10, %191) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %193 = ttir.empty() : tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %194 = "ttir.gt"(%190, %192, %193) : (tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %195 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %197 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %198 = "ttir.reshape"(%188, %197) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %199 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %200 = "ttir.multiply"(%198, %196, %199) : (tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %201 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %202 = "ttir.add"(%176, %200, %201) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %203 = ttir.empty() : tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %204 = "ttir.max"(%202, %203) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %205 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %206 = "ttir.subtract"(%202, %204, %205) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %207 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %208 = "ttir.softmax"(%206, %207) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %209 = ttir.empty() : tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %210 = "ttir.reshape"(%208, %209) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %211 = ttir.empty() : tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %212 = "ttir.reshape"(%132, %211) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %213 = ttir.empty() : tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %215 = ttir.empty() : tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %216 = "ttir.broadcast"(%214, %215) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %217 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %218 = "ttir.matmul"(%210, %216, %217) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %219 = ttir.empty() : tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %221 = ttir.empty() : tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %223 = ttir.empty() : tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %224 = "ttir.reshape"(%222, %223) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %225 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %226 = "ttir.matmul"(%224, %26, %225) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %227 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %228 = "ttir.all_reduce"(%226, %227) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %229 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %230 = "ttir.add"(%54, %228, %229) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %231 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %233 = ttir.empty() : tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %234 = "ttir.reshape"(%34, %233) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %235 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %236 = "ttir.pow"(%232, %44, %235) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %237 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %238 = "ttir.sum"(%236, %237) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %239 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %240 = "ttir.multiply"(%238, %2, %239) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %241 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %242 = "ttir.reshape"(%68, %241) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %243 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %244 = "ttir.add"(%240, %242, %243) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %245 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %246 = "ttir.rsqrt"(%244, %245) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %247 = ttir.empty() : tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %248 = "ttir.reshape"(%246, %247) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %249 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %250 = "ttir.multiply"(%230, %248, %249) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %251 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %252 = "ttir.multiply"(%234, %250, %251) : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %253 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %254 = "ttir.matmul"(%252, %36, %253) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %255 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %256 = "ttir.sigmoid"(%254, %255) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %257 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %258 = "ttir.multiply"(%254, %256, %257) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %259 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %260 = "ttir.matmul"(%252, %24, %259) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %261 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %262 = "ttir.multiply"(%258, %260, %261) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %263 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %264 = "ttir.matmul"(%262, %22, %263) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %265 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %266 = "ttir.all_reduce"(%264, %265) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %267 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %268 = "ttir.add"(%230, %266, %267) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %269 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %270 = "ttir.reshape"(%268, %269) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %271 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %272 = "ttir.pow"(%270, %44, %271) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %273 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %274 = "ttir.sum"(%272, %273) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %275 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %276 = "ttir.multiply"(%274, %2, %275) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %277 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %278 = "ttir.reshape"(%276, %277) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %279 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %280 = "ttir.add"(%278, %66, %279) : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %281 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %282 = "ttir.rsqrt"(%280, %281) : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %283 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %284 = "ttir.multiply"(%270, %282, %283) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %285 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %286 = "ttir.multiply"(%134, %284, %285) : (tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %287 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %288 = "ttir.reshape"(%286, %287) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %289 = ttir.empty() : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %290 = "ttir.matmul"(%288, %40, %289) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %291 = ttir.empty() : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %293 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %294 = ttir.to_layout %56, %293 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %295 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %296 = "ttir.mesh_shard"(%294, %295) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %297 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %298 = ttir.to_layout %123, %297 : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>> -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %299 = ttir.empty() : tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %300 = "ttir.mesh_shard"(%298, %299) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %301 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %302 = ttir.to_layout %132, %301 : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>> -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %303 = ttir.empty() : tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %304 = "ttir.mesh_shard"(%302, %303) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %305 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %306 = ttir.to_layout %286, %305 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %307 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %308 = "ttir.mesh_shard"(%306, %307) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %309 = ttir.empty() : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %310 = ttir.to_layout %290, %309 : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %311 = ttir.empty() : tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %312 = "ttir.mesh_shard"(%310, %311) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %313 = ttir.empty() : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %314 = ttir.to_layout %292, %313 : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %315 = ttir.empty() : tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %316 = "ttir.mesh_shard"(%314, %315) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    return %296, %300, %304, %308, %312, %316 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
  }
}

llama_ttir.mlir:144:12: error: failed to legalize operation 'ttir.fill_cache'
    %141 = "ttir.scatter"(%53, %5, %139, %140) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x1x19x128xbf16>, tensor<14xi64>, tensor<1x1x14x128xbf16>, tensor<1x1x19x128xbf16>) -> tensor<1x1x19x128xbf16>
           ^
llama_ttir.mlir:144:12: note: see current operation: %247 = "ttir.fill_cache"(%98, %246) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
// -----// IR Dump After ConvertTTIRToTTNN Failed (convert-ttir-to-ttnn) //----- //
module @SyncTensorsGraph.337 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073159968, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 2560032, dram_unreserved_end = 1073176608, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [1 : i32, 1 : i32, 1 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
  ttcore.device @default_device = <workerGrid = #ttcore.grid<8x64, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 8)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 8, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x8, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
  func.func @main(%arg0: tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<4096x14336xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x448x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]> : tensor<19xsi32>}> : () -> tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %1 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %2 = "ttir.full"() <{fill_value = 2.44140625E-4 : f32, shape = array<i32: 1, 14>}> : () -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %3 = "ttir.full"() <{fill_value = 2.000000e+00 : f32, shape = array<i32>}> : () -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %4 = "ttir.full"() <{fill_value = 0.000000e+00 : f32, shape = array<i32>}> : () -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %5 = ttir.empty() : tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %6 = "ttir.mesh_shard"(%arg0, %5) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %7 = ttir.empty() : tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %8 = "ttir.mesh_shard"(%arg1, %7) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %9 = ttir.empty() : tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %10 = "ttir.mesh_shard"(%arg2, %9) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %11 = ttir.empty() : tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %12 = "ttir.mesh_shard"(%arg3, %11) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %13 = ttir.empty() : tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %14 = "ttir.mesh_shard"(%arg4, %13) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %15 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %16 = "ttir.mesh_shard"(%arg5, %15) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %17 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %18 = "ttir.mesh_shard"(%arg6, %17) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %19 = ttir.empty() : tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %20 = "ttir.mesh_shard"(%arg7, %19) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1024x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %21 = ttir.empty() : tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %22 = "ttir.mesh_shard"(%arg8, %21) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x14336xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x448x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %23 = ttir.empty() : tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %24 = "ttir.mesh_shard"(%arg9, %23) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %25 = ttir.empty() : tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %26 = "ttir.mesh_shard"(%arg10, %25) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %27 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %28 = "ttir.mesh_shard"(%arg11, %27) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %29 = ttir.empty() : tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %30 = "ttir.mesh_shard"(%arg12, %29) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %31 = ttir.empty() : tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %32 = "ttir.mesh_shard"(%arg13, %31) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %33 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %34 = "ttir.mesh_shard"(%arg14, %33) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %35 = ttir.empty() : tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %36 = "ttir.mesh_shard"(%arg15, %35) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<14336x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<448x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %37 = ttir.empty() : tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %38 = "ttir.mesh_shard"(%arg16, %37) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %39 = ttir.empty() : tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %40 = "ttir.mesh_shard"(%arg17, %39) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 8, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %41 = ttir.empty() : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %42 = "ttir.reshape"(%1, %41) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %43 = ttir.empty() : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %44 = "ttir.reshape"(%3, %43) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %45 = ttir.empty() : tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %46 = "ttir.reshape"(%4, %45) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %47 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %48 = "ttir.broadcast"(%46, %47) <{broadcast_dimensions = array<i64: 1, 1, 19, 128>}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %49 = ttir.empty() : tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %50 = "ttir.typecast"(%6, %49) <{conservative_folding = false}> : (tensor<1x14xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %51 = ttir.empty() : tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %52 = "ttir.reshape"(%50, %51) <{shape = [14 : i32]}> : (tensor<1x14xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %53 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %54 = "ttir.embedding"(%52, %8, %53) : (tensor<14xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128256x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %55 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %56 = "ttir.reshape"(%54, %55) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %57 = ttir.empty() : tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %58 = "ttir.reshape"(%18, %57) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %59 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %60 = "ttir.pow"(%56, %44, %59) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %61 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %62 = "ttir.sum"(%60, %61) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %63 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %64 = "ttir.multiply"(%62, %2, %63) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %65 = ttir.empty() : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %66 = "ttir.reshape"(%16, %65) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %67 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %68 = "ttir.broadcast"(%66, %67) <{broadcast_dimensions = array<i64: 1, 14, 1>}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %69 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %71 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %72 = "ttir.add"(%64, %70, %71) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %73 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %74 = "ttir.rsqrt"(%72, %73) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %75 = ttir.empty() : tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %76 = "ttir.reshape"(%74, %75) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %77 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %78 = "ttir.multiply"(%54, %76, %77) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %79 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %80 = "ttir.multiply"(%58, %78, %79) : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %81 = ttir.empty() : tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %82 = "ttir.matmul"(%80, %14, %81) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %83 = ttir.empty() : tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %85 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %86 = "ttir.permute"(%84, %85) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %87 = ttir.empty() : tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %88 = "ttir.reshape"(%12, %87) <{shape = [1 : i32, 64 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %89 = ttir.empty() : tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %90 = "ttir.typecast"(%10, %89) <{conservative_folding = false}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %91 = ttir.empty() : tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32, 14 : i32]}> : (tensor<14xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %93 = ttir.empty() : tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %94 = "ttir.matmul"(%88, %92, %93) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %95 = ttir.empty() : tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %96 = "ttir.permute"(%94, %95) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x14xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %97 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %98 = "ttir.concat"(%96, %96, %97) <{dim = 2 : si32}> : (tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %99 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %100 = "ttir.cos"(%98, %99) : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %101 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %102 = "ttir.reshape"(%100, %101) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %103 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %104 = "ttir.multiply"(%86, %102, %103) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %105 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %106 = "ttir.slice"(%86, %105) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %107 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %108 = "ttir.neg"(%106, %107) : (tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %109 = ttir.empty() : tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %110 = "ttir.slice"(%86, %109) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %111 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %112 = "ttir.concat"(%108, %110, %111) <{dim = 3 : si32}> : (tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %113 = ttir.empty() : tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %114 = "ttir.sin"(%98, %113) : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %115 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %116 = "ttir.reshape"(%114, %115) <{shape = [1 : i32, 1 : i32, 14 : i32, 128 : i32]}> : (tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %117 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %118 = "ttir.multiply"(%112, %116, %117) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %119 = ttir.empty() : tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %120 = "ttir.add"(%104, %118, %119) : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %121 = ttir.empty() : tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %122 = "ttir.typecast"(%120, %121) <{conservative_folding = false}> : (tensor<1x1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %123 = "ttir.fill_cache"(%48, %122) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %124 = ttir.empty() : tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %125 = "ttir.matmul"(%80, %20, %124) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<128x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %126 = ttir.empty() : tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %127 = "ttir.typecast"(%125, %126) <{conservative_folding = false}> : (tensor<14x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %128 = ttir.empty() : tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %129 = "ttir.reshape"(%127, %128) <{shape = [1 : i32, 14 : i32, 1 : i32, 128 : i32]}> : (tensor<14x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %130 = ttir.empty() : tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %131 = "ttir.permute"(%129, %130) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %132 = "ttir.fill_cache"(%48, %131) <{batch_offset = 0 : i32}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %133 = ttir.empty() : tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %134 = "ttir.reshape"(%38, %133) <{shape = [1 : i32, 1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %135 = ttir.empty() : tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %136 = "ttir.matmul"(%80, %32, %135) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<512x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<16x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %137 = ttir.empty() : tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 14 : i32, 4 : i32, 128 : i32]}> : (tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %139 = ttir.empty() : tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %140 = "ttir.permute"(%138, %139) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %141 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %142 = "ttir.reshape"(%140, %141) <{shape = [4 : i32, 14 : i32, 128 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %143 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %144 = "ttir.multiply"(%142, %100, %143) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %145 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %146 = "ttir.slice"(%140, %145) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %147 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %148 = "ttir.neg"(%146, %147) : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %149 = ttir.empty() : tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %150 = "ttir.reshape"(%148, %149) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %151 = ttir.empty() : tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %152 = "ttir.slice"(%140, %151) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 4 : i32, 14 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %153 = ttir.empty() : tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %154 = "ttir.reshape"(%152, %153) <{shape = [4 : i32, 14 : i32, 64 : i32]}> : (tensor<1x4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %155 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %156 = "ttir.concat"(%150, %154, %155) <{dim = 2 : si32}> : (tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %157 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %158 = "ttir.multiply"(%156, %114, %157) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %159 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %160 = "ttir.add"(%144, %158, %159) : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %161 = ttir.empty() : tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %162 = "ttir.permute"(%123, %161) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %163 = ttir.empty() : tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %164 = "ttir.reshape"(%162, %163) <{shape = [1 : i32, 128 : i32, 19 : i32]}> : (tensor<1x1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 128 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %165 = ttir.empty() : tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %166 = "ttir.typecast"(%164, %165) <{conservative_folding = false}> : (tensor<1x128x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %167 = ttir.empty() : tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %168 = "ttir.broadcast"(%166, %167) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %169 = ttir.empty() : tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %170 = "ttir.matmul"(%160, %168, %169) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x128x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<16x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %171 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %172 = "ttir.reshape"(%170, %171) <{shape = [1 : i32, 4 : i32, 14 : i32, 19 : i32]}> : (tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %173 = ttir.empty() : tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %174 = "ttir.reshape"(%30, %173) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %175 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %176 = "ttir.multiply"(%172, %174, %175) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %177 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 14 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %178 = ttir.empty() : tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %179 = "ttir.reshape"(%177, %178) <{shape = [14 : i32, 1 : i32]}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %180 = "ttir.arange"() <{arange_dimension = 0 : i64, end = 19 : si64, start = 0 : si64, step = 1 : si64}> : () -> tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %181 = ttir.empty() : tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %182 = "ttir.reshape"(%180, %181) <{shape = [1 : i32, 19 : i32]}> : (tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %183 = ttir.empty() : tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %184 = "ttir.ge"(%179, %182, %183) : (tensor<14x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %185 = ttir.empty() : tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %186 = "ttir.reshape"(%28, %185) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %187 = ttir.empty() : tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %188 = "ttir.where"(%184, %42, %186, %187) : (tensor<14x19xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %189 = ttir.empty() : tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %190 = "ttir.reshape"(%0, %189) <{shape = [1 : i32, 1 : i32, 1 : i32, 19 : i32]}> : (tensor<19xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %191 = ttir.empty() : tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %192 = "ttir.reshape"(%10, %191) <{shape = [1 : i32, 1 : i32, 14 : i32, 1 : i32]}> : (tensor<14xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %193 = ttir.empty() : tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %194 = "ttir.gt"(%190, %192, %193) : (tensor<1x1x1x19xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x1xsi32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %195 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x1x14x19xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %197 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %198 = "ttir.reshape"(%188, %197) <{shape = [1 : i32, 1 : i32, 14 : i32, 19 : i32]}> : (tensor<14x19xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %199 = ttir.empty() : tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %200 = "ttir.multiply"(%198, %196, %199) : (tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %201 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %202 = "ttir.add"(%176, %200, %201) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %203 = ttir.empty() : tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %204 = "ttir.max"(%202, %203) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %205 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %206 = "ttir.subtract"(%202, %204, %205) : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %207 = ttir.empty() : tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %208 = "ttir.softmax"(%206, %207) <{dimension = 3 : si32}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %209 = ttir.empty() : tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %210 = "ttir.reshape"(%208, %209) <{shape = [4 : i32, 14 : i32, 19 : i32]}> : (tensor<1x4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %211 = ttir.empty() : tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %212 = "ttir.reshape"(%132, %211) <{shape = [1 : i32, 19 : i32, 128 : i32]}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %213 = ttir.empty() : tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %214 = "ttir.typecast"(%212, %213) <{conservative_folding = false}> : (tensor<1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %215 = ttir.empty() : tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %216 = "ttir.broadcast"(%214, %215) <{broadcast_dimensions = array<i64: 4, 1, 1>}> : (tensor<1x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %217 = ttir.empty() : tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %218 = "ttir.matmul"(%210, %216, %217) <{transpose_a = false, transpose_b = false}> : (tensor<4x14x19xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x19x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %219 = ttir.empty() : tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %220 = "ttir.reshape"(%218, %219) <{shape = [1 : i32, 4 : i32, 14 : i32, 128 : i32]}> : (tensor<4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %221 = ttir.empty() : tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %222 = "ttir.permute"(%220, %221) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x4x14x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %223 = ttir.empty() : tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %224 = "ttir.reshape"(%222, %223) <{shape = [14 : i32, 512 : i32]}> : (tensor<1x14x4x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %225 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %226 = "ttir.matmul"(%224, %26, %225) <{transpose_a = false, transpose_b = true}> : (tensor<14x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x512xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x16x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %227 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %228 = "ttir.all_reduce"(%226, %227) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %229 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %230 = "ttir.add"(%54, %228, %229) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %231 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %232 = "ttir.reshape"(%230, %231) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %233 = ttir.empty() : tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %234 = "ttir.reshape"(%34, %233) <{shape = [1 : i32, 4096 : i32]}> : (tensor<4096xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %235 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %236 = "ttir.pow"(%232, %44, %235) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %237 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %238 = "ttir.sum"(%236, %237) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %239 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %240 = "ttir.multiply"(%238, %2, %239) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %241 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %242 = "ttir.reshape"(%68, %241) <{shape = [1 : i32, 14 : i32]}> : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %243 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %244 = "ttir.add"(%240, %242, %243) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %245 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %246 = "ttir.rsqrt"(%244, %245) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %247 = ttir.empty() : tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %248 = "ttir.reshape"(%246, %247) <{shape = [14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %249 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %250 = "ttir.multiply"(%230, %248, %249) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %251 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %252 = "ttir.multiply"(%234, %250, %251) : (tensor<1x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %253 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %254 = "ttir.matmul"(%252, %36, %253) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %255 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %256 = "ttir.sigmoid"(%254, %255) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %257 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %258 = "ttir.multiply"(%254, %256, %257) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %259 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %260 = "ttir.matmul"(%252, %24, %259) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1792x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<56x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %261 = ttir.empty() : tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %262 = "ttir.multiply"(%258, %260, %261) : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %263 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %264 = "ttir.matmul"(%262, %22, %263) <{transpose_a = false, transpose_b = true}> : (tensor<14x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<4096x1792xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x56x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %265 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %266 = "ttir.all_reduce"(%264, %265) <{cluster_axis = 1 : ui32, reduce_type = #ttcore.reduce_type<sum>}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %267 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %268 = "ttir.add"(%230, %266, %267) : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %269 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %270 = "ttir.reshape"(%268, %269) <{shape = [1 : i32, 14 : i32, 4096 : i32]}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %271 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %272 = "ttir.pow"(%270, %44, %271) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %273 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %274 = "ttir.sum"(%272, %273) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %275 = ttir.empty() : tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %276 = "ttir.multiply"(%274, %2, %275) : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %277 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %278 = "ttir.reshape"(%276, %277) <{shape = [1 : i32, 14 : i32, 1 : i32]}> : (tensor<1x14xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %279 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %280 = "ttir.add"(%278, %66, %279) : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %281 = ttir.empty() : tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %282 = "ttir.rsqrt"(%280, %281) : (tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %283 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %284 = "ttir.multiply"(%270, %282, %283) : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %285 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %286 = "ttir.multiply"(%134, %284, %285) : (tensor<1x1x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %287 = ttir.empty() : tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %288 = "ttir.reshape"(%286, %287) <{shape = [14 : i32, 4096 : i32]}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %289 = ttir.empty() : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %290 = "ttir.matmul"(%288, %40, %289) <{transpose_a = false, transpose_b = true}> : (tensor<14x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<16032x4096xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<501x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %291 = ttir.empty() : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %292 = "ttir.reshape"(%290, %291) <{shape = [1 : i32, 14 : i32, 16032 : i32]}> : (tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %293 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %294 = ttir.to_layout %56, %293 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %295 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %296 = "ttir.mesh_shard"(%294, %295) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %297 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %298 = ttir.to_layout %123, %297 : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>> -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %299 = ttir.empty() : tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %300 = "ttir.mesh_shard"(%298, %299) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %301 = ttir.empty() : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %302 = ttir.to_layout %132, %301 : tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>> -> tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %303 = ttir.empty() : tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %304 = "ttir.mesh_shard"(%302, %303) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8, 1, 1>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x1x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 19 + d1 * 19 + d2, d3), <1x1>, memref<19x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>
    %305 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %306 = ttir.to_layout %286, %305 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x128x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %307 = ttir.empty() : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %308 = "ttir.mesh_shard"(%306, %307) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>
    %309 = ttir.empty() : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %310 = ttir.to_layout %290, %309 : tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %311 = ttir.empty() : tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %312 = "ttir.mesh_shard"(%310, %311) <{shard_dims = array<i64: -1, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<14x16032xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %313 = ttir.empty() : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %314 = ttir.to_layout %292, %313 : tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x501x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> into tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>> -> tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>
    %315 = ttir.empty() : tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    %316 = "ttir.mesh_shard"(%314, %315) <{shard_dims = array<i64: -1, 2>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1, 1, 8>, shard_type = #ttcore.shard_type<devices>}> : (tensor<1x14x16032xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x16032xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
    return %296, %300, %304, %308, %312, %316 : tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x19x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 152 + d1 * 19 + d2, d3), <1x1>, memref<152x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x4096xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x4096xf32, #ttnn.buffer_type<system_memory>>>>, tensor<14x128256xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>, tensor<1x14x128256xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 14 + d1, d2), <1x1>, memref<14x128256xf32, #ttnn.buffer_type<system_memory>>>>
  }
}

