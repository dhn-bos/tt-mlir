import ttnn
import my_get_device
import utils
def forward(v1): 
  v2 = v1[0]
  v3 = v1[1]
  v4 = v1[2]
  v5 = v1[3]
  v6 = v1[4]
  v7 = v1[5]
  v8 = v1[6]
  v9 = v1[7]
  v10 = v1[8]
  v11 = v1[9]
  v12 = v1[10]
  v13 = v1[11]
  v14 = v1[12]
  v15 = v1[13]
  v16 = v1[14]
  v17 = v1[15]
  v18 = v1[16]
  v19 = v1[17]
  v20 = v1[18]
  v21 = v1[19]
  v22 = v1[20]
  v23 = v1[21]
  v24 = v1[22]
  v25 = v1[23]
  v26 = v1[24]
  v27 = v1[25]
  v28 = v1[26]
  v29 = v1[27]
  v30 = v1[28]
  v31 = v1[29]
  v32 = v1[30]
  v33 = v1[31]
  v34 = v1[32]
  v35 = v1[33]
  v36 = v1[34]
  v37 = v1[35]
  v38 = v1[36]
  v39 = v1[37]
  v40 = v1[38]
  v41 = v1[39]
  v42 = v1[40]
  v43 = v1[41]
  v44 = v1[42]
  v45 = v1[43]
  v46 = v1[44]
  v47 = v1[45]
  v48 = v1[46]
  v49 = v1[47]
  v50 = v1[48]
  v51 = v1[49]
  v52 = v1[50]
  v53 = v1[51]
  v54 = v1[52]
  v55 = v1[53]
  v56 = v1[54]
  v57 = v1[55]
  v58 = v1[56]
  v59 = v1[57]
  v60 = v1[58]
  v61 = v1[59]
  v62 = v1[60]
  v63 = v1[61]
  v64 = v1[62]
  v65 = v1[63]
  v66 = v1[64]
  v67 = v1[65]
  v68 = v1[66]
  v69 = v1[67]
  v70 = v1[68]
  v71 = v1[69]
  v72 = v1[70]
  v73 = v1[71]
  v74 = v1[72]
  v75 = v1[73]
  v76 = v1[74]
  v77 = v1[75]
  v78 = v1[76]
  v79 = v1[77]
  v80 = v1[78]
  v81 = v1[79]
  v82 = v1[80]
  v83 = v1[81]
  v84 = v1[82]
  v85 = v1[83]
  v86 = v1[84]
  v87 = v1[85]
  v88 = v1[86]
  v89 = v1[87]
  v90 = v1[88]
  v91 = v1[89]
  v92 = v1[90]
  v93 = v1[91]
  v94 = v1[92]
  v95 = v1[93]
  v96 = v1[94]
  v97 = v1[95]
  v98 = v1[96]
  v99 = v1[97]
  v100 = v1[98]
  v101 = v1[99]
  v102 = v1[100]
  v103 = v1[101]
  v104 = v1[102]
  v105 = v1[103]
  v106 = v1[104]
  v107 = v1[105]
  v108 = v1[106]
  v109 = v1[107]
  v110 = v1[108]
  v111 = v1[109]
  v112 = v1[110]
  v113 = v1[111]
  v114 = v1[112]
  v115 = v1[113]
  v116 = v1[114]
  v117 = v1[115]
  v118 = v1[116]
  v119 = v1[117]
  v120 = v1[118]
  v121 = v1[119]
  v122 = v1[120]
  v123 = v1[121]
  v124 = v1[122]
  v125 = v1[123]
  v126 = v1[124]
  v127 = v1[125]
  v128 = v1[126]
  v129 = v1[127]
  v130 = v1[128]
  v131 = v1[129]
  v132 = v1[130]
  v133 = v1[131]
  v134 = v1[132]
  v135 = v1[133]
  v136 = v1[134]
  v137 = v1[135]
  v138 = v1[136]
  v139 = v1[137]
  v140 = v1[138]
  v141 = v1[139]
  v142 = v1[140]
  v143 = v1[141]
  v144 = v1[142]
  v145 = v1[143]
  v146 = v1[144]
  v147 = v1[145]
  v148 = v1[146]
  v149 = v1[147]
  v150 = v1[148]
  v151 = v1[149]
  v152 = v1[150]
  v153 = v1[151]
  v154 = v1[152]
  v155 = v1[153]
  v156 = v1[154]
  v157 = v1[155]
  v158 = v1[156]
  v159 = v1[157]
  v160 = v1[158]
  v161 = v1[159]
  v162 = v1[160]
  v163 = v1[161]
  v164 = my_get_device.DeviceGetter.get_device()
  v165 = ttnn.permute(v2, [0, 2, 3, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v2, False)
  v166 = ttnn.reshape(v165, [1, 1, 50176, 3], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v165, False)
  v167 = ttnn.to_layout(v166, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v166, False)
  v168 = ttnn.conv2d(input_tensor=v167, weight_tensor=v109, device=v164, in_channels=3, out_channels=64, batch_size=1, input_height=224, input_width=224, kernel_size=[7, 7], stride=[2, 2], padding=[3, 3, 3, 3], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v167, False)
  ttnn.deallocate(v109, False)
  v169 = ttnn.multiply(v168, v3, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v168, False)
  ttnn.deallocate(v3, False)
  v170 = ttnn.add(v169, v4, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v169, False)
  ttnn.deallocate(v4, False)
  v171 = ttnn.relu(v170, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v170, False)
  v172 = ttnn.to_layout(v171, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v171, False)
  v173 = ttnn.max_pool2d(v172, 1, 112, 112, 64, [3, 3], [2, 2], [1, 1], [1, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), applied_shard_scheme=None, ceil_mode=False, in_place_halo=False)
  ttnn.deallocate(v172, False)
  v174 = ttnn.conv2d(input_tensor=v173, weight_tensor=v110, device=v164, in_channels=64, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v110, False)
  v175 = ttnn.multiply(v174, v5, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v174, False)
  ttnn.deallocate(v5, False)
  v176 = ttnn.add(v175, v6, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v175, False)
  ttnn.deallocate(v6, False)
  v177 = ttnn.relu(v176, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v176, False)
  v178 = ttnn.to_layout(v177, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v177, False)
  v179 = ttnn.conv2d(input_tensor=v178, weight_tensor=v111, device=v164, in_channels=64, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v178, False)
  ttnn.deallocate(v111, False)
  v180 = ttnn.multiply(v179, v7, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v179, False)
  ttnn.deallocate(v7, False)
  v181 = ttnn.add(v180, v8, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v180, False)
  ttnn.deallocate(v8, False)
  v182 = ttnn.relu(v181, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v181, False)
  v183 = ttnn.to_layout(v182, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v182, False)
  v184 = ttnn.conv2d(input_tensor=v183, weight_tensor=v112, device=v164, in_channels=64, out_channels=256, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v183, False)
  ttnn.deallocate(v112, False)
  v185 = ttnn.multiply(v184, v9, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v184, False)
  ttnn.deallocate(v9, False)
  v186 = ttnn.add(v185, v10, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v185, False)
  ttnn.deallocate(v10, False)
  v187 = ttnn.conv2d(input_tensor=v173, weight_tensor=v113, device=v164, in_channels=64, out_channels=256, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v173, False)
  ttnn.deallocate(v113, False)
  v188 = ttnn.multiply(v187, v11, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v187, False)
  ttnn.deallocate(v11, False)
  v189 = ttnn.add(v188, v12, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v188, False)
  ttnn.deallocate(v12, False)
  v190 = ttnn.add(v186, v189, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v189, False)
  ttnn.deallocate(v186, False)
  v191 = ttnn.relu(v190, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v190, False)
  v192 = ttnn.to_layout(v191, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v193 = ttnn.conv2d(input_tensor=v192, weight_tensor=v114, device=v164, in_channels=256, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v192, False)
  ttnn.deallocate(v114, False)
  v194 = ttnn.multiply(v193, v13, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v193, False)
  ttnn.deallocate(v13, False)
  v195 = ttnn.add(v194, v14, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v194, False)
  ttnn.deallocate(v14, False)
  v196 = ttnn.relu(v195, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v195, False)
  v197 = ttnn.to_layout(v196, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v196, False)
  v198 = ttnn.conv2d(input_tensor=v197, weight_tensor=v115, device=v164, in_channels=64, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v197, False)
  ttnn.deallocate(v115, False)
  v199 = ttnn.multiply(v198, v15, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v198, False)
  ttnn.deallocate(v15, False)
  v200 = ttnn.add(v199, v16, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v199, False)
  ttnn.deallocate(v16, False)
  v201 = ttnn.relu(v200, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v200, False)
  v202 = ttnn.to_layout(v201, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v201, False)
  v203 = ttnn.conv2d(input_tensor=v202, weight_tensor=v116, device=v164, in_channels=64, out_channels=256, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v202, False)
  ttnn.deallocate(v116, False)
  v204 = ttnn.multiply(v203, v17, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v203, False)
  ttnn.deallocate(v17, False)
  v205 = ttnn.add(v204, v18, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v204, False)
  ttnn.deallocate(v18, False)
  v206 = ttnn.add(v205, v191, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v205, False)
  ttnn.deallocate(v191, False)
  v207 = ttnn.relu(v206, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v206, False)
  v208 = ttnn.to_layout(v207, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v209 = ttnn.conv2d(input_tensor=v208, weight_tensor=v117, device=v164, in_channels=256, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v208, False)
  ttnn.deallocate(v117, False)
  v210 = ttnn.multiply(v209, v19, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v209, False)
  ttnn.deallocate(v19, False)
  v211 = ttnn.add(v210, v20, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v210, False)
  ttnn.deallocate(v20, False)
  v212 = ttnn.relu(v211, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v211, False)
  v213 = ttnn.to_layout(v212, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v212, False)
  v214 = ttnn.conv2d(input_tensor=v213, weight_tensor=v118, device=v164, in_channels=64, out_channels=64, batch_size=1, input_height=56, input_width=56, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v213, False)
  ttnn.deallocate(v118, False)
  v215 = ttnn.multiply(v214, v21, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v214, False)
  ttnn.deallocate(v21, False)
  v216 = ttnn.add(v215, v22, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v215, False)
  ttnn.deallocate(v22, False)
  v217 = ttnn.relu(v216, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v216, False)
  v218 = ttnn.to_layout(v217, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v217, False)
  v219 = ttnn.conv2d(input_tensor=v218, weight_tensor=v119, device=v164, in_channels=64, out_channels=256, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v218, False)
  ttnn.deallocate(v119, False)
  v220 = ttnn.multiply(v219, v23, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v219, False)
  ttnn.deallocate(v23, False)
  v221 = ttnn.add(v220, v24, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v220, False)
  ttnn.deallocate(v24, False)
  v222 = ttnn.add(v221, v207, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v221, False)
  ttnn.deallocate(v207, False)
  v223 = ttnn.relu(v222, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v222, False)
  v224 = ttnn.to_layout(v223, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v225 = ttnn.conv2d(input_tensor=v224, weight_tensor=v120, device=v164, in_channels=256, out_channels=128, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v224, False)
  ttnn.deallocate(v120, False)
  v226 = ttnn.multiply(v225, v25, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v225, False)
  ttnn.deallocate(v25, False)
  v227 = ttnn.add(v226, v26, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v226, False)
  ttnn.deallocate(v26, False)
  v228 = ttnn.relu(v227, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v227, False)
  v229 = ttnn.to_layout(v228, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v228, False)
  v230 = ttnn.conv2d(input_tensor=v229, weight_tensor=v121, device=v164, in_channels=128, out_channels=128, batch_size=1, input_height=56, input_width=56, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v229, False)
  ttnn.deallocate(v121, False)
  v231 = ttnn.multiply(v230, v27, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v230, False)
  ttnn.deallocate(v27, False)
  v232 = ttnn.add(v231, v28, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v231, False)
  ttnn.deallocate(v28, False)
  v233 = ttnn.relu(v232, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v232, False)
  v234 = ttnn.to_layout(v233, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v233, False)
  v235 = ttnn.conv2d(input_tensor=v234, weight_tensor=v122, device=v164, in_channels=128, out_channels=512, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v234, False)
  ttnn.deallocate(v122, False)
  v236 = ttnn.multiply(v235, v29, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v235, False)
  ttnn.deallocate(v29, False)
  v237 = ttnn.add(v236, v30, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v236, False)
  ttnn.deallocate(v30, False)
  v238 = ttnn.to_layout(v223, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v223, False)
  v239 = ttnn.conv2d(input_tensor=v238, weight_tensor=v123, device=v164, in_channels=256, out_channels=512, batch_size=1, input_height=56, input_width=56, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v238, False)
  ttnn.deallocate(v123, False)
  v240 = ttnn.multiply(v239, v31, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v239, False)
  ttnn.deallocate(v31, False)
  v241 = ttnn.add(v240, v32, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v240, False)
  ttnn.deallocate(v32, False)
  v242 = ttnn.add(v237, v241, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v241, False)
  ttnn.deallocate(v237, False)
  v243 = ttnn.relu(v242, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v242, False)
  v244 = ttnn.to_layout(v243, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v245 = ttnn.conv2d(input_tensor=v244, weight_tensor=v124, device=v164, in_channels=512, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v244, False)
  ttnn.deallocate(v124, False)
  v246 = ttnn.multiply(v245, v33, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v245, False)
  ttnn.deallocate(v33, False)
  v247 = ttnn.add(v246, v34, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v246, False)
  ttnn.deallocate(v34, False)
  v248 = ttnn.relu(v247, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v247, False)
  v249 = ttnn.to_layout(v248, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v248, False)
  v250 = ttnn.conv2d(input_tensor=v249, weight_tensor=v125, device=v164, in_channels=128, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v249, False)
  ttnn.deallocate(v125, False)
  v251 = ttnn.multiply(v250, v35, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v250, False)
  ttnn.deallocate(v35, False)
  v252 = ttnn.add(v251, v36, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v251, False)
  ttnn.deallocate(v36, False)
  v253 = ttnn.relu(v252, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v252, False)
  v254 = ttnn.to_layout(v253, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v253, False)
  v255 = ttnn.conv2d(input_tensor=v254, weight_tensor=v126, device=v164, in_channels=128, out_channels=512, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v254, False)
  ttnn.deallocate(v126, False)
  v256 = ttnn.multiply(v255, v37, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v255, False)
  ttnn.deallocate(v37, False)
  v257 = ttnn.add(v256, v38, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v256, False)
  ttnn.deallocate(v38, False)
  v258 = ttnn.add(v257, v243, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v257, False)
  ttnn.deallocate(v243, False)
  v259 = ttnn.relu(v258, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v258, False)
  v260 = ttnn.to_layout(v259, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v261 = ttnn.conv2d(input_tensor=v260, weight_tensor=v127, device=v164, in_channels=512, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v260, False)
  ttnn.deallocate(v127, False)
  v262 = ttnn.multiply(v261, v39, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v261, False)
  ttnn.deallocate(v39, False)
  v263 = ttnn.add(v262, v40, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v262, False)
  ttnn.deallocate(v40, False)
  v264 = ttnn.relu(v263, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v263, False)
  v265 = ttnn.to_layout(v264, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v264, False)
  v266 = ttnn.conv2d(input_tensor=v265, weight_tensor=v128, device=v164, in_channels=128, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v265, False)
  ttnn.deallocate(v128, False)
  v267 = ttnn.multiply(v266, v41, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v266, False)
  ttnn.deallocate(v41, False)
  v268 = ttnn.add(v267, v42, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v267, False)
  ttnn.deallocate(v42, False)
  v269 = ttnn.relu(v268, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v268, False)
  v270 = ttnn.to_layout(v269, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v269, False)
  v271 = ttnn.conv2d(input_tensor=v270, weight_tensor=v129, device=v164, in_channels=128, out_channels=512, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v270, False)
  ttnn.deallocate(v129, False)
  v272 = ttnn.multiply(v271, v43, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v271, False)
  ttnn.deallocate(v43, False)
  v273 = ttnn.add(v272, v44, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v272, False)
  ttnn.deallocate(v44, False)
  v274 = ttnn.add(v273, v259, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v273, False)
  ttnn.deallocate(v259, False)
  v275 = ttnn.relu(v274, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v274, False)
  v276 = ttnn.to_layout(v275, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v277 = ttnn.conv2d(input_tensor=v276, weight_tensor=v130, device=v164, in_channels=512, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v276, False)
  ttnn.deallocate(v130, False)
  v278 = ttnn.multiply(v277, v45, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v277, False)
  ttnn.deallocate(v45, False)
  v279 = ttnn.add(v278, v46, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v278, False)
  ttnn.deallocate(v46, False)
  v280 = ttnn.relu(v279, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v279, False)
  v281 = ttnn.to_layout(v280, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v280, False)
  v282 = ttnn.conv2d(input_tensor=v281, weight_tensor=v131, device=v164, in_channels=128, out_channels=128, batch_size=1, input_height=28, input_width=28, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v281, False)
  ttnn.deallocate(v131, False)
  v283 = ttnn.multiply(v282, v47, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v282, False)
  ttnn.deallocate(v47, False)
  v284 = ttnn.add(v283, v48, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v283, False)
  ttnn.deallocate(v48, False)
  v285 = ttnn.relu(v284, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v284, False)
  v286 = ttnn.to_layout(v285, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v285, False)
  v287 = ttnn.conv2d(input_tensor=v286, weight_tensor=v132, device=v164, in_channels=128, out_channels=512, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v286, False)
  ttnn.deallocate(v132, False)
  v288 = ttnn.multiply(v287, v49, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v287, False)
  ttnn.deallocate(v49, False)
  v289 = ttnn.add(v288, v50, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v288, False)
  ttnn.deallocate(v50, False)
  v290 = ttnn.add(v289, v275, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v289, False)
  ttnn.deallocate(v275, False)
  v291 = ttnn.relu(v290, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v290, False)
  v292 = ttnn.to_layout(v291, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v293 = ttnn.conv2d(input_tensor=v292, weight_tensor=v133, device=v164, in_channels=512, out_channels=256, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v292, False)
  ttnn.deallocate(v133, False)
  v294 = ttnn.multiply(v293, v51, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v293, False)
  ttnn.deallocate(v51, False)
  v295 = ttnn.add(v294, v52, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v294, False)
  ttnn.deallocate(v52, False)
  v296 = ttnn.relu(v295, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v295, False)
  v297 = ttnn.to_layout(v296, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v296, False)
  v298 = ttnn.conv2d(input_tensor=v297, weight_tensor=v134, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=28, input_width=28, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v297, False)
  ttnn.deallocate(v134, False)
  v299 = ttnn.multiply(v298, v53, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v298, False)
  ttnn.deallocate(v53, False)
  v300 = ttnn.add(v299, v54, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v299, False)
  ttnn.deallocate(v54, False)
  v301 = ttnn.relu(v300, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v300, False)
  v302 = ttnn.to_layout(v301, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v301, False)
  v303 = ttnn.conv2d(input_tensor=v302, weight_tensor=v135, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v302, False)
  ttnn.deallocate(v135, False)
  v304 = ttnn.multiply(v303, v55, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v303, False)
  ttnn.deallocate(v55, False)
  v305 = ttnn.add(v304, v56, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v304, False)
  ttnn.deallocate(v56, False)
  v306 = ttnn.to_layout(v291, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v291, False)
  v307 = ttnn.conv2d(input_tensor=v306, weight_tensor=v136, device=v164, in_channels=512, out_channels=1024, batch_size=1, input_height=28, input_width=28, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v306, False)
  ttnn.deallocate(v136, False)
  v308 = ttnn.multiply(v307, v57, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v307, False)
  ttnn.deallocate(v57, False)
  v309 = ttnn.add(v308, v58, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v308, False)
  ttnn.deallocate(v58, False)
  v310 = ttnn.add(v305, v309, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v309, False)
  ttnn.deallocate(v305, False)
  v311 = ttnn.relu(v310, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v310, False)
  v312 = ttnn.to_layout(v311, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v313 = ttnn.conv2d(input_tensor=v312, weight_tensor=v137, device=v164, in_channels=1024, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v312, False)
  ttnn.deallocate(v137, False)
  v314 = ttnn.multiply(v313, v59, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v313, False)
  ttnn.deallocate(v59, False)
  v315 = ttnn.add(v314, v60, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v314, False)
  ttnn.deallocate(v60, False)
  v316 = ttnn.relu(v315, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v315, False)
  v317 = ttnn.to_layout(v316, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v316, False)
  v318 = ttnn.conv2d(input_tensor=v317, weight_tensor=v138, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v317, False)
  ttnn.deallocate(v138, False)
  v319 = ttnn.multiply(v318, v61, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v318, False)
  ttnn.deallocate(v61, False)
  v320 = ttnn.add(v319, v62, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v319, False)
  ttnn.deallocate(v62, False)
  v321 = ttnn.relu(v320, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v320, False)
  v322 = ttnn.to_layout(v321, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v321, False)
  v323 = ttnn.conv2d(input_tensor=v322, weight_tensor=v139, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v322, False)
  ttnn.deallocate(v139, False)
  v324 = ttnn.multiply(v323, v63, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v323, False)
  ttnn.deallocate(v63, False)
  v325 = ttnn.add(v324, v64, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v324, False)
  ttnn.deallocate(v64, False)
  v326 = ttnn.add(v325, v311, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v325, False)
  ttnn.deallocate(v311, False)
  v327 = ttnn.relu(v326, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v326, False)
  v328 = ttnn.to_layout(v327, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v329 = ttnn.conv2d(input_tensor=v328, weight_tensor=v140, device=v164, in_channels=1024, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v328, False)
  ttnn.deallocate(v140, False)
  v330 = ttnn.multiply(v329, v65, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v329, False)
  ttnn.deallocate(v65, False)
  v331 = ttnn.add(v330, v66, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v330, False)
  ttnn.deallocate(v66, False)
  v332 = ttnn.relu(v331, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v331, False)
  v333 = ttnn.to_layout(v332, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v332, False)
  v334 = ttnn.conv2d(input_tensor=v333, weight_tensor=v141, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v333, False)
  ttnn.deallocate(v141, False)
  v335 = ttnn.multiply(v334, v67, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v334, False)
  ttnn.deallocate(v67, False)
  v336 = ttnn.add(v335, v68, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v335, False)
  ttnn.deallocate(v68, False)
  v337 = ttnn.relu(v336, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v336, False)
  v338 = ttnn.to_layout(v337, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v337, False)
  v339 = ttnn.conv2d(input_tensor=v338, weight_tensor=v142, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v338, False)
  ttnn.deallocate(v142, False)
  v340 = ttnn.multiply(v339, v69, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v339, False)
  ttnn.deallocate(v69, False)
  v341 = ttnn.add(v340, v70, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v340, False)
  ttnn.deallocate(v70, False)
  v342 = ttnn.add(v341, v327, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v341, False)
  ttnn.deallocate(v327, False)
  v343 = ttnn.relu(v342, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v342, False)
  v344 = ttnn.to_layout(v343, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v345 = ttnn.conv2d(input_tensor=v344, weight_tensor=v143, device=v164, in_channels=1024, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v344, False)
  ttnn.deallocate(v143, False)
  v346 = ttnn.multiply(v345, v71, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v345, False)
  ttnn.deallocate(v71, False)
  v347 = ttnn.add(v346, v72, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v346, False)
  ttnn.deallocate(v72, False)
  v348 = ttnn.relu(v347, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v347, False)
  v349 = ttnn.to_layout(v348, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v348, False)
  v350 = ttnn.conv2d(input_tensor=v349, weight_tensor=v144, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v349, False)
  ttnn.deallocate(v144, False)
  v351 = ttnn.multiply(v350, v73, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v350, False)
  ttnn.deallocate(v73, False)
  v352 = ttnn.add(v351, v74, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v351, False)
  ttnn.deallocate(v74, False)
  v353 = ttnn.relu(v352, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v352, False)
  v354 = ttnn.to_layout(v353, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v353, False)
  v355 = ttnn.conv2d(input_tensor=v354, weight_tensor=v145, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v354, False)
  ttnn.deallocate(v145, False)
  v356 = ttnn.multiply(v355, v75, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v355, False)
  ttnn.deallocate(v75, False)
  v357 = ttnn.add(v356, v76, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v356, False)
  ttnn.deallocate(v76, False)
  v358 = ttnn.add(v357, v343, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v357, False)
  ttnn.deallocate(v343, False)
  v359 = ttnn.relu(v358, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v358, False)
  v360 = ttnn.to_layout(v359, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v361 = ttnn.conv2d(input_tensor=v360, weight_tensor=v146, device=v164, in_channels=1024, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v360, False)
  ttnn.deallocate(v146, False)
  v362 = ttnn.multiply(v361, v77, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v361, False)
  ttnn.deallocate(v77, False)
  v363 = ttnn.add(v362, v78, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v362, False)
  ttnn.deallocate(v78, False)
  v364 = ttnn.relu(v363, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v363, False)
  v365 = ttnn.to_layout(v364, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v364, False)
  v366 = ttnn.conv2d(input_tensor=v365, weight_tensor=v147, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v365, False)
  ttnn.deallocate(v147, False)
  v367 = ttnn.multiply(v366, v79, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v366, False)
  ttnn.deallocate(v79, False)
  v368 = ttnn.add(v367, v80, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v367, False)
  ttnn.deallocate(v80, False)
  v369 = ttnn.relu(v368, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v368, False)
  v370 = ttnn.to_layout(v369, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v369, False)
  v371 = ttnn.conv2d(input_tensor=v370, weight_tensor=v148, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v370, False)
  ttnn.deallocate(v148, False)
  v372 = ttnn.multiply(v371, v81, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v371, False)
  ttnn.deallocate(v81, False)
  v373 = ttnn.add(v372, v82, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v372, False)
  ttnn.deallocate(v82, False)
  v374 = ttnn.add(v373, v359, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v373, False)
  ttnn.deallocate(v359, False)
  v375 = ttnn.relu(v374, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v374, False)
  v376 = ttnn.to_layout(v375, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v377 = ttnn.conv2d(input_tensor=v376, weight_tensor=v149, device=v164, in_channels=1024, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v376, False)
  ttnn.deallocate(v149, False)
  v378 = ttnn.multiply(v377, v83, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v377, False)
  ttnn.deallocate(v83, False)
  v379 = ttnn.add(v378, v84, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v378, False)
  ttnn.deallocate(v84, False)
  v380 = ttnn.relu(v379, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v379, False)
  v381 = ttnn.to_layout(v380, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v380, False)
  v382 = ttnn.conv2d(input_tensor=v381, weight_tensor=v150, device=v164, in_channels=256, out_channels=256, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v381, False)
  ttnn.deallocate(v150, False)
  v383 = ttnn.multiply(v382, v85, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v382, False)
  ttnn.deallocate(v85, False)
  v384 = ttnn.add(v383, v86, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v383, False)
  ttnn.deallocate(v86, False)
  v385 = ttnn.relu(v384, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v384, False)
  v386 = ttnn.to_layout(v385, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v385, False)
  v387 = ttnn.conv2d(input_tensor=v386, weight_tensor=v151, device=v164, in_channels=256, out_channels=1024, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v386, False)
  ttnn.deallocate(v151, False)
  v388 = ttnn.multiply(v387, v87, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v387, False)
  ttnn.deallocate(v87, False)
  v389 = ttnn.add(v388, v88, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v388, False)
  ttnn.deallocate(v88, False)
  v390 = ttnn.add(v389, v375, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v389, False)
  ttnn.deallocate(v375, False)
  v391 = ttnn.relu(v390, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v390, False)
  v392 = ttnn.to_layout(v391, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  v393 = ttnn.conv2d(input_tensor=v392, weight_tensor=v152, device=v164, in_channels=1024, out_channels=512, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v392, False)
  ttnn.deallocate(v152, False)
  v394 = ttnn.multiply(v393, v89, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v393, False)
  ttnn.deallocate(v89, False)
  v395 = ttnn.add(v394, v90, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v394, False)
  ttnn.deallocate(v90, False)
  v396 = ttnn.relu(v395, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v395, False)
  v397 = ttnn.to_layout(v396, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v396, False)
  v398 = ttnn.conv2d(input_tensor=v397, weight_tensor=v153, device=v164, in_channels=512, out_channels=512, batch_size=1, input_height=14, input_width=14, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v397, False)
  ttnn.deallocate(v153, False)
  v399 = ttnn.multiply(v398, v91, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v398, False)
  ttnn.deallocate(v91, False)
  v400 = ttnn.add(v399, v92, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v399, False)
  ttnn.deallocate(v92, False)
  v401 = ttnn.relu(v400, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v400, False)
  v402 = ttnn.to_layout(v401, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v401, False)
  v403 = ttnn.conv2d(input_tensor=v402, weight_tensor=v154, device=v164, in_channels=512, out_channels=2048, batch_size=1, input_height=7, input_width=7, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v402, False)
  ttnn.deallocate(v154, False)
  v404 = ttnn.reshape(v403, [1, 7, 7, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v403, False)
  v405 = ttnn.permute(v404, [0, 3, 1, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v404, False)
  v406 = ttnn.multiply(v405, v93, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v405, False)
  ttnn.deallocate(v93, False)
  v407 = ttnn.add(v406, v94, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v406, False)
  ttnn.deallocate(v94, False)
  v408 = ttnn.to_layout(v391, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v391, False)
  v409 = ttnn.conv2d(input_tensor=v408, weight_tensor=v155, device=v164, in_channels=1024, out_channels=2048, batch_size=1, input_height=14, input_width=14, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v408, False)
  ttnn.deallocate(v155, False)
  v410 = ttnn.reshape(v409, [1, 7, 7, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v409, False)
  v411 = ttnn.permute(v410, [0, 3, 1, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v410, False)
  v412 = ttnn.multiply(v411, v95, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v411, False)
  ttnn.deallocate(v95, False)
  v413 = ttnn.add(v412, v96, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v412, False)
  ttnn.deallocate(v96, False)
  v414 = ttnn.add(v407, v413, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v413, False)
  ttnn.deallocate(v407, False)
  v415 = ttnn.relu(v414, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v414, False)
  v416 = ttnn.permute(v415, [0, 2, 3, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  v417 = ttnn.reshape(v416, [1, 1, 49, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v416, False)
  v418 = ttnn.to_layout(v417, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v417, False)
  v419 = ttnn.conv2d(input_tensor=v418, weight_tensor=v156, device=v164, in_channels=2048, out_channels=512, batch_size=1, input_height=7, input_width=7, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v418, False)
  ttnn.deallocate(v156, False)
  v420 = ttnn.multiply(v419, v97, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v419, False)
  ttnn.deallocate(v97, False)
  v421 = ttnn.add(v420, v98, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v420, False)
  ttnn.deallocate(v98, False)
  v422 = ttnn.relu(v421, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v421, False)
  v423 = ttnn.to_layout(v422, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v422, False)
  v424 = ttnn.conv2d(input_tensor=v423, weight_tensor=v157, device=v164, in_channels=512, out_channels=512, batch_size=1, input_height=7, input_width=7, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v423, False)
  ttnn.deallocate(v157, False)
  v425 = ttnn.multiply(v424, v99, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v424, False)
  ttnn.deallocate(v99, False)
  v426 = ttnn.add(v425, v100, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v425, False)
  ttnn.deallocate(v100, False)
  v427 = ttnn.relu(v426, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v426, False)
  v428 = ttnn.to_layout(v427, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v427, False)
  v429 = ttnn.conv2d(input_tensor=v428, weight_tensor=v158, device=v164, in_channels=512, out_channels=2048, batch_size=1, input_height=7, input_width=7, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v428, False)
  ttnn.deallocate(v158, False)
  v430 = ttnn.reshape(v429, [1, 7, 7, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v429, False)
  v431 = ttnn.permute(v430, [0, 3, 1, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v430, False)
  v432 = ttnn.multiply(v431, v101, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v431, False)
  ttnn.deallocate(v101, False)
  v433 = ttnn.add(v432, v102, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v432, False)
  ttnn.deallocate(v102, False)
  v434 = ttnn.add(v433, v415, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v433, False)
  ttnn.deallocate(v415, False)
  v435 = ttnn.relu(v434, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v434, False)
  v436 = ttnn.permute(v435, [0, 2, 3, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  v437 = ttnn.reshape(v436, [1, 1, 49, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v436, False)
  v438 = ttnn.to_layout(v437, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v437, False)
  v439 = ttnn.conv2d(input_tensor=v438, weight_tensor=v159, device=v164, in_channels=2048, out_channels=512, batch_size=1, input_height=7, input_width=7, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v438, False)
  ttnn.deallocate(v159, False)
  v440 = ttnn.multiply(v439, v103, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v439, False)
  ttnn.deallocate(v103, False)
  v441 = ttnn.add(v440, v104, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v440, False)
  ttnn.deallocate(v104, False)
  v442 = ttnn.relu(v441, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v441, False)
  v443 = ttnn.to_layout(v442, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v442, False)
  v444 = ttnn.conv2d(input_tensor=v443, weight_tensor=v160, device=v164, in_channels=512, out_channels=512, batch_size=1, input_height=7, input_width=7, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v443, False)
  ttnn.deallocate(v160, False)
  v445 = ttnn.multiply(v444, v105, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v444, False)
  ttnn.deallocate(v105, False)
  v446 = ttnn.add(v445, v106, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v445, False)
  ttnn.deallocate(v106, False)
  v447 = ttnn.relu(v446, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v446, False)
  v448 = ttnn.to_layout(v447, ttnn.Layout.ROW_MAJOR, None, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v447, False)
  v449 = ttnn.conv2d(input_tensor=v448, weight_tensor=v161, device=v164, in_channels=512, out_channels=2048, batch_size=1, input_height=7, input_width=7, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, bias_tensor=None, conv_config=None, compute_config=None, slice_config=ttnn.Conv2dSliceConfig(slice_type=ttnn.Conv2dL1Full, num_slices=0), memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v448, False)
  ttnn.deallocate(v161, False)
  v450 = ttnn.reshape(v107, [1, 1, 2048, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v107, False)
  v451 = ttnn.permute(v450, [0, 1, 3, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v450, False)
  v452 = ttnn.multiply(v449, v451, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v451, False)
  ttnn.deallocate(v449, False)
  v453 = ttnn.reshape(v108, [1, 1, 2048, 1], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v108, False)
  v454 = ttnn.permute(v453, [0, 1, 3, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v453, False)
  v455 = ttnn.add(v452, v454, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v454, False)
  ttnn.deallocate(v452, False)
  v456 = ttnn.reshape(v435, [1, 1, 2048, 49], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v435, False)
  v457 = ttnn.permute(v456, [0, 1, 3, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v456, False)
  v458 = ttnn.add(v455, v457, dtype=ttnn.DataType.BFLOAT16, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v457, False)
  ttnn.deallocate(v455, False)
  v459 = ttnn.relu(v458, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v458, False)
  v460 = ttnn.mean(v459, [-2], True, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v459, False)
  v461 = ttnn.permute(v460, [0, 1, 3, 2], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None), pad_value=0.0)
  ttnn.deallocate(v460, False)
  v462 = ttnn.reshape(v461, [1, 2048], memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v461, False)
  v463 = ttnn.linear(v462, v162, bias=v163, transpose_a=False, transpose_b=False, memory_config=ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.DRAM, None))
  ttnn.deallocate(v462, False)
  ttnn.deallocate(v163, False)
  ttnn.deallocate(v162, False)
  v464 = [v463]
  return v464

def create_inputs_for_forward(): 
  v1 = my_get_device.DeviceGetter.get_device()
  v2 = ttnn.ones(shape=ttnn.Shape([1, 3, 224, 224]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v3 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v4 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v5 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v6 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v7 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v8 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v9 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v10 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v11 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v12 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v13 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v14 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v15 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v16 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v17 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v18 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v19 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v20 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v21 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v22 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 64]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v23 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v24 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v25 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v26 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v27 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v28 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v29 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v30 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v31 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v32 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v33 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v34 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v35 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v36 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v37 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v38 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v39 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v40 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v41 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v42 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v43 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v44 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v45 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v46 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v47 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v48 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 128]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v49 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v50 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v51 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v52 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v53 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v54 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v55 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v56 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v57 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v58 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v59 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v60 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v61 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v62 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v63 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v64 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v65 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v66 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v67 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v68 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v69 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v70 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v71 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v72 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v73 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v74 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v75 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v76 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v77 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v78 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v79 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v80 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v81 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v82 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v83 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v84 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v85 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v86 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 256]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v87 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v88 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 1024]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v89 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v90 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v91 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v92 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v93 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v94 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v95 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v96 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v97 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v98 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v99 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v100 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v101 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v102 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v103 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v104 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v105 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v106 = ttnn.ones(shape=ttnn.Shape([1, 1, 1, 512]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v107 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v108 = ttnn.ones(shape=ttnn.Shape([1, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v109 = ttnn.ones(shape=ttnn.Shape([64, 3, 7, 7]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v110 = ttnn.ones(shape=ttnn.Shape([64, 64, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v111 = ttnn.ones(shape=ttnn.Shape([64, 64, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v112 = ttnn.ones(shape=ttnn.Shape([256, 64, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v113 = ttnn.ones(shape=ttnn.Shape([256, 64, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v114 = ttnn.ones(shape=ttnn.Shape([64, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v115 = ttnn.ones(shape=ttnn.Shape([64, 64, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v116 = ttnn.ones(shape=ttnn.Shape([256, 64, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v117 = ttnn.ones(shape=ttnn.Shape([64, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v118 = ttnn.ones(shape=ttnn.Shape([64, 64, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v119 = ttnn.ones(shape=ttnn.Shape([256, 64, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v120 = ttnn.ones(shape=ttnn.Shape([128, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v121 = ttnn.ones(shape=ttnn.Shape([128, 128, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v122 = ttnn.ones(shape=ttnn.Shape([512, 128, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v123 = ttnn.ones(shape=ttnn.Shape([512, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v124 = ttnn.ones(shape=ttnn.Shape([128, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v125 = ttnn.ones(shape=ttnn.Shape([128, 128, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v126 = ttnn.ones(shape=ttnn.Shape([512, 128, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v127 = ttnn.ones(shape=ttnn.Shape([128, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v128 = ttnn.ones(shape=ttnn.Shape([128, 128, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v129 = ttnn.ones(shape=ttnn.Shape([512, 128, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v130 = ttnn.ones(shape=ttnn.Shape([128, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v131 = ttnn.ones(shape=ttnn.Shape([128, 128, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v132 = ttnn.ones(shape=ttnn.Shape([512, 128, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v133 = ttnn.ones(shape=ttnn.Shape([256, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v134 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v135 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v136 = ttnn.ones(shape=ttnn.Shape([1024, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v137 = ttnn.ones(shape=ttnn.Shape([256, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v138 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v139 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v140 = ttnn.ones(shape=ttnn.Shape([256, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v141 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v142 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v143 = ttnn.ones(shape=ttnn.Shape([256, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v144 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v145 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v146 = ttnn.ones(shape=ttnn.Shape([256, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v147 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v148 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v149 = ttnn.ones(shape=ttnn.Shape([256, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v150 = ttnn.ones(shape=ttnn.Shape([256, 256, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v151 = ttnn.ones(shape=ttnn.Shape([1024, 256, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v152 = ttnn.ones(shape=ttnn.Shape([512, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v153 = ttnn.ones(shape=ttnn.Shape([512, 512, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v154 = ttnn.ones(shape=ttnn.Shape([2048, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v155 = ttnn.ones(shape=ttnn.Shape([2048, 1024, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v156 = ttnn.ones(shape=ttnn.Shape([512, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v157 = ttnn.ones(shape=ttnn.Shape([512, 512, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v158 = ttnn.ones(shape=ttnn.Shape([2048, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v159 = ttnn.ones(shape=ttnn.Shape([512, 2048, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v160 = ttnn.ones(shape=ttnn.Shape([512, 512, 3, 3]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v161 = ttnn.ones(shape=ttnn.Shape([2048, 512, 1, 1]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.ROW_MAJOR, device=None)
  v162 = ttnn.ones(shape=ttnn.Shape([2048, 1000]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v163 = ttnn.ones(shape=ttnn.Shape([1000]), dtype=ttnn.DataType.BFLOAT16, layout=ttnn.Layout.TILE, device=v1)
  v164 = [v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31, v32, v33, v34, v35, v36, v37, v38, v39, v40, v41, v42, v43, v44, v45, v46, v47, v48, v49, v50, v51, v52, v53, v54, v55, v56, v57, v58, v59, v60, v61, v62, v63, v64, v65, v66, v67, v68, v69, v70, v71, v72, v73, v74, v75, v76, v77, v78, v79, v80, v81, v82, v83, v84, v85, v86, v87, v88, v89, v90, v91, v92, v93, v94, v95, v96, v97, v98, v99, v100, v101, v102, v103, v104, v105, v106, v107, v108, v109, v110, v111, v112, v113, v114, v115, v116, v117, v118, v119, v120, v121, v122, v123, v124, v125, v126, v127, v128, v129, v130, v131, v132, v133, v134, v135, v136, v137, v138, v139, v140, v141, v142, v143, v144, v145, v146, v147, v148, v149, v150, v151, v152, v153, v154, v155, v156, v157, v158, v159, v160, v161, v162, v163]
  return v164

def main(): 
  v1 = create_inputs_for_forward()
  v2 = forward(v1)
  v3 = 0
  return v3

if __name__ == '__main__':
  main()


